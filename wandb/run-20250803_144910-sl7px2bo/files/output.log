/root/autodl-tmp/MLLM/ImageCaption/hf_style_trainer.py:178: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.fp16)
Training:   0%|                                                            | 0/9000 [00:00<?, ?it/s]/root/autodl-tmp/MLLM/ImageCaption/hf_style_trainer.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.bfloat16 if args.bf16 else torch.float16):
/root/autodl-tmp/MLLM/jpeglm/models/jpeglm_encoder.py:242: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/root/autodl-tmp/MLLM/jpeglm/models/jpeglm_encoder.py:272: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
Training:   1%| | 128/9000 [02:07<2:37:18,  1.06s/it, ep=0.04/3, step=128, loss=6.6719, lr=5.00e-05]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
[Batch    64] [Opt    8] [Ep  0.021] | Loss:  9.4375 | GradNorm:  69.462 | LR: 2.50e-05
[Batch   128] [Opt   16] [Ep  0.043] | Loss:  6.6719 | GradNorm:  64.444 | LR: 5.00e-05
Training:   6%| | 512/9000 [09:01<2:11:58,  1.07it/s, ep=0.17/3, step=512, loss=2.6719, lr=2.00e-04]/root/miniconda3/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.
  warnings.warn(                                                                                    
[Batch   128] [EVAL] | Loss:  2.9656 | accuracy: 0.1250 | rouge2_fmeasure: 0.0000 | bleu1: 0.1250 | bleu4: 0.1250
[Batch   192] [Opt   24] [Ep  0.064] | Loss:  6.0234 | GradNorm:  19.779 | LR: 7.50e-05
[Batch   256] [Opt   32] [Ep  0.085] | Loss:  4.2344 | GradNorm:  58.397 | LR: 1.00e-04
[Batch   256] [EVAL] | Loss:  2.8539 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch   320] [Opt   40] [Ep  0.107] | Loss:  2.4297 | GradNorm:  28.704 | LR: 1.25e-04
[Batch   384] [Opt   48] [Ep  0.128] | Loss:  2.4141 | GradNorm:  32.680 | LR: 1.50e-04
[Batch   384] [EVAL] | Loss:  2.6018 | accuracy: 0.1250 | rouge2_fmeasure: 0.0000 | bleu1: 0.1250 | bleu4: 0.1250
[Batch   448] [Opt   56] [Ep  0.149] | Loss:  2.4531 | GradNorm:  21.840 | LR: 1.75e-04
[Batch   512] [Opt   64] [Ep  0.171] | Loss:  2.6719 | GradNorm:  81.120 | LR: 2.00e-04
[Batch   512] [EVAL] | Loss:  2.2698 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch   512] [SAVE] | 保存检查点到 checkpoint-512
                                                                                                    
[Batch   576] [Opt   72] [Ep  0.192] | Loss:  2.6875 | GradNorm:  42.402 | LR: 1.98e-04
[Batch   640] [Opt   80] [Ep  0.213] | Loss:  2.6094 | GradNorm:  25.309 | LR: 1.97e-04
                                                                                                    
[Batch   640] [EVAL] | Loss:  2.0592 | accuracy: 0.0000 | rouge2_fmeasure: 0.0000 | bleu1: 0.0000 | bleu4: 0.0000
[Batch   704] [Opt   88] [Ep  0.235] | Loss:  2.6484 | GradNorm:  17.796 | LR: 1.95e-04
[Batch   768] [Opt   96] [Ep  0.256] | Loss:  1.5625 | GradNorm:  24.228 | LR: 1.94e-04
[Batch   768] [EVAL] | Loss:  2.1947 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch   832] [Opt  104] [Ep  0.277] | Loss:  2.3398 | GradNorm:  53.599 | LR: 1.92e-04
[Batch   896] [Opt  112] [Ep  0.299] | Loss:  2.4609 | GradNorm:  21.208 | LR: 1.91e-04
[Batch   896] [EVAL] | Loss:  1.8154 | accuracy: 0.1250 | rouge2_fmeasure: 0.0000 | bleu1: 0.1250 | bleu4: 0.1250
[Batch   960] [Opt  120] [Ep  0.320] | Loss:  2.0977 | GradNorm:  52.028 | LR: 1.89e-04
[Batch  1024] [Opt  128] [Ep  0.341] | Loss:  2.8750 | GradNorm:  36.654 | LR: 1.88e-04
[Batch  1024] [EVAL] | Loss:  1.9078 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  1024] [SAVE] | 保存检查点到 checkpoint-1024
[Batch  1088] [Opt  136] [Ep  0.363] | Loss:  1.9805 | GradNorm:  43.684 | LR: 1.86e-04
[Batch  1152] [Opt  144] [Ep  0.384] | Loss:  0.8828 | GradNorm:  44.663 | LR: 1.85e-04
[Batch  1152] [EVAL] | Loss:  1.7111 | accuracy: 0.1250 | rouge2_fmeasure: 0.0000 | bleu1: 0.1250 | bleu4: 0.1250
[Batch  1216] [Opt  152] [Ep  0.405] | Loss:  1.6523 | GradNorm:  29.115 | LR: 1.83e-04
[Batch  1280] [Opt  160] [Ep  0.427] | Loss:  2.6406 | GradNorm:  30.980 | LR: 1.82e-04
[Batch  1280] [EVAL] | Loss:  1.4305 | accuracy: 0.1250 | rouge2_fmeasure: 0.0000 | bleu1: 0.1250 | bleu4: 0.1250
[Batch  1344] [Opt  168] [Ep  0.448] | Loss:  1.5146 | GradNorm:  28.988 | LR: 1.80e-04
[Batch  1408] [Opt  176] [Ep  0.469] | Loss:  2.9531 | GradNorm:  36.038 | LR: 1.79e-04
[Batch  1408] [EVAL] | Loss:  1.3095 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  1472] [Opt  184] [Ep  0.491] | Loss:  0.7441 | GradNorm:  19.266 | LR: 1.77e-04
[Batch  1536] [Opt  192] [Ep  0.512] | Loss:  2.5430 | GradNorm:  46.349 | LR: 1.76e-04
[Batch  1536] [EVAL] | Loss:  1.7100 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  1536] [SAVE] | 保存检查点到 checkpoint-1536
[Batch  1600] [Opt  200] [Ep  0.533] | Loss:  0.9844 | GradNorm:  26.072 | LR: 1.74e-04
[Batch  1664] [Opt  208] [Ep  0.555] | Loss:  2.5156 | GradNorm:  45.124 | LR: 1.73e-04
[Batch  1664] [EVAL] | Loss:  2.2324 | accuracy: 0.3125 | rouge2_fmeasure: 0.0000 | bleu1: 0.3125 | bleu4: 0.3125
[Batch  1728] [Opt  216] [Ep  0.576] | Loss:  1.9805 | GradNorm:  64.215 | LR: 1.71e-04
[Batch  1792] [Opt  224] [Ep  0.597] | Loss:  2.1562 | GradNorm:  41.992 | LR: 1.70e-04
[Batch  1792] [EVAL] | Loss:  1.7252 | accuracy: 0.2500 | rouge2_fmeasure: 0.0000 | bleu1: 0.2500 | bleu4: 0.2500
[Batch  1856] [Opt  232] [Ep  0.619] | Loss:  2.9141 | GradNorm:  38.822 | LR: 1.68e-04
[Batch  1920] [Opt  240] [Ep  0.640] | Loss:  1.7656 | GradNorm:  13.465 | LR: 1.67e-04
[Batch  1920] [EVAL] | Loss:  1.6596 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  1984] [Opt  248] [Ep  0.661] | Loss:  1.5898 | GradNorm:  26.282 | LR: 1.65e-04
[Batch  2048] [Opt  256] [Ep  0.683] | Loss:  1.5176 | GradNorm:  42.553 | LR: 1.64e-04
[Batch  2048] [EVAL] | Loss:  1.2549 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  2048] [SAVE] | 保存检查点到 checkpoint-2048
[Batch  2112] [Opt  264] [Ep  0.704] | Loss:  1.4727 | GradNorm:  35.157 | LR: 1.62e-04
[Batch  2176] [Opt  272] [Ep  0.725] | Loss:  2.6172 | GradNorm:  40.032 | LR: 1.61e-04
[Batch  2176] [EVAL] | Loss:  1.7939 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  2240] [Opt  280] [Ep  0.747] | Loss:  4.0391 | GradNorm:  41.512 | LR: 1.59e-04
[Batch  2304] [Opt  288] [Ep  0.768] | Loss:  1.8242 | GradNorm:  50.204 | LR: 1.58e-04
[Batch  2304] [EVAL] | Loss:  2.0153 | accuracy: 0.0000 | rouge2_fmeasure: 0.0000 | bleu1: 0.0000 | bleu4: 0.0000
[Batch  2368] [Opt  296] [Ep  0.789] | Loss:  1.7070 | GradNorm:  35.474 | LR: 1.56e-04
[Batch  2432] [Opt  304] [Ep  0.811] | Loss:  1.7852 | GradNorm:  24.441 | LR: 1.55e-04
[Batch  2432] [EVAL] | Loss:  1.4922 | accuracy: 0.0000 | rouge2_fmeasure: 0.0000 | bleu1: 0.0000 | bleu4: 0.0000
[Batch  2496] [Opt  312] [Ep  0.832] | Loss:  1.0918 | GradNorm:  17.446 | LR: 1.53e-04
[Batch  2560] [Opt  320] [Ep  0.853] | Loss:  3.7109 | GradNorm:  54.594 | LR: 1.52e-04
[Batch  2560] [EVAL] | Loss:  3.2541 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  2560] [SAVE] | 保存检查点到 checkpoint-2560
[Batch  2624] [Opt  328] [Ep  0.875] | Loss:  3.0781 | GradNorm:  33.428 | LR: 1.50e-04
[Batch  2688] [Opt  336] [Ep  0.896] | Loss:  2.8125 | GradNorm:  38.205 | LR: 1.49e-04
[Batch  2688] [EVAL] | Loss:  2.2282 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  2752] [Opt  344] [Ep  0.917] | Loss:  1.9062 | GradNorm:  33.746 | LR: 1.47e-04
[Batch  2816] [Opt  352] [Ep  0.939] | Loss:  2.4141 | GradNorm:  48.222 | LR: 1.46e-04
[Batch  2816] [EVAL] | Loss:  2.2367 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  2880] [Opt  360] [Ep  0.960] | Loss:  2.2109 | GradNorm:  40.079 | LR: 1.44e-04
[Batch  2944] [Opt  368] [Ep  0.981] | Loss:  2.0938 | GradNorm:  26.308 | LR: 1.43e-04
[Batch  2944] [EVAL] | Loss:  2.3323 | accuracy: 0.1250 | rouge2_fmeasure: 0.0000 | bleu1: 0.1250 | bleu4: 0.1250
=== [EPOCH 1/3 完成] | 平均Loss: 2.4268 | 总Batch步数: 3000 | 总Opt步数: 375 ===
[Batch  3008] [Opt  376] [Ep  1.003] | Loss:  2.9219 | GradNorm:  33.545 | LR: 1.41e-04
[Batch  3072] [Opt  384] [Ep  1.024] | Loss:  2.0742 | GradNorm:  24.534 | LR: 1.40e-04
[Batch  3072] [EVAL] | Loss:  2.3736 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  3072] [SAVE] | 保存检查点到 checkpoint-3072
[Batch  3136] [Opt  392] [Ep  1.045] | Loss:  2.3281 | GradNorm:  29.760 | LR: 1.38e-04
[Batch  3200] [Opt  400] [Ep  1.067] | Loss:  2.6016 | GradNorm:  43.698 | LR: 1.37e-04
[Batch  3200] [EVAL] | Loss:  2.4456 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  3264] [Opt  408] [Ep  1.088] | Loss:  2.4688 | GradNorm:  30.739 | LR: 1.35e-04
[Batch  3328] [Opt  416] [Ep  1.109] | Loss:  2.1055 | GradNorm:  26.887 | LR: 1.34e-04
[Batch  3328] [EVAL] | Loss:  2.4022 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  3392] [Opt  424] [Ep  1.131] | Loss:  2.3203 | GradNorm:  35.372 | LR: 1.32e-04
[Batch  3456] [Opt  432] [Ep  1.152] | Loss:  2.0273 | GradNorm:  21.716 | LR: 1.31e-04
[Batch  3456] [EVAL] | Loss:  2.2781 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  3520] [Opt  440] [Ep  1.173] | Loss:  2.3047 | GradNorm:  33.889 | LR: 1.29e-04
[Batch  3584] [Opt  448] [Ep  1.195] | Loss:  2.3672 | GradNorm:  31.246 | LR: 1.28e-04
[Batch  3584] [EVAL] | Loss:  2.4163 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  3584] [SAVE] | 保存检查点到 checkpoint-3584
[Batch  3648] [Opt  456] [Ep  1.216] | Loss:  2.4297 | GradNorm:  15.237 | LR: 1.26e-04
[Batch  3712] [Opt  464] [Ep  1.237] | Loss:  2.2344 | GradNorm:  30.648 | LR: 1.25e-04
[Batch  3712] [EVAL] | Loss:  2.2871 | accuracy: 0.1875 | rouge2_fmeasure: 0.0000 | bleu1: 0.1875 | bleu4: 0.1875
[Batch  3776] [Opt  472] [Ep  1.259] | Loss:  2.3359 | GradNorm:  24.497 | LR: 1.23e-04
[Batch  3840] [Opt  480] [Ep  1.280] | Loss:  2.0391 | GradNorm:  19.261 | LR: 1.22e-04
[Batch  3840] [EVAL] | Loss:  2.4117 | accuracy: 0.0625 | rouge2_fmeasure: 0.0000 | bleu1: 0.0625 | bleu4: 0.0625
[Batch  3904] [Opt  488] [Ep  1.301] | Loss:  2.0781 | GradNorm:  22.318 | LR: 1.20e-04
[Batch  3968] [Opt  496] [Ep  1.323] | Loss:  2.3750 | GradNorm:  15.670 | LR: 1.19e-04
[Batch  3968] [EVAL] | Loss:  2.4276 | accuracy: 0.0000 | rouge2_fmeasure: 0.0000 | bleu1: 0.0000 | bleu4: 0.0000
[Batch  4032] [Opt  504] [Ep  1.344] | Loss:  2.5000 | GradNorm:  13.842 | LR: 1.17e-04
    trainer.train()
  File "/root/autodl-tmp/MLLM/ImageCaption/hf_style_trainer.py", line 229, in train
    for step, batch in enumerate(train_loader):
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 733, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 789, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/root/autodl-tmp/MLLM/ImageCaption/train_jpeglm-gpt2_cls.py", line 124, in __getitem__
    jpeg_str = convert_img_to_bytes(img, bit_flip_prob=self.bit_flip_prob)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/utils/data_utils.py", line 26, in convert_img_to_bytes
    img.save("cache_tables.jpg", format="JPEG", quality=quality, subsampling="4:2:0", streamtype=1, restart_marker_blocks=1)
  File "/root/miniconda3/lib/python3.12/site-packages/PIL/Image.py", line 2410, in save
    ext = os.path.splitext(filename)[1].lower()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen posixpath>", line 117, in splitext
KeyboardInterrupt
