  0%|                                                                       | 0/16200 [00:00<?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 9.137, 'grad_norm': nan, 'learning_rate': 0.0001999753086419753, 'epoch': 0.0}
{'loss': 9.2604, 'grad_norm': nan, 'learning_rate': 0.00019992592592592593, 'epoch': 0.0}
{'loss': 9.1581, 'grad_norm': 14.184733390808105, 'learning_rate': 0.00019987654320987653, 'epoch': 0.0}
{'loss': 8.3803, 'grad_norm': 20.19847869873047, 'learning_rate': 0.00019981481481481483, 'epoch': 0.0}
{'loss': 6.7109, 'grad_norm': 13.621017456054688, 'learning_rate': 0.0001997530864197531, 'epoch': 0.0}
{'loss': 6.7108, 'grad_norm': 12.727845191955566, 'learning_rate': 0.00019969135802469138, 'epoch': 0.01}
{'loss': 6.792, 'grad_norm': 9.405223846435547, 'learning_rate': 0.00019962962962962963, 'epoch': 0.01}
{'loss': 6.1604, 'grad_norm': 6.607109069824219, 'learning_rate': 0.0001995679012345679, 'epoch': 0.01}
{'loss': 5.8239, 'grad_norm': 11.350926399230957, 'learning_rate': 0.0001995061728395062, 'epoch': 0.01}
{'loss': 5.3312, 'grad_norm': 8.933354377746582, 'learning_rate': 0.00019944444444444445, 'epoch': 0.01}
{'loss': 5.6815, 'grad_norm': 7.667684555053711, 'learning_rate': 0.00019938271604938272, 'epoch': 0.01}
{'loss': 6.3124, 'grad_norm': 14.378418922424316, 'learning_rate': 0.000199320987654321, 'epoch': 0.01}
{'loss': 5.6224, 'grad_norm': 10.910720825195312, 'learning_rate': 0.00019925925925925927, 'epoch': 0.01}
{'loss': 4.969, 'grad_norm': 8.539955139160156, 'learning_rate': 0.00019919753086419754, 'epoch': 0.01}
{'loss': 5.2875, 'grad_norm': 11.089689254760742, 'learning_rate': 0.00019913580246913582, 'epoch': 0.01}
{'loss': 5.2146, 'grad_norm': 14.273775100708008, 'learning_rate': 0.0001990740740740741, 'epoch': 0.01}
{'loss': 4.8776, 'grad_norm': 8.72522258758545, 'learning_rate': 0.00019901234567901234, 'epoch': 0.02}
{'loss': 6.0302, 'grad_norm': 16.939682006835938, 'learning_rate': 0.00019895061728395064, 'epoch': 0.02}
{'loss': 4.3188, 'grad_norm': 10.378087043762207, 'learning_rate': 0.0001988888888888889, 'epoch': 0.02}
{'loss': 5.3184, 'grad_norm': 14.083292007446289, 'learning_rate': 0.00019882716049382716, 'epoch': 0.02}
{'loss': 4.6732, 'grad_norm': 8.79088306427002, 'learning_rate': 0.00019876543209876543, 'epoch': 0.02}
{'loss': 4.8833, 'grad_norm': 15.668684005737305, 'learning_rate': 0.0001987037037037037, 'epoch': 0.02}
{'loss': 4.8443, 'grad_norm': 12.492205619812012, 'learning_rate': 0.00019864197530864198, 'epoch': 0.02}
{'loss': 4.0009, 'grad_norm': 15.561022758483887, 'learning_rate': 0.00019858024691358025, 'epoch': 0.02}
{'loss': 4.351, 'grad_norm': 10.039549827575684, 'learning_rate': 0.00019851851851851853, 'epoch': 0.02}
{'loss': 4.3112, 'grad_norm': 8.429351806640625, 'learning_rate': 0.0001984567901234568, 'epoch': 0.02}
{'loss': 4.804, 'grad_norm': 8.480158805847168, 'learning_rate': 0.00019839506172839507, 'epoch': 0.03}
{'loss': 5.0403, 'grad_norm': 28.315828323364258, 'learning_rate': 0.00019833333333333335, 'epoch': 0.03}
{'loss': 4.6818, 'grad_norm': 13.08845329284668, 'learning_rate': 0.00019827160493827162, 'epoch': 0.03}
{'loss': 4.3975, 'grad_norm': 8.691146850585938, 'learning_rate': 0.0001982098765432099, 'epoch': 0.03}
{'loss': 4.4297, 'grad_norm': 9.04754638671875, 'learning_rate': 0.00019814814814814814, 'epoch': 0.03}
{'loss': 3.5916, 'grad_norm': 8.840506553649902, 'learning_rate': 0.00019808641975308644, 'epoch': 0.03}
{'loss': 3.7984, 'grad_norm': 14.694331169128418, 'learning_rate': 0.00019802469135802472, 'epoch': 0.03}
{'loss': 4.3963, 'grad_norm': 7.947936058044434, 'learning_rate': 0.00019796296296296296, 'epoch': 0.03}
{'loss': 4.1011, 'grad_norm': 10.18507194519043, 'learning_rate': 0.00019790123456790124, 'epoch': 0.03}
{'loss': 4.6658, 'grad_norm': 11.055024147033691, 'learning_rate': 0.0001978395061728395, 'epoch': 0.03}
{'loss': 5.2555, 'grad_norm': 12.476544380187988, 'learning_rate': 0.00019777777777777778, 'epoch': 0.03}
{'loss': 4.2996, 'grad_norm': 9.175044059753418, 'learning_rate': 0.00019771604938271606, 'epoch': 0.04}
{'loss': 4.2114, 'grad_norm': 6.169828414916992, 'learning_rate': 0.00019765432098765433, 'epoch': 0.04}
{'loss': 3.8124, 'grad_norm': 11.920524597167969, 'learning_rate': 0.0001975925925925926, 'epoch': 0.04}
{'loss': 4.0175, 'grad_norm': 8.247503280639648, 'learning_rate': 0.00019753086419753085, 'epoch': 0.04}
{'loss': 3.7564, 'grad_norm': 9.016297340393066, 'learning_rate': 0.00019746913580246915, 'epoch': 0.04}
{'loss': 4.1832, 'grad_norm': 8.508902549743652, 'learning_rate': 0.00019740740740740743, 'epoch': 0.04}
{'loss': 3.7777, 'grad_norm': 10.062039375305176, 'learning_rate': 0.00019734567901234567, 'epoch': 0.04}
{'loss': 3.8242, 'grad_norm': 6.286856174468994, 'learning_rate': 0.00019728395061728395, 'epoch': 0.04}
{'loss': 3.9137, 'grad_norm': 12.333340644836426, 'learning_rate': 0.00019722222222222225, 'epoch': 0.04}
{'loss': 4.252, 'grad_norm': 5.828189373016357, 'learning_rate': 0.0001971604938271605, 'epoch': 0.04}
{'loss': 4.208, 'grad_norm': 9.930990219116211, 'learning_rate': 0.00019709876543209877, 'epoch': 0.04}
{'loss': 4.3429, 'grad_norm': 22.525476455688477, 'learning_rate': 0.00019703703703703704, 'epoch': 0.05}
{'loss': 3.8892, 'grad_norm': 6.162234783172607, 'learning_rate': 0.00019697530864197532, 'epoch': 0.05}
{'loss': 3.6794, 'grad_norm': 15.473274230957031, 'learning_rate': 0.0001969135802469136, 'epoch': 0.05}
{'loss': 3.2995, 'grad_norm': 7.459486961364746, 'learning_rate': 0.00019685185185185186, 'epoch': 0.05}
{'loss': 3.282, 'grad_norm': 40.02542495727539, 'learning_rate': 0.00019679012345679014, 'epoch': 0.05}
{'loss': 3.286, 'grad_norm': 10.269131660461426, 'learning_rate': 0.0001967283950617284, 'epoch': 0.05}
{'loss': 4.1825, 'grad_norm': 7.085021495819092, 'learning_rate': 0.00019666666666666666, 'epoch': 0.05}
{'loss': 3.8125, 'grad_norm': 10.789412498474121, 'learning_rate': 0.00019660493827160496, 'epoch': 0.05}
{'loss': 3.1634, 'grad_norm': 8.905004501342773, 'learning_rate': 0.00019654320987654323, 'epoch': 0.05}
{'loss': 4.5016, 'grad_norm': 14.638284683227539, 'learning_rate': 0.00019648148148148148, 'epoch': 0.05}
{'loss': 4.6524, 'grad_norm': 11.410823822021484, 'learning_rate': 0.00019641975308641975, 'epoch': 0.05}
{'loss': 4.3428, 'grad_norm': 8.316832542419434, 'learning_rate': 0.00019635802469135805, 'epoch': 0.06}
{'loss': 4.0268, 'grad_norm': 8.307538986206055, 'learning_rate': 0.0001962962962962963, 'epoch': 0.06}
{'loss': 3.4429, 'grad_norm': 11.353927612304688, 'learning_rate': 0.00019623456790123457, 'epoch': 0.06}
{'loss': 4.1061, 'grad_norm': 11.493851661682129, 'learning_rate': 0.00019617283950617285, 'epoch': 0.06}
{'loss': 4.1559, 'grad_norm': 5.974112510681152, 'learning_rate': 0.00019611111111111112, 'epoch': 0.06}
{'loss': 4.2979, 'grad_norm': 36.95156478881836, 'learning_rate': 0.0001960493827160494, 'epoch': 0.06}
{'loss': 3.3595, 'grad_norm': 7.375889778137207, 'learning_rate': 0.00019598765432098767, 'epoch': 0.06}
{'loss': 3.7036, 'grad_norm': 11.391109466552734, 'learning_rate': 0.00019592592592592594, 'epoch': 0.06}
{'loss': 3.8963, 'grad_norm': 18.557222366333008, 'learning_rate': 0.0001958641975308642, 'epoch': 0.06}
{'loss': 3.498, 'grad_norm': 8.190902709960938, 'learning_rate': 0.00019580246913580246, 'epoch': 0.06}
{'loss': 3.0155, 'grad_norm': 6.439294815063477, 'learning_rate': 0.00019574074074074077, 'epoch': 0.06}
{'loss': 3.9151, 'grad_norm': 6.267655849456787, 'learning_rate': 0.000195679012345679, 'epoch': 0.07}
{'loss': 3.6747, 'grad_norm': 6.554978847503662, 'learning_rate': 0.00019561728395061729, 'epoch': 0.07}
{'loss': 3.174, 'grad_norm': 11.546006202697754, 'learning_rate': 0.00019555555555555556, 'epoch': 0.07}
{'loss': 4.1563, 'grad_norm': 9.645583152770996, 'learning_rate': 0.00019549382716049386, 'epoch': 0.07}
{'loss': 3.8883, 'grad_norm': 14.476700782775879, 'learning_rate': 0.0001954320987654321, 'epoch': 0.07}
{'loss': 3.6994, 'grad_norm': 15.83546257019043, 'learning_rate': 0.00019537037037037038, 'epoch': 0.07}
{'loss': 3.2748, 'grad_norm': 7.5046706199646, 'learning_rate': 0.00019530864197530865, 'epoch': 0.07}
{'loss': 3.5709, 'grad_norm': 15.118308067321777, 'learning_rate': 0.00019524691358024693, 'epoch': 0.07}
{'loss': 4.4596, 'grad_norm': 14.749327659606934, 'learning_rate': 0.0001951851851851852, 'epoch': 0.07}
{'loss': 3.6113, 'grad_norm': 8.062714576721191, 'learning_rate': 0.00019512345679012348, 'epoch': 0.07}
{'loss': 3.2694, 'grad_norm': 7.421031951904297, 'learning_rate': 0.00019506172839506175, 'epoch': 0.07}
{'loss': 3.3762, 'grad_norm': 17.005386352539062, 'learning_rate': 0.000195, 'epoch': 0.08}
{'loss': 3.7044, 'grad_norm': 6.5099945068359375, 'learning_rate': 0.00019493827160493827, 'epoch': 0.08}
{'loss': 4.0703, 'grad_norm': 6.722643852233887, 'learning_rate': 0.00019487654320987657, 'epoch': 0.08}
{'loss': 3.7195, 'grad_norm': 6.459977626800537, 'learning_rate': 0.00019481481481481482, 'epoch': 0.08}
{'loss': 3.4078, 'grad_norm': 13.36444091796875, 'learning_rate': 0.0001947530864197531, 'epoch': 0.08}
{'loss': 3.6089, 'grad_norm': 6.199448585510254, 'learning_rate': 0.00019469135802469137, 'epoch': 0.08}
{'loss': 2.8204, 'grad_norm': 8.887015342712402, 'learning_rate': 0.00019462962962962964, 'epoch': 0.08}
{'loss': 2.7901, 'grad_norm': 10.418254852294922, 'learning_rate': 0.0001945679012345679, 'epoch': 0.08}
{'loss': 4.1012, 'grad_norm': 9.266898155212402, 'learning_rate': 0.0001945061728395062, 'epoch': 0.08}
{'loss': 3.2334, 'grad_norm': 11.1925630569458, 'learning_rate': 0.00019444444444444446, 'epoch': 0.08}
{'loss': 3.0177, 'grad_norm': 8.343331336975098, 'learning_rate': 0.0001943827160493827, 'epoch': 0.09}
{'loss': 3.5603, 'grad_norm': 5.631224155426025, 'learning_rate': 0.000194320987654321, 'epoch': 0.09}
{'loss': 3.5229, 'grad_norm': 5.895113468170166, 'learning_rate': 0.00019425925925925928, 'epoch': 0.09}
{'loss': 3.2365, 'grad_norm': 5.501543998718262, 'learning_rate': 0.00019419753086419753, 'epoch': 0.09}
{'loss': 3.0611, 'grad_norm': 11.696539878845215, 'learning_rate': 0.0001941358024691358, 'epoch': 0.09}
{'loss': 4.4602, 'grad_norm': 9.7203369140625, 'learning_rate': 0.00019407407407407408, 'epoch': 0.09}
{'loss': 4.5132, 'grad_norm': 6.313962459564209, 'learning_rate': 0.00019401234567901238, 'epoch': 0.09}
{'loss': 3.1785, 'grad_norm': 9.9984130859375, 'learning_rate': 0.00019395061728395062, 'epoch': 0.09}
{'loss': 3.5809, 'grad_norm': 7.720208644866943, 'learning_rate': 0.0001938888888888889, 'epoch': 0.09}
  warnings.warn(                                                                                      
{'eval_loss': 3.145261764526367, 'eval_runtime': 414.0432, 'eval_samples_per_second': 1.449, 'eval_steps_per_second': 1.449, 'epoch': 0.09}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 3.4903, 'grad_norm': 10.181840896606445, 'learning_rate': 0.00019382716049382717, 'epoch': 0.09}
{'loss': 2.9947, 'grad_norm': 7.616023063659668, 'learning_rate': 0.00019376543209876542, 'epoch': 0.09}
{'loss': 3.49, 'grad_norm': 5.724084854125977, 'learning_rate': 0.00019370370370370372, 'epoch': 0.1}
{'loss': 3.4284, 'grad_norm': 7.248626708984375, 'learning_rate': 0.000193641975308642, 'epoch': 0.1}
{'loss': 3.6184, 'grad_norm': 6.048671245574951, 'learning_rate': 0.00019358024691358027, 'epoch': 0.1}
{'loss': 3.7908, 'grad_norm': 7.0198469161987305, 'learning_rate': 0.0001935185185185185, 'epoch': 0.1}
{'loss': 2.721, 'grad_norm': 6.563958168029785, 'learning_rate': 0.00019345679012345681, 'epoch': 0.1}
{'loss': 3.4481, 'grad_norm': 5.29697847366333, 'learning_rate': 0.0001933950617283951, 'epoch': 0.1}
{'loss': 3.7927, 'grad_norm': 23.4228458404541, 'learning_rate': 0.00019333333333333333, 'epoch': 0.1}
{'loss': 3.2078, 'grad_norm': 9.282489776611328, 'learning_rate': 0.0001932716049382716, 'epoch': 0.1}
{'loss': 3.5201, 'grad_norm': 6.596360206604004, 'learning_rate': 0.00019320987654320988, 'epoch': 0.1}
{'loss': 3.3233, 'grad_norm': 7.291106700897217, 'learning_rate': 0.00019314814814814816, 'epoch': 0.1}
{'loss': 3.8359, 'grad_norm': 7.795210361480713, 'learning_rate': 0.00019308641975308643, 'epoch': 0.1}
{'loss': 3.6864, 'grad_norm': 12.455098152160645, 'learning_rate': 0.0001930246913580247, 'epoch': 0.11}
{'loss': 3.3834, 'grad_norm': 6.468979358673096, 'learning_rate': 0.00019296296296296298, 'epoch': 0.11}
{'loss': 3.4676, 'grad_norm': 6.650251388549805, 'learning_rate': 0.00019290123456790122, 'epoch': 0.11}
{'loss': 3.6213, 'grad_norm': 8.366819381713867, 'learning_rate': 0.00019283950617283952, 'epoch': 0.11}
{'loss': 3.4291, 'grad_norm': 6.586604118347168, 'learning_rate': 0.0001927777777777778, 'epoch': 0.11}
{'loss': 3.6743, 'grad_norm': 6.189530372619629, 'learning_rate': 0.00019271604938271605, 'epoch': 0.11}
{'loss': 3.293, 'grad_norm': 5.457849979400635, 'learning_rate': 0.00019265432098765432, 'epoch': 0.11}
{'loss': 3.2509, 'grad_norm': 5.367327690124512, 'learning_rate': 0.0001925925925925926, 'epoch': 0.11}
{'loss': 3.7914, 'grad_norm': 8.640896797180176, 'learning_rate': 0.0001925308641975309, 'epoch': 0.11}
{'loss': 3.2637, 'grad_norm': 4.694164276123047, 'learning_rate': 0.00019246913580246914, 'epoch': 0.11}
{'loss': 3.6883, 'grad_norm': 12.728911399841309, 'learning_rate': 0.00019240740740740741, 'epoch': 0.11}
{'loss': 3.7356, 'grad_norm': 6.162914276123047, 'learning_rate': 0.0001923456790123457, 'epoch': 0.12}
{'loss': 3.3861, 'grad_norm': 7.503389358520508, 'learning_rate': 0.00019228395061728396, 'epoch': 0.12}
{'loss': 3.1644, 'grad_norm': 12.139238357543945, 'learning_rate': 0.00019222222222222224, 'epoch': 0.12}
{'loss': 3.2836, 'grad_norm': 7.807848930358887, 'learning_rate': 0.0001921604938271605, 'epoch': 0.12}
{'loss': 2.9032, 'grad_norm': 4.916998863220215, 'learning_rate': 0.00019209876543209878, 'epoch': 0.12}
{'loss': 4.5554, 'grad_norm': 20.72572135925293, 'learning_rate': 0.00019203703703703703, 'epoch': 0.12}
{'loss': 2.9698, 'grad_norm': 3.995712995529175, 'learning_rate': 0.00019197530864197533, 'epoch': 0.12}
{'loss': 3.6075, 'grad_norm': 6.022473335266113, 'learning_rate': 0.0001919135802469136, 'epoch': 0.12}
{'loss': 3.049, 'grad_norm': 5.219223499298096, 'learning_rate': 0.00019185185185185185, 'epoch': 0.12}
{'loss': 3.8419, 'grad_norm': 16.224828720092773, 'learning_rate': 0.00019179012345679012, 'epoch': 0.12}
{'loss': 3.466, 'grad_norm': 10.382229804992676, 'learning_rate': 0.0001917283950617284, 'epoch': 0.12}
{'loss': 3.9847, 'grad_norm': 12.65738296508789, 'learning_rate': 0.00019166666666666667, 'epoch': 0.13}
{'loss': 2.9088, 'grad_norm': 7.252695083618164, 'learning_rate': 0.00019160493827160495, 'epoch': 0.13}
{'loss': 3.5531, 'grad_norm': 8.566587448120117, 'learning_rate': 0.00019154320987654322, 'epoch': 0.13}
{'loss': 3.4678, 'grad_norm': 11.872379302978516, 'learning_rate': 0.0001914814814814815, 'epoch': 0.13}
{'loss': 3.3026, 'grad_norm': 5.423629283905029, 'learning_rate': 0.00019141975308641977, 'epoch': 0.13}
{'loss': 3.0459, 'grad_norm': 8.449477195739746, 'learning_rate': 0.00019135802469135804, 'epoch': 0.13}
{'loss': 3.62, 'grad_norm': 7.587066173553467, 'learning_rate': 0.00019129629629629632, 'epoch': 0.13}
{'loss': 3.4302, 'grad_norm': 13.739468574523926, 'learning_rate': 0.00019123456790123456, 'epoch': 0.13}
{'loss': 3.6115, 'grad_norm': 11.153858184814453, 'learning_rate': 0.00019117283950617284, 'epoch': 0.13}
{'loss': 2.8774, 'grad_norm': 5.094882011413574, 'learning_rate': 0.00019111111111111114, 'epoch': 0.13}
{'loss': 3.5171, 'grad_norm': 9.570791244506836, 'learning_rate': 0.00019104938271604938, 'epoch': 0.14}
{'loss': 3.009, 'grad_norm': 8.140852928161621, 'learning_rate': 0.00019098765432098766, 'epoch': 0.14}
{'loss': 3.7824, 'grad_norm': 7.059525489807129, 'learning_rate': 0.00019092592592592593, 'epoch': 0.14}
{'loss': 3.5732, 'grad_norm': 6.925453186035156, 'learning_rate': 0.0001908641975308642, 'epoch': 0.14}
{'loss': 2.8527, 'grad_norm': 5.502496719360352, 'learning_rate': 0.00019080246913580248, 'epoch': 0.14}
{'loss': 3.3889, 'grad_norm': 7.720320224761963, 'learning_rate': 0.00019074074074074075, 'epoch': 0.14}
{'loss': 3.6471, 'grad_norm': 6.672675609588623, 'learning_rate': 0.00019067901234567903, 'epoch': 0.14}
{'loss': 3.3882, 'grad_norm': 7.745292663574219, 'learning_rate': 0.0001906172839506173, 'epoch': 0.14}
{'loss': 3.5976, 'grad_norm': 13.076911926269531, 'learning_rate': 0.00019055555555555555, 'epoch': 0.14}
{'loss': 3.5502, 'grad_norm': 7.924073696136475, 'learning_rate': 0.00019049382716049385, 'epoch': 0.14}
{'loss': 3.2988, 'grad_norm': 15.46238899230957, 'learning_rate': 0.00019043209876543212, 'epoch': 0.14}
{'loss': 2.88, 'grad_norm': 4.846343994140625, 'learning_rate': 0.00019037037037037037, 'epoch': 0.15}
{'loss': 2.6627, 'grad_norm': 5.081417560577393, 'learning_rate': 0.00019030864197530864, 'epoch': 0.15}
{'loss': 3.1335, 'grad_norm': 7.348114967346191, 'learning_rate': 0.00019024691358024694, 'epoch': 0.15}
{'loss': 3.4681, 'grad_norm': 4.506019592285156, 'learning_rate': 0.0001901851851851852, 'epoch': 0.15}
{'loss': 3.4211, 'grad_norm': 11.233297348022461, 'learning_rate': 0.00019012345679012346, 'epoch': 0.15}
{'loss': 3.009, 'grad_norm': 9.600447654724121, 'learning_rate': 0.00019006172839506174, 'epoch': 0.15}
{'loss': 2.7496, 'grad_norm': 5.48635196685791, 'learning_rate': 0.00019, 'epoch': 0.15}
{'loss': 3.6443, 'grad_norm': 5.65532112121582, 'learning_rate': 0.00018993827160493828, 'epoch': 0.15}
{'loss': 3.0961, 'grad_norm': 7.7192769050598145, 'learning_rate': 0.00018987654320987656, 'epoch': 0.15}
{'loss': 3.3692, 'grad_norm': 10.094099998474121, 'learning_rate': 0.00018981481481481483, 'epoch': 0.15}
{'loss': 3.0703, 'grad_norm': 10.583879470825195, 'learning_rate': 0.00018975308641975308, 'epoch': 0.15}
{'loss': 2.9706, 'grad_norm': 8.621075630187988, 'learning_rate': 0.00018969135802469135, 'epoch': 0.16}
{'loss': 3.5782, 'grad_norm': 9.302000045776367, 'learning_rate': 0.00018962962962962965, 'epoch': 0.16}
{'loss': 3.4426, 'grad_norm': 6.4426093101501465, 'learning_rate': 0.0001895679012345679, 'epoch': 0.16}
{'loss': 3.305, 'grad_norm': 7.663449287414551, 'learning_rate': 0.00018950617283950617, 'epoch': 0.16}
{'loss': 2.7485, 'grad_norm': 5.62996244430542, 'learning_rate': 0.00018944444444444445, 'epoch': 0.16}
{'loss': 3.8302, 'grad_norm': 6.571327209472656, 'learning_rate': 0.00018938271604938275, 'epoch': 0.16}
{'loss': 2.8891, 'grad_norm': 20.924728393554688, 'learning_rate': 0.000189320987654321, 'epoch': 0.16}
{'loss': 3.049, 'grad_norm': 6.5639967918396, 'learning_rate': 0.00018925925925925927, 'epoch': 0.16}
{'loss': 3.0571, 'grad_norm': 10.655790328979492, 'learning_rate': 0.00018919753086419754, 'epoch': 0.16}
{'loss': 3.6047, 'grad_norm': 6.432441711425781, 'learning_rate': 0.00018913580246913582, 'epoch': 0.16}
{'loss': 3.5106, 'grad_norm': 6.358458518981934, 'learning_rate': 0.0001890740740740741, 'epoch': 0.16}
{'loss': 3.6567, 'grad_norm': 8.543269157409668, 'learning_rate': 0.00018901234567901236, 'epoch': 0.17}
{'loss': 3.614, 'grad_norm': 13.037322044372559, 'learning_rate': 0.00018895061728395064, 'epoch': 0.17}
{'loss': 3.1508, 'grad_norm': 8.585877418518066, 'learning_rate': 0.00018888888888888888, 'epoch': 0.17}
{'loss': 3.8632, 'grad_norm': 10.645017623901367, 'learning_rate': 0.00018882716049382716, 'epoch': 0.17}
{'loss': 3.5183, 'grad_norm': 12.365767478942871, 'learning_rate': 0.00018876543209876546, 'epoch': 0.17}
{'loss': 4.1479, 'grad_norm': 5.931811332702637, 'learning_rate': 0.0001887037037037037, 'epoch': 0.17}
{'loss': 3.2924, 'grad_norm': 8.587997436523438, 'learning_rate': 0.00018864197530864198, 'epoch': 0.17}
{'loss': 3.329, 'grad_norm': 8.708208084106445, 'learning_rate': 0.00018858024691358025, 'epoch': 0.17}
{'loss': 2.8019, 'grad_norm': 4.201338291168213, 'learning_rate': 0.00018851851851851853, 'epoch': 0.17}
{'loss': 2.5385, 'grad_norm': 10.431682586669922, 'learning_rate': 0.0001884567901234568, 'epoch': 0.17}
{'loss': 3.2604, 'grad_norm': 8.472437858581543, 'learning_rate': 0.00018839506172839507, 'epoch': 0.17}
{'loss': 3.2205, 'grad_norm': 5.778090000152588, 'learning_rate': 0.00018833333333333335, 'epoch': 0.18}
{'loss': 3.4875, 'grad_norm': 5.186820983886719, 'learning_rate': 0.0001882716049382716, 'epoch': 0.18}
{'loss': 3.6464, 'grad_norm': 4.4121904373168945, 'learning_rate': 0.0001882098765432099, 'epoch': 0.18}
{'loss': 2.9673, 'grad_norm': 5.268876552581787, 'learning_rate': 0.00018814814814814817, 'epoch': 0.18}
{'loss': 3.0615, 'grad_norm': 5.49829626083374, 'learning_rate': 0.00018808641975308642, 'epoch': 0.18}
{'loss': 3.0957, 'grad_norm': 6.365408420562744, 'learning_rate': 0.0001880246913580247, 'epoch': 0.18}
{'loss': 2.7549, 'grad_norm': 16.33177375793457, 'learning_rate': 0.00018796296296296296, 'epoch': 0.18}
{'loss': 3.5528, 'grad_norm': 5.783020973205566, 'learning_rate': 0.00018790123456790126, 'epoch': 0.18}
{'loss': 2.7351, 'grad_norm': 9.363245010375977, 'learning_rate': 0.0001878395061728395, 'epoch': 0.18}
{'loss': 3.407, 'grad_norm': 8.958420753479004, 'learning_rate': 0.00018777777777777779, 'epoch': 0.18}
{'loss': 3.7643, 'grad_norm': 6.202800273895264, 'learning_rate': 0.00018771604938271606, 'epoch': 0.19}
  warnings.warn(                                  
{'eval_loss': 2.9911866188049316, 'eval_runtime': 412.861, 'eval_samples_per_second': 1.453, 'eval_steps_per_second': 1.453, 'epoch': 0.19}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.1063, 'grad_norm': 5.128060340881348, 'learning_rate': 0.00018765432098765433, 'epoch': 0.19}
{'loss': 3.0866, 'grad_norm': 8.030401229858398, 'learning_rate': 0.0001875925925925926, 'epoch': 0.19}
{'loss': 3.6105, 'grad_norm': 13.497366905212402, 'learning_rate': 0.00018753086419753088, 'epoch': 0.19}
{'loss': 3.954, 'grad_norm': 4.225550174713135, 'learning_rate': 0.00018746913580246915, 'epoch': 0.19}
{'loss': 3.4269, 'grad_norm': 4.522258281707764, 'learning_rate': 0.0001874074074074074, 'epoch': 0.19}
{'loss': 3.2218, 'grad_norm': 5.821231842041016, 'learning_rate': 0.0001873456790123457, 'epoch': 0.19}
{'loss': 3.9139, 'grad_norm': 5.732687950134277, 'learning_rate': 0.00018728395061728398, 'epoch': 0.19}
{'loss': 2.6739, 'grad_norm': 6.476191520690918, 'learning_rate': 0.00018722222222222222, 'epoch': 0.19}
{'loss': 3.2719, 'grad_norm': 5.6820478439331055, 'learning_rate': 0.0001871604938271605, 'epoch': 0.19}
{'loss': 3.1533, 'grad_norm': 8.23654556274414, 'learning_rate': 0.00018709876543209877, 'epoch': 0.19}
{'loss': 2.969, 'grad_norm': 6.433566570281982, 'learning_rate': 0.00018703703703703704, 'epoch': 0.2}
{'loss': 3.8621, 'grad_norm': 5.268479347229004, 'learning_rate': 0.00018697530864197532, 'epoch': 0.2}
{'loss': 4.2456, 'grad_norm': 5.242626667022705, 'learning_rate': 0.0001869135802469136, 'epoch': 0.2}
{'loss': 3.0113, 'grad_norm': 6.298603534698486, 'learning_rate': 0.00018685185185185187, 'epoch': 0.2}
{'loss': 3.9879, 'grad_norm': 14.892535209655762, 'learning_rate': 0.0001867901234567901, 'epoch': 0.2}
{'loss': 3.2103, 'grad_norm': 6.1169304847717285, 'learning_rate': 0.0001867283950617284, 'epoch': 0.2}
{'loss': 3.0339, 'grad_norm': 7.731891632080078, 'learning_rate': 0.0001866666666666667, 'epoch': 0.2}
{'loss': 3.3947, 'grad_norm': 7.0089111328125, 'learning_rate': 0.00018660493827160493, 'epoch': 0.2}
{'loss': 3.1688, 'grad_norm': 5.184998512268066, 'learning_rate': 0.0001865432098765432, 'epoch': 0.2}
{'loss': 3.1275, 'grad_norm': 5.5753493309021, 'learning_rate': 0.0001864814814814815, 'epoch': 0.2}
{'loss': 3.0658, 'grad_norm': 13.341964721679688, 'learning_rate': 0.00018641975308641978, 'epoch': 0.2}
{'loss': 3.5757, 'grad_norm': 7.869340419769287, 'learning_rate': 0.00018635802469135803, 'epoch': 0.21}
{'loss': 3.4016, 'grad_norm': 6.3000054359436035, 'learning_rate': 0.0001862962962962963, 'epoch': 0.21}
{'loss': 3.732, 'grad_norm': 4.754044055938721, 'learning_rate': 0.00018623456790123458, 'epoch': 0.21}
{'loss': 3.4943, 'grad_norm': 11.526768684387207, 'learning_rate': 0.00018617283950617285, 'epoch': 0.21}
{'loss': 3.5302, 'grad_norm': 9.061134338378906, 'learning_rate': 0.00018611111111111112, 'epoch': 0.21}
{'loss': 2.6858, 'grad_norm': 7.2419843673706055, 'learning_rate': 0.0001860493827160494, 'epoch': 0.21}
{'loss': 2.7967, 'grad_norm': 5.2246413230896, 'learning_rate': 0.00018598765432098767, 'epoch': 0.21}
{'loss': 2.9792, 'grad_norm': 6.952796936035156, 'learning_rate': 0.00018592592592592592, 'epoch': 0.21}
{'loss': 2.9648, 'grad_norm': 5.18582820892334, 'learning_rate': 0.00018586419753086422, 'epoch': 0.21}
{'loss': 3.1145, 'grad_norm': 4.740078926086426, 'learning_rate': 0.0001858024691358025, 'epoch': 0.21}
{'loss': 3.3021, 'grad_norm': 6.588246822357178, 'learning_rate': 0.00018574074074074074, 'epoch': 0.21}
{'loss': 2.9493, 'grad_norm': 4.899299621582031, 'learning_rate': 0.000185679012345679, 'epoch': 0.22}
{'loss': 3.0002, 'grad_norm': 5.713747024536133, 'learning_rate': 0.0001856172839506173, 'epoch': 0.22}
{'loss': 3.6392, 'grad_norm': 5.56520414352417, 'learning_rate': 0.00018555555555555556, 'epoch': 0.22}
{'loss': 3.929, 'grad_norm': 5.8960676193237305, 'learning_rate': 0.00018549382716049383, 'epoch': 0.22}
{'loss': 3.0052, 'grad_norm': 4.966411113739014, 'learning_rate': 0.0001854320987654321, 'epoch': 0.22}
{'loss': 3.0502, 'grad_norm': 10.226622581481934, 'learning_rate': 0.00018537037037037038, 'epoch': 0.22}
{'loss': 3.2816, 'grad_norm': 4.126611709594727, 'learning_rate': 0.00018530864197530866, 'epoch': 0.22}
{'loss': 3.0915, 'grad_norm': 12.623927116394043, 'learning_rate': 0.00018524691358024693, 'epoch': 0.22}
{'loss': 2.8909, 'grad_norm': 5.439854145050049, 'learning_rate': 0.0001851851851851852, 'epoch': 0.22}
{'loss': 3.3593, 'grad_norm': 5.0664262771606445, 'learning_rate': 0.00018512345679012345, 'epoch': 0.22}
{'loss': 2.8229, 'grad_norm': 6.551376819610596, 'learning_rate': 0.00018506172839506172, 'epoch': 0.23}
{'loss': 3.2565, 'grad_norm': 7.364134311676025, 'learning_rate': 0.00018500000000000002, 'epoch': 0.23}
{'loss': 2.7307, 'grad_norm': 4.79791259765625, 'learning_rate': 0.00018493827160493827, 'epoch': 0.23}
{'loss': 3.1408, 'grad_norm': 6.871981143951416, 'learning_rate': 0.00018487654320987654, 'epoch': 0.23}
{'loss': 3.088, 'grad_norm': 5.47735071182251, 'learning_rate': 0.00018481481481481482, 'epoch': 0.23}
{'loss': 2.8053, 'grad_norm': 4.932888507843018, 'learning_rate': 0.0001847530864197531, 'epoch': 0.23}
{'loss': 3.3223, 'grad_norm': 4.586861610412598, 'learning_rate': 0.00018469135802469137, 'epoch': 0.23}
{'loss': 3.401, 'grad_norm': 4.527710914611816, 'learning_rate': 0.00018462962962962964, 'epoch': 0.23}
{'loss': 3.1785, 'grad_norm': 11.394593238830566, 'learning_rate': 0.00018456790123456791, 'epoch': 0.23}
{'loss': 3.2652, 'grad_norm': 11.33835506439209, 'learning_rate': 0.0001845061728395062, 'epoch': 0.23}
{'loss': 3.1384, 'grad_norm': 8.114599227905273, 'learning_rate': 0.00018444444444444446, 'epoch': 0.23}
{'loss': 2.5333, 'grad_norm': 5.4373040199279785, 'learning_rate': 0.00018438271604938274, 'epoch': 0.24}
{'loss': 2.7176, 'grad_norm': 5.642034530639648, 'learning_rate': 0.000184320987654321, 'epoch': 0.24}
{'loss': 3.715, 'grad_norm': 21.82872200012207, 'learning_rate': 0.00018425925925925926, 'epoch': 0.24}
{'loss': 3.0391, 'grad_norm': 5.829474449157715, 'learning_rate': 0.00018419753086419753, 'epoch': 0.24}
{'loss': 2.7974, 'grad_norm': 5.590964317321777, 'learning_rate': 0.00018413580246913583, 'epoch': 0.24}
{'loss': 2.726, 'grad_norm': 6.116664409637451, 'learning_rate': 0.00018407407407407408, 'epoch': 0.24}
{'loss': 3.015, 'grad_norm': 4.712342262268066, 'learning_rate': 0.00018401234567901235, 'epoch': 0.24}
{'loss': 3.1409, 'grad_norm': 6.929386615753174, 'learning_rate': 0.00018395061728395062, 'epoch': 0.24}
{'loss': 2.576, 'grad_norm': 8.784074783325195, 'learning_rate': 0.0001838888888888889, 'epoch': 0.24}
{'loss': 2.8293, 'grad_norm': 4.994351863861084, 'learning_rate': 0.00018382716049382717, 'epoch': 0.24}
{'loss': 3.294, 'grad_norm': 5.0443434715271, 'learning_rate': 0.00018376543209876545, 'epoch': 0.24}
{'loss': 4.0351, 'grad_norm': 8.64999771118164, 'learning_rate': 0.00018370370370370372, 'epoch': 0.25}
{'loss': 3.3749, 'grad_norm': 6.504336833953857, 'learning_rate': 0.00018364197530864197, 'epoch': 0.25}
{'loss': 3.2747, 'grad_norm': 12.726982116699219, 'learning_rate': 0.00018358024691358024, 'epoch': 0.25}
{'loss': 2.8643, 'grad_norm': 4.5813798904418945, 'learning_rate': 0.00018351851851851854, 'epoch': 0.25}
{'loss': 2.5607, 'grad_norm': 4.126575469970703, 'learning_rate': 0.0001834567901234568, 'epoch': 0.25}
{'loss': 2.8855, 'grad_norm': 9.124002456665039, 'learning_rate': 0.00018339506172839506, 'epoch': 0.25}
{'loss': 3.3759, 'grad_norm': 5.395801067352295, 'learning_rate': 0.00018333333333333334, 'epoch': 0.25}
{'loss': 3.7628, 'grad_norm': 8.634346008300781, 'learning_rate': 0.00018327160493827164, 'epoch': 0.25}
{'loss': 3.1561, 'grad_norm': 5.316288948059082, 'learning_rate': 0.00018320987654320988, 'epoch': 0.25}
{'loss': 3.0706, 'grad_norm': 5.52078914642334, 'learning_rate': 0.00018314814814814816, 'epoch': 0.25}
{'loss': 2.8348, 'grad_norm': 7.958326816558838, 'learning_rate': 0.00018308641975308643, 'epoch': 0.25}
{'loss': 2.5639, 'grad_norm': 3.9837722778320312, 'learning_rate': 0.0001830246913580247, 'epoch': 0.26}
{'loss': 3.6565, 'grad_norm': 7.105870246887207, 'learning_rate': 0.00018296296296296298, 'epoch': 0.26}
{'loss': 3.4802, 'grad_norm': 8.31205940246582, 'learning_rate': 0.00018290123456790125, 'epoch': 0.26}
{'loss': 2.8636, 'grad_norm': 5.116549491882324, 'learning_rate': 0.00018283950617283953, 'epoch': 0.26}
{'loss': 3.3407, 'grad_norm': 6.968352317810059, 'learning_rate': 0.00018277777777777777, 'epoch': 0.26}
{'loss': 4.0326, 'grad_norm': 9.921187400817871, 'learning_rate': 0.00018271604938271605, 'epoch': 0.26}
{'loss': 2.8336, 'grad_norm': 6.457816123962402, 'learning_rate': 0.00018265432098765435, 'epoch': 0.26}
{'loss': 2.8374, 'grad_norm': 7.150948524475098, 'learning_rate': 0.0001825925925925926, 'epoch': 0.26}
{'loss': 3.0827, 'grad_norm': 9.445368766784668, 'learning_rate': 0.00018253086419753087, 'epoch': 0.26}
{'loss': 3.1502, 'grad_norm': 5.226698875427246, 'learning_rate': 0.00018246913580246914, 'epoch': 0.26}
{'loss': 3.6798, 'grad_norm': 10.845035552978516, 'learning_rate': 0.00018240740740740742, 'epoch': 0.26}
{'loss': 3.2827, 'grad_norm': 8.473514556884766, 'learning_rate': 0.0001823456790123457, 'epoch': 0.27}
{'loss': 3.5014, 'grad_norm': 7.070474624633789, 'learning_rate': 0.00018228395061728396, 'epoch': 0.27}
{'loss': 3.7025, 'grad_norm': 5.461257457733154, 'learning_rate': 0.00018222222222222224, 'epoch': 0.27}
{'loss': 3.4623, 'grad_norm': 4.9968976974487305, 'learning_rate': 0.00018216049382716048, 'epoch': 0.27}
{'loss': 3.0867, 'grad_norm': 9.076019287109375, 'learning_rate': 0.00018209876543209878, 'epoch': 0.27}
{'loss': 3.1145, 'grad_norm': 17.341163635253906, 'learning_rate': 0.00018203703703703706, 'epoch': 0.27}
{'loss': 3.347, 'grad_norm': 8.166594505310059, 'learning_rate': 0.0001819753086419753, 'epoch': 0.27}
{'loss': 3.0074, 'grad_norm': 5.315151691436768, 'learning_rate': 0.00018191358024691358, 'epoch': 0.27}
{'loss': 3.3323, 'grad_norm': 5.042641639709473, 'learning_rate': 0.00018185185185185185, 'epoch': 0.27}
{'loss': 3.0918, 'grad_norm': 8.984641075134277, 'learning_rate': 0.00018179012345679015, 'epoch': 0.27}
{'loss': 2.998, 'grad_norm': 5.8546671867370605, 'learning_rate': 0.0001817283950617284, 'epoch': 0.28}
{'loss': 2.5057, 'grad_norm': 5.247723579406738, 'learning_rate': 0.00018166666666666667, 'epoch': 0.28}
{'loss': 2.9251, 'grad_norm': 13.420929908752441, 'learning_rate': 0.00018160493827160495, 'epoch': 0.28}
{'loss': 3.0622, 'grad_norm': 8.2620267868042, 'learning_rate': 0.00018154320987654322, 'epoch': 0.28}
  warnings.warn(                                  
{'eval_loss': 2.9340219497680664, 'eval_runtime': 413.0152, 'eval_samples_per_second': 1.453, 'eval_steps_per_second': 1.453, 'epoch': 0.28}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.9851, 'grad_norm': 6.2600603103637695, 'learning_rate': 0.0001814814814814815, 'epoch': 0.28}
{'loss': 2.9844, 'grad_norm': 9.853693962097168, 'learning_rate': 0.00018141975308641977, 'epoch': 0.28}
{'loss': 2.8102, 'grad_norm': 6.26645040512085, 'learning_rate': 0.00018135802469135804, 'epoch': 0.28}
{'loss': 4.1056, 'grad_norm': 10.943058013916016, 'learning_rate': 0.0001812962962962963, 'epoch': 0.28}
{'loss': 2.8611, 'grad_norm': 6.139059543609619, 'learning_rate': 0.0001812345679012346, 'epoch': 0.28}
{'loss': 3.1659, 'grad_norm': 4.555965900421143, 'learning_rate': 0.00018117283950617286, 'epoch': 0.28}
{'loss': 2.7038, 'grad_norm': 7.142441272735596, 'learning_rate': 0.0001811111111111111, 'epoch': 0.28}
{'loss': 3.3235, 'grad_norm': 5.79142427444458, 'learning_rate': 0.00018104938271604938, 'epoch': 0.29}
{'loss': 2.7297, 'grad_norm': 5.01455020904541, 'learning_rate': 0.00018098765432098766, 'epoch': 0.29}
{'loss': 3.2649, 'grad_norm': 4.763707637786865, 'learning_rate': 0.00018092592592592593, 'epoch': 0.29}
{'loss': 3.1638, 'grad_norm': 9.371360778808594, 'learning_rate': 0.0001808641975308642, 'epoch': 0.29}
{'loss': 2.8281, 'grad_norm': 6.922119140625, 'learning_rate': 0.00018080246913580248, 'epoch': 0.29}
{'loss': 3.4746, 'grad_norm': 5.181164741516113, 'learning_rate': 0.00018074074074074075, 'epoch': 0.29}
{'loss': 3.173, 'grad_norm': 4.5001220703125, 'learning_rate': 0.000180679012345679, 'epoch': 0.29}
{'loss': 3.1821, 'grad_norm': 4.768124580383301, 'learning_rate': 0.0001806172839506173, 'epoch': 0.29}
{'loss': 3.1416, 'grad_norm': 6.986703395843506, 'learning_rate': 0.00018055555555555557, 'epoch': 0.29}
{'loss': 3.5925, 'grad_norm': 4.834737777709961, 'learning_rate': 0.00018049382716049382, 'epoch': 0.29}
{'loss': 3.5908, 'grad_norm': 6.252139568328857, 'learning_rate': 0.0001804320987654321, 'epoch': 0.29}
{'loss': 2.9212, 'grad_norm': 6.392021179199219, 'learning_rate': 0.0001803703703703704, 'epoch': 0.3}
{'loss': 3.5294, 'grad_norm': 6.8146467208862305, 'learning_rate': 0.00018030864197530867, 'epoch': 0.3}
{'loss': 3.6514, 'grad_norm': 6.343966484069824, 'learning_rate': 0.00018024691358024692, 'epoch': 0.3}
{'loss': 3.3447, 'grad_norm': 3.913264274597168, 'learning_rate': 0.0001801851851851852, 'epoch': 0.3}
{'loss': 2.8949, 'grad_norm': 4.444665431976318, 'learning_rate': 0.00018012345679012346, 'epoch': 0.3}
{'loss': 2.7699, 'grad_norm': 7.935741901397705, 'learning_rate': 0.00018006172839506174, 'epoch': 0.3}
{'loss': 3.2603, 'grad_norm': 5.890599250793457, 'learning_rate': 0.00018, 'epoch': 0.3}
{'loss': 2.7116, 'grad_norm': 3.668128490447998, 'learning_rate': 0.00017993827160493829, 'epoch': 0.3}
{'loss': 3.569, 'grad_norm': 6.470262050628662, 'learning_rate': 0.00017987654320987656, 'epoch': 0.3}
{'loss': 3.3172, 'grad_norm': 5.012662887573242, 'learning_rate': 0.0001798148148148148, 'epoch': 0.3}
{'loss': 4.2214, 'grad_norm': 11.054760932922363, 'learning_rate': 0.0001797530864197531, 'epoch': 0.3}
{'loss': 3.9457, 'grad_norm': 8.277628898620605, 'learning_rate': 0.00017969135802469138, 'epoch': 0.31}
{'loss': 3.8847, 'grad_norm': 5.121440887451172, 'learning_rate': 0.00017962962962962963, 'epoch': 0.31}
{'loss': 3.3624, 'grad_norm': 4.554723739624023, 'learning_rate': 0.0001795679012345679, 'epoch': 0.31}
{'loss': 3.3159, 'grad_norm': 6.837127685546875, 'learning_rate': 0.00017950617283950617, 'epoch': 0.31}
{'loss': 3.0681, 'grad_norm': 4.538522243499756, 'learning_rate': 0.00017944444444444445, 'epoch': 0.31}
{'loss': 3.0578, 'grad_norm': 5.502044677734375, 'learning_rate': 0.00017938271604938272, 'epoch': 0.31}
{'loss': 3.4292, 'grad_norm': 4.650655269622803, 'learning_rate': 0.000179320987654321, 'epoch': 0.31}
{'loss': 3.4598, 'grad_norm': 9.685264587402344, 'learning_rate': 0.00017925925925925927, 'epoch': 0.31}
{'loss': 2.555, 'grad_norm': 6.767502784729004, 'learning_rate': 0.00017919753086419754, 'epoch': 0.31}
{'loss': 3.1514, 'grad_norm': 4.109594821929932, 'learning_rate': 0.00017913580246913582, 'epoch': 0.31}
{'loss': 3.1053, 'grad_norm': 7.35987663269043, 'learning_rate': 0.0001790740740740741, 'epoch': 0.31}
{'loss': 3.6717, 'grad_norm': 6.75106954574585, 'learning_rate': 0.00017901234567901234, 'epoch': 0.32}
{'loss': 3.0476, 'grad_norm': 7.874790668487549, 'learning_rate': 0.0001789506172839506, 'epoch': 0.32}
{'loss': 3.3304, 'grad_norm': 4.654252052307129, 'learning_rate': 0.0001788888888888889, 'epoch': 0.32}
{'loss': 3.0221, 'grad_norm': 6.413415431976318, 'learning_rate': 0.00017882716049382719, 'epoch': 0.32}
{'loss': 4.1231, 'grad_norm': 4.524125099182129, 'learning_rate': 0.00017876543209876543, 'epoch': 0.32}
{'loss': 2.9496, 'grad_norm': 14.183981895446777, 'learning_rate': 0.0001787037037037037, 'epoch': 0.32}
{'loss': 3.2168, 'grad_norm': 5.4467854499816895, 'learning_rate': 0.00017864197530864198, 'epoch': 0.32}
{'loss': 3.117, 'grad_norm': 8.004175186157227, 'learning_rate': 0.00017858024691358025, 'epoch': 0.32}
{'loss': 2.8455, 'grad_norm': 4.803330421447754, 'learning_rate': 0.00017851851851851853, 'epoch': 0.32}
{'loss': 3.2313, 'grad_norm': 4.504850387573242, 'learning_rate': 0.0001784567901234568, 'epoch': 0.32}
{'loss': 3.9075, 'grad_norm': 25.422515869140625, 'learning_rate': 0.00017839506172839508, 'epoch': 0.33}
{'loss': 2.4759, 'grad_norm': 5.180099010467529, 'learning_rate': 0.00017833333333333335, 'epoch': 0.33}
{'loss': 2.6085, 'grad_norm': 6.087046146392822, 'learning_rate': 0.00017827160493827162, 'epoch': 0.33}
{'loss': 2.9779, 'grad_norm': 4.878492832183838, 'learning_rate': 0.0001782098765432099, 'epoch': 0.33}
{'loss': 3.5819, 'grad_norm': 6.784118175506592, 'learning_rate': 0.00017814814814814814, 'epoch': 0.33}
{'loss': 3.9327, 'grad_norm': 6.268719673156738, 'learning_rate': 0.00017808641975308642, 'epoch': 0.33}
{'loss': 3.4243, 'grad_norm': 13.020895957946777, 'learning_rate': 0.00017802469135802472, 'epoch': 0.33}
{'loss': 2.3756, 'grad_norm': 10.331538200378418, 'learning_rate': 0.00017796296296296296, 'epoch': 0.33}
{'loss': 2.8006, 'grad_norm': 4.768280506134033, 'learning_rate': 0.00017790123456790124, 'epoch': 0.33}
{'loss': 2.7173, 'grad_norm': 9.001670837402344, 'learning_rate': 0.0001778395061728395, 'epoch': 0.33}
{'loss': 3.1711, 'grad_norm': 5.878404140472412, 'learning_rate': 0.00017777777777777779, 'epoch': 0.33}
{'loss': 2.984, 'grad_norm': 5.399944305419922, 'learning_rate': 0.00017771604938271606, 'epoch': 0.34}
{'loss': 3.4909, 'grad_norm': 4.672999382019043, 'learning_rate': 0.00017765432098765433, 'epoch': 0.34}
{'loss': 3.1704, 'grad_norm': 5.519278049468994, 'learning_rate': 0.0001775925925925926, 'epoch': 0.34}
{'loss': 3.7182, 'grad_norm': 4.191612720489502, 'learning_rate': 0.00017753086419753085, 'epoch': 0.34}
{'loss': 3.3206, 'grad_norm': 3.7511537075042725, 'learning_rate': 0.00017746913580246916, 'epoch': 0.34}
{'loss': 2.8728, 'grad_norm': 5.875454425811768, 'learning_rate': 0.00017740740740740743, 'epoch': 0.34}
{'loss': 3.6111, 'grad_norm': 3.557494878768921, 'learning_rate': 0.00017734567901234568, 'epoch': 0.34}
{'loss': 2.886, 'grad_norm': 8.091696739196777, 'learning_rate': 0.00017728395061728395, 'epoch': 0.34}
{'loss': 2.9275, 'grad_norm': 21.304332733154297, 'learning_rate': 0.00017722222222222222, 'epoch': 0.34}
{'loss': 3.6, 'grad_norm': 5.069077968597412, 'learning_rate': 0.00017716049382716052, 'epoch': 0.34}
{'loss': 2.9614, 'grad_norm': 5.6845502853393555, 'learning_rate': 0.00017709876543209877, 'epoch': 0.34}
{'loss': 2.7923, 'grad_norm': 6.214137554168701, 'learning_rate': 0.00017703703703703704, 'epoch': 0.35}
{'loss': 3.384, 'grad_norm': 4.866084098815918, 'learning_rate': 0.00017697530864197532, 'epoch': 0.35}
{'loss': 3.8189, 'grad_norm': 27.819454193115234, 'learning_rate': 0.0001769135802469136, 'epoch': 0.35}
{'loss': 3.4731, 'grad_norm': 5.89988899230957, 'learning_rate': 0.00017685185185185187, 'epoch': 0.35}
{'loss': 3.0224, 'grad_norm': 3.8406553268432617, 'learning_rate': 0.00017679012345679014, 'epoch': 0.35}
{'loss': 3.2646, 'grad_norm': 4.679866313934326, 'learning_rate': 0.0001767283950617284, 'epoch': 0.35}
{'loss': 3.363, 'grad_norm': 5.769514560699463, 'learning_rate': 0.00017666666666666666, 'epoch': 0.35}
{'loss': 2.9989, 'grad_norm': 8.616857528686523, 'learning_rate': 0.00017660493827160493, 'epoch': 0.35}
{'loss': 3.1802, 'grad_norm': 6.3271260261535645, 'learning_rate': 0.00017654320987654323, 'epoch': 0.35}
{'loss': 3.2199, 'grad_norm': 5.862753391265869, 'learning_rate': 0.00017648148148148148, 'epoch': 0.35}
{'loss': 2.7944, 'grad_norm': 3.7858974933624268, 'learning_rate': 0.00017641975308641976, 'epoch': 0.35}
{'loss': 2.8458, 'grad_norm': 15.3825101852417, 'learning_rate': 0.00017635802469135803, 'epoch': 0.36}
{'loss': 3.3359, 'grad_norm': 4.754457473754883, 'learning_rate': 0.0001762962962962963, 'epoch': 0.36}
{'loss': 3.5614, 'grad_norm': 8.577190399169922, 'learning_rate': 0.00017623456790123458, 'epoch': 0.36}
{'loss': 2.9858, 'grad_norm': 6.883522987365723, 'learning_rate': 0.00017617283950617285, 'epoch': 0.36}
{'loss': 3.277, 'grad_norm': 4.21486759185791, 'learning_rate': 0.00017611111111111112, 'epoch': 0.36}
{'loss': 3.2649, 'grad_norm': 5.4616217613220215, 'learning_rate': 0.00017604938271604937, 'epoch': 0.36}
{'loss': 2.7129, 'grad_norm': 6.182432174682617, 'learning_rate': 0.00017598765432098767, 'epoch': 0.36}
{'loss': 3.0069, 'grad_norm': 6.564855575561523, 'learning_rate': 0.00017592592592592595, 'epoch': 0.36}
{'loss': 2.6929, 'grad_norm': 4.990926742553711, 'learning_rate': 0.0001758641975308642, 'epoch': 0.36}
{'loss': 2.5462, 'grad_norm': 5.064045429229736, 'learning_rate': 0.00017580246913580247, 'epoch': 0.36}
{'loss': 3.8443, 'grad_norm': 6.930077075958252, 'learning_rate': 0.00017574074074074074, 'epoch': 0.36}
{'loss': 3.3145, 'grad_norm': 11.02267837524414, 'learning_rate': 0.00017567901234567904, 'epoch': 0.37}
{'loss': 3.4508, 'grad_norm': 7.838573455810547, 'learning_rate': 0.0001756172839506173, 'epoch': 0.37}
{'loss': 3.8211, 'grad_norm': 5.5070366859436035, 'learning_rate': 0.00017555555555555556, 'epoch': 0.37}
{'loss': 3.3597, 'grad_norm': 6.4407758712768555, 'learning_rate': 0.00017549382716049384, 'epoch': 0.37}
{'loss': 2.6579, 'grad_norm': 5.8401198387146, 'learning_rate': 0.0001754320987654321, 'epoch': 0.37}
{'loss': 3.1462, 'grad_norm': 4.611474990844727, 'learning_rate': 0.00017537037037037038, 'epoch': 0.37}
  warnings.warn(                                  
{'eval_loss': 2.886949062347412, 'eval_runtime': 412.7049, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 1.454, 'epoch': 0.37}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.8712, 'grad_norm': 6.185885906219482, 'learning_rate': 0.00017530864197530866, 'epoch': 0.37}
{'loss': 2.9259, 'grad_norm': 4.91703987121582, 'learning_rate': 0.00017524691358024693, 'epoch': 0.37}
{'loss': 3.1866, 'grad_norm': 3.7977633476257324, 'learning_rate': 0.00017518518518518518, 'epoch': 0.37}
{'loss': 2.8822, 'grad_norm': 4.9095377922058105, 'learning_rate': 0.00017512345679012348, 'epoch': 0.37}
{'loss': 2.7181, 'grad_norm': 5.434573173522949, 'learning_rate': 0.00017506172839506175, 'epoch': 0.38}
{'loss': 2.8763, 'grad_norm': 5.4737772941589355, 'learning_rate': 0.000175, 'epoch': 0.38}
{'loss': 3.0455, 'grad_norm': 4.843019962310791, 'learning_rate': 0.00017493827160493827, 'epoch': 0.38}
{'loss': 4.5938, 'grad_norm': 16.84629249572754, 'learning_rate': 0.00017487654320987655, 'epoch': 0.38}
{'loss': 3.8772, 'grad_norm': 10.180624961853027, 'learning_rate': 0.00017481481481481482, 'epoch': 0.38}
{'loss': 2.5247, 'grad_norm': 5.556023120880127, 'learning_rate': 0.0001747530864197531, 'epoch': 0.38}
{'loss': 3.1654, 'grad_norm': 7.950138092041016, 'learning_rate': 0.00017469135802469137, 'epoch': 0.38}
{'loss': 2.8202, 'grad_norm': 5.566756248474121, 'learning_rate': 0.00017462962962962964, 'epoch': 0.38}
{'loss': 3.2198, 'grad_norm': 7.024324893951416, 'learning_rate': 0.0001745679012345679, 'epoch': 0.38}
{'loss': 3.7852, 'grad_norm': 9.206900596618652, 'learning_rate': 0.0001745061728395062, 'epoch': 0.38}
{'loss': 2.8383, 'grad_norm': 5.782026767730713, 'learning_rate': 0.00017444444444444446, 'epoch': 0.38}
{'loss': 3.7331, 'grad_norm': 4.88012170791626, 'learning_rate': 0.0001743827160493827, 'epoch': 0.39}
{'loss': 3.0336, 'grad_norm': 6.253050804138184, 'learning_rate': 0.00017432098765432098, 'epoch': 0.39}
{'loss': 3.0802, 'grad_norm': 4.641896724700928, 'learning_rate': 0.00017425925925925928, 'epoch': 0.39}
{'loss': 2.2327, 'grad_norm': 8.24530029296875, 'learning_rate': 0.00017419753086419756, 'epoch': 0.39}
{'loss': 3.0447, 'grad_norm': 7.429463863372803, 'learning_rate': 0.0001741358024691358, 'epoch': 0.39}
{'loss': 3.0553, 'grad_norm': 5.260675430297852, 'learning_rate': 0.00017407407407407408, 'epoch': 0.39}
{'loss': 2.598, 'grad_norm': 6.4655351638793945, 'learning_rate': 0.00017401234567901235, 'epoch': 0.39}
{'loss': 3.0312, 'grad_norm': 5.443065643310547, 'learning_rate': 0.00017395061728395063, 'epoch': 0.39}
{'loss': 2.2052, 'grad_norm': 5.539063930511475, 'learning_rate': 0.0001738888888888889, 'epoch': 0.39}
{'loss': 3.5072, 'grad_norm': 9.196955680847168, 'learning_rate': 0.00017382716049382717, 'epoch': 0.39}
{'loss': 2.8849, 'grad_norm': 6.506307601928711, 'learning_rate': 0.00017376543209876545, 'epoch': 0.39}
{'loss': 2.452, 'grad_norm': 6.979432106018066, 'learning_rate': 0.0001737037037037037, 'epoch': 0.4}
{'loss': 2.5786, 'grad_norm': 5.652518272399902, 'learning_rate': 0.000173641975308642, 'epoch': 0.4}
{'loss': 2.1908, 'grad_norm': 7.423426151275635, 'learning_rate': 0.00017358024691358027, 'epoch': 0.4}
{'loss': 3.0163, 'grad_norm': 5.747622966766357, 'learning_rate': 0.00017351851851851851, 'epoch': 0.4}
{'loss': 3.5024, 'grad_norm': 6.350533962249756, 'learning_rate': 0.0001734567901234568, 'epoch': 0.4}
{'loss': 3.6307, 'grad_norm': 8.322837829589844, 'learning_rate': 0.0001733950617283951, 'epoch': 0.4}
{'loss': 3.1714, 'grad_norm': 6.164181232452393, 'learning_rate': 0.0001733456790123457, 'epoch': 0.4}
{'loss': 3.0835, 'grad_norm': 5.786762714385986, 'learning_rate': 0.00017328395061728396, 'epoch': 0.4}
{'loss': 4.4009, 'grad_norm': 9.330338478088379, 'learning_rate': 0.00017322222222222223, 'epoch': 0.4}
{'loss': 3.0351, 'grad_norm': 5.685861110687256, 'learning_rate': 0.0001731604938271605, 'epoch': 0.4}
{'loss': 2.7437, 'grad_norm': 6.901741981506348, 'learning_rate': 0.00017309876543209878, 'epoch': 0.4}
{'loss': 2.9077, 'grad_norm': 6.967805862426758, 'learning_rate': 0.00017303703703703703, 'epoch': 0.41}
{'loss': 2.4525, 'grad_norm': 6.425191879272461, 'learning_rate': 0.0001729753086419753, 'epoch': 0.41}
{'loss': 3.0754, 'grad_norm': 7.875725269317627, 'learning_rate': 0.0001729135802469136, 'epoch': 0.41}
{'loss': 3.4591, 'grad_norm': 11.826153755187988, 'learning_rate': 0.00017285185185185188, 'epoch': 0.41}
{'loss': 2.8001, 'grad_norm': 3.628138780593872, 'learning_rate': 0.00017279012345679012, 'epoch': 0.41}
{'loss': 3.118, 'grad_norm': 6.52279806137085, 'learning_rate': 0.0001727283950617284, 'epoch': 0.41}
{'loss': 2.7696, 'grad_norm': 6.767341136932373, 'learning_rate': 0.00017266666666666667, 'epoch': 0.41}
{'loss': 3.1356, 'grad_norm': 6.5507001876831055, 'learning_rate': 0.00017260493827160495, 'epoch': 0.41}
{'loss': 2.6495, 'grad_norm': 4.953347206115723, 'learning_rate': 0.00017254320987654322, 'epoch': 0.41}
{'loss': 3.6186, 'grad_norm': 6.35308313369751, 'learning_rate': 0.0001724814814814815, 'epoch': 0.41}
{'loss': 2.8428, 'grad_norm': 9.995915412902832, 'learning_rate': 0.00017241975308641977, 'epoch': 0.41}
{'loss': 3.1023, 'grad_norm': 8.097039222717285, 'learning_rate': 0.000172358024691358, 'epoch': 0.42}
{'loss': 2.9541, 'grad_norm': 5.569618225097656, 'learning_rate': 0.00017229629629629631, 'epoch': 0.42}
{'loss': 2.6747, 'grad_norm': 8.843345642089844, 'learning_rate': 0.0001722345679012346, 'epoch': 0.42}
{'loss': 3.1916, 'grad_norm': 5.383183002471924, 'learning_rate': 0.00017217283950617283, 'epoch': 0.42}
{'loss': 3.6246, 'grad_norm': 5.757192134857178, 'learning_rate': 0.0001721111111111111, 'epoch': 0.42}
{'loss': 3.4932, 'grad_norm': 6.801513671875, 'learning_rate': 0.0001720493827160494, 'epoch': 0.42}
{'loss': 3.284, 'grad_norm': 35.420692443847656, 'learning_rate': 0.00017198765432098766, 'epoch': 0.42}
{'loss': 2.9155, 'grad_norm': 6.828913688659668, 'learning_rate': 0.00017192592592592593, 'epoch': 0.42}
{'loss': 2.6753, 'grad_norm': 3.9840543270111084, 'learning_rate': 0.0001718641975308642, 'epoch': 0.42}
{'loss': 2.7043, 'grad_norm': 5.372325420379639, 'learning_rate': 0.00017180246913580248, 'epoch': 0.42}
{'loss': 3.2113, 'grad_norm': 7.505619525909424, 'learning_rate': 0.00017174074074074075, 'epoch': 0.42}
{'loss': 2.6171, 'grad_norm': 4.357440948486328, 'learning_rate': 0.00017167901234567902, 'epoch': 0.43}
{'loss': 2.9226, 'grad_norm': 5.133424282073975, 'learning_rate': 0.0001716172839506173, 'epoch': 0.43}
{'loss': 3.1304, 'grad_norm': 6.674751281738281, 'learning_rate': 0.00017155555555555555, 'epoch': 0.43}
{'loss': 3.3829, 'grad_norm': 4.073583126068115, 'learning_rate': 0.00017149382716049382, 'epoch': 0.43}
{'loss': 3.5373, 'grad_norm': 10.915719032287598, 'learning_rate': 0.00017143209876543212, 'epoch': 0.43}
{'loss': 2.7969, 'grad_norm': 6.6147141456604, 'learning_rate': 0.0001713703703703704, 'epoch': 0.43}
{'loss': 2.7751, 'grad_norm': 16.025264739990234, 'learning_rate': 0.00017130864197530864, 'epoch': 0.43}
{'loss': 3.4184, 'grad_norm': 5.76778507232666, 'learning_rate': 0.00017124691358024691, 'epoch': 0.43}
{'loss': 3.626, 'grad_norm': 11.11812686920166, 'learning_rate': 0.00017118518518518522, 'epoch': 0.43}
{'loss': 2.53, 'grad_norm': 6.493053913116455, 'learning_rate': 0.00017112345679012346, 'epoch': 0.43}
{'loss': 2.7243, 'grad_norm': 8.49417495727539, 'learning_rate': 0.00017106172839506174, 'epoch': 0.44}
{'loss': 2.8888, 'grad_norm': 5.1017889976501465, 'learning_rate': 0.000171, 'epoch': 0.44}
{'loss': 3.3305, 'grad_norm': 5.232516765594482, 'learning_rate': 0.00017093827160493828, 'epoch': 0.44}
{'loss': 2.9059, 'grad_norm': 4.496297359466553, 'learning_rate': 0.00017087654320987656, 'epoch': 0.44}
{'loss': 3.3316, 'grad_norm': 7.070674419403076, 'learning_rate': 0.00017081481481481483, 'epoch': 0.44}
{'loss': 3.3593, 'grad_norm': 4.426258087158203, 'learning_rate': 0.0001707530864197531, 'epoch': 0.44}
{'loss': 2.9097, 'grad_norm': 5.719385147094727, 'learning_rate': 0.00017069135802469135, 'epoch': 0.44}
{'loss': 2.4118, 'grad_norm': 9.654621124267578, 'learning_rate': 0.00017062962962962963, 'epoch': 0.44}
{'loss': 2.8431, 'grad_norm': 4.968163013458252, 'learning_rate': 0.00017056790123456793, 'epoch': 0.44}
{'loss': 2.8729, 'grad_norm': 3.627150297164917, 'learning_rate': 0.00017050617283950617, 'epoch': 0.44}
{'loss': 3.2795, 'grad_norm': 3.5732929706573486, 'learning_rate': 0.00017044444444444445, 'epoch': 0.44}
{'loss': 2.9501, 'grad_norm': 5.101264953613281, 'learning_rate': 0.00017038271604938272, 'epoch': 0.45}
{'loss': 2.8289, 'grad_norm': 6.800990581512451, 'learning_rate': 0.000170320987654321, 'epoch': 0.45}
{'loss': 2.9518, 'grad_norm': 7.579318523406982, 'learning_rate': 0.00017025925925925927, 'epoch': 0.45}
{'loss': 3.0361, 'grad_norm': 6.629129886627197, 'learning_rate': 0.00017019753086419754, 'epoch': 0.45}
{'loss': 2.6681, 'grad_norm': 6.837529182434082, 'learning_rate': 0.00017013580246913582, 'epoch': 0.45}
{'loss': 2.8947, 'grad_norm': 4.648131847381592, 'learning_rate': 0.00017007407407407406, 'epoch': 0.45}
{'loss': 2.7723, 'grad_norm': 5.784599304199219, 'learning_rate': 0.00017001234567901236, 'epoch': 0.45}
{'loss': 2.3996, 'grad_norm': 5.493463039398193, 'learning_rate': 0.00016995061728395064, 'epoch': 0.45}
{'loss': 2.7692, 'grad_norm': 5.125641345977783, 'learning_rate': 0.0001698888888888889, 'epoch': 0.45}
{'loss': 3.2489, 'grad_norm': 6.030919551849365, 'learning_rate': 0.00016982716049382716, 'epoch': 0.45}
{'loss': 3.2752, 'grad_norm': 8.382749557495117, 'learning_rate': 0.00016976543209876543, 'epoch': 0.45}
{'loss': 3.2746, 'grad_norm': 9.885754585266113, 'learning_rate': 0.00016970370370370373, 'epoch': 0.46}
{'loss': 3.6631, 'grad_norm': 12.839265823364258, 'learning_rate': 0.00016964197530864198, 'epoch': 0.46}
{'loss': 3.5121, 'grad_norm': 5.883410453796387, 'learning_rate': 0.00016958024691358025, 'epoch': 0.46}
{'loss': 3.035, 'grad_norm': 5.193885803222656, 'learning_rate': 0.00016951851851851853, 'epoch': 0.46}
{'loss': 3.9642, 'grad_norm': 6.782276630401611, 'learning_rate': 0.0001694567901234568, 'epoch': 0.46}
{'loss': 3.0982, 'grad_norm': 8.262933731079102, 'learning_rate': 0.00016939506172839507, 'epoch': 0.46}
{'loss': 3.5177, 'grad_norm': 40.6054573059082, 'learning_rate': 0.00016933333333333335, 'epoch': 0.46}
{'loss': 2.9268, 'grad_norm': 5.760422706604004, 'learning_rate': 0.00016927160493827162, 'epoch': 0.46}
{'loss': 3.2606, 'grad_norm': 6.973002910614014, 'learning_rate': 0.00016920987654320987, 'epoch': 0.46}
  warnings.warn(                                  
{'eval_loss': 2.8565046787261963, 'eval_runtime': 413.0832, 'eval_samples_per_second': 1.452, 'eval_steps_per_second': 1.452, 'epoch': 0.46}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.5047, 'grad_norm': 4.548501014709473, 'learning_rate': 0.00016914814814814817, 'epoch': 0.46}
{'loss': 3.0751, 'grad_norm': 6.265486240386963, 'learning_rate': 0.00016908641975308644, 'epoch': 0.46}
{'loss': 2.7946, 'grad_norm': 12.601987838745117, 'learning_rate': 0.0001690246913580247, 'epoch': 0.47}
{'loss': 3.2023, 'grad_norm': 9.954782485961914, 'learning_rate': 0.00016896296296296296, 'epoch': 0.47}
{'loss': 3.3505, 'grad_norm': 6.422201633453369, 'learning_rate': 0.00016890123456790124, 'epoch': 0.47}
{'loss': 3.2133, 'grad_norm': 6.5617451667785645, 'learning_rate': 0.0001688395061728395, 'epoch': 0.47}
{'loss': 2.5819, 'grad_norm': 4.928734302520752, 'learning_rate': 0.00016877777777777778, 'epoch': 0.47}
{'loss': 2.5923, 'grad_norm': 6.860991954803467, 'learning_rate': 0.00016871604938271606, 'epoch': 0.47}
{'loss': 3.5128, 'grad_norm': 6.730347156524658, 'learning_rate': 0.00016865432098765433, 'epoch': 0.47}
{'loss': 3.3875, 'grad_norm': 4.77834939956665, 'learning_rate': 0.00016859259259259258, 'epoch': 0.47}
{'loss': 3.2857, 'grad_norm': 5.48303747177124, 'learning_rate': 0.00016853086419753088, 'epoch': 0.47}
{'loss': 2.8027, 'grad_norm': 3.9887704849243164, 'learning_rate': 0.00016846913580246915, 'epoch': 0.47}
{'loss': 2.9681, 'grad_norm': 7.836352825164795, 'learning_rate': 0.00016840740740740743, 'epoch': 0.47}
{'loss': 2.8356, 'grad_norm': 4.1848344802856445, 'learning_rate': 0.00016834567901234567, 'epoch': 0.48}
{'loss': 3.5298, 'grad_norm': 5.7432355880737305, 'learning_rate': 0.00016828395061728397, 'epoch': 0.48}
{'loss': 2.4533, 'grad_norm': 3.592536449432373, 'learning_rate': 0.00016822222222222225, 'epoch': 0.48}
{'loss': 2.8343, 'grad_norm': 13.453079223632812, 'learning_rate': 0.0001681604938271605, 'epoch': 0.48}
{'loss': 3.4803, 'grad_norm': 10.798929214477539, 'learning_rate': 0.00016809876543209877, 'epoch': 0.48}
{'loss': 3.3635, 'grad_norm': 6.9394917488098145, 'learning_rate': 0.00016803703703703704, 'epoch': 0.48}
{'loss': 2.8904, 'grad_norm': 5.172698020935059, 'learning_rate': 0.00016797530864197532, 'epoch': 0.48}
{'loss': 3.3917, 'grad_norm': 6.189842224121094, 'learning_rate': 0.0001679135802469136, 'epoch': 0.48}
{'loss': 3.2987, 'grad_norm': 4.641568183898926, 'learning_rate': 0.00016785185185185186, 'epoch': 0.48}
{'loss': 2.912, 'grad_norm': 7.991298198699951, 'learning_rate': 0.00016779012345679014, 'epoch': 0.48}
{'loss': 3.2529, 'grad_norm': 6.223719596862793, 'learning_rate': 0.00016772839506172838, 'epoch': 0.49}
{'loss': 3.0924, 'grad_norm': 5.719967842102051, 'learning_rate': 0.00016766666666666669, 'epoch': 0.49}
{'loss': 3.339, 'grad_norm': 5.4731831550598145, 'learning_rate': 0.00016760493827160496, 'epoch': 0.49}
{'loss': 3.2531, 'grad_norm': 9.360952377319336, 'learning_rate': 0.0001675432098765432, 'epoch': 0.49}
{'loss': 3.0844, 'grad_norm': 5.704334259033203, 'learning_rate': 0.00016748148148148148, 'epoch': 0.49}
{'loss': 3.0247, 'grad_norm': 8.866912841796875, 'learning_rate': 0.00016741975308641975, 'epoch': 0.49}
{'loss': 2.9472, 'grad_norm': 4.7366790771484375, 'learning_rate': 0.00016735802469135803, 'epoch': 0.49}
{'loss': 2.6587, 'grad_norm': 4.458047389984131, 'learning_rate': 0.0001672962962962963, 'epoch': 0.49}
{'loss': 3.3342, 'grad_norm': 6.083825588226318, 'learning_rate': 0.00016723456790123457, 'epoch': 0.49}
{'loss': 3.2744, 'grad_norm': 6.397294998168945, 'learning_rate': 0.00016717283950617285, 'epoch': 0.49}
{'loss': 2.4238, 'grad_norm': 5.017608642578125, 'learning_rate': 0.00016711111111111112, 'epoch': 0.49}
{'loss': 3.0144, 'grad_norm': 5.4944281578063965, 'learning_rate': 0.0001670493827160494, 'epoch': 0.5}
{'loss': 4.1673, 'grad_norm': 7.401468276977539, 'learning_rate': 0.00016698765432098767, 'epoch': 0.5}
{'loss': 2.8364, 'grad_norm': 4.454735279083252, 'learning_rate': 0.00016692592592592592, 'epoch': 0.5}
{'loss': 3.1786, 'grad_norm': 9.569193840026855, 'learning_rate': 0.0001668641975308642, 'epoch': 0.5}
{'loss': 3.1841, 'grad_norm': 7.194334983825684, 'learning_rate': 0.0001668024691358025, 'epoch': 0.5}
{'loss': 2.8827, 'grad_norm': 5.656721591949463, 'learning_rate': 0.00016674074074074077, 'epoch': 0.5}
{'loss': 3.2326, 'grad_norm': 6.994481086730957, 'learning_rate': 0.000166679012345679, 'epoch': 0.5}
{'loss': 2.7411, 'grad_norm': 5.770854473114014, 'learning_rate': 0.00016661728395061729, 'epoch': 0.5}
{'loss': 3.5066, 'grad_norm': 6.7521162033081055, 'learning_rate': 0.00016655555555555556, 'epoch': 0.5}
{'loss': 3.6094, 'grad_norm': 7.346812725067139, 'learning_rate': 0.00016649382716049383, 'epoch': 0.5}
{'loss': 2.8964, 'grad_norm': 5.687241554260254, 'learning_rate': 0.0001664320987654321, 'epoch': 0.5}
{'loss': 2.7471, 'grad_norm': 5.496464729309082, 'learning_rate': 0.00016637037037037038, 'epoch': 0.51}
{'loss': 2.6969, 'grad_norm': 5.859839916229248, 'learning_rate': 0.00016630864197530865, 'epoch': 0.51}
{'loss': 3.1516, 'grad_norm': 4.853349208831787, 'learning_rate': 0.00016624691358024693, 'epoch': 0.51}
{'loss': 3.3483, 'grad_norm': 4.15739631652832, 'learning_rate': 0.0001661851851851852, 'epoch': 0.51}
{'loss': 3.0171, 'grad_norm': 5.561861515045166, 'learning_rate': 0.00016612345679012348, 'epoch': 0.51}
{'loss': 2.9711, 'grad_norm': 6.157247543334961, 'learning_rate': 0.00016606172839506172, 'epoch': 0.51}
{'loss': 3.0692, 'grad_norm': 6.473584175109863, 'learning_rate': 0.000166, 'epoch': 0.51}
{'loss': 2.7525, 'grad_norm': 4.9371771812438965, 'learning_rate': 0.0001659382716049383, 'epoch': 0.51}
{'loss': 3.6875, 'grad_norm': 5.768684387207031, 'learning_rate': 0.00016587654320987654, 'epoch': 0.51}
{'loss': 3.4725, 'grad_norm': 6.113589763641357, 'learning_rate': 0.00016581481481481482, 'epoch': 0.51}
{'loss': 2.6829, 'grad_norm': 4.840065002441406, 'learning_rate': 0.0001657530864197531, 'epoch': 0.51}
{'loss': 2.9435, 'grad_norm': 4.21586799621582, 'learning_rate': 0.00016569135802469137, 'epoch': 0.52}
{'loss': 2.8364, 'grad_norm': 4.841678142547607, 'learning_rate': 0.00016562962962962964, 'epoch': 0.52}
{'loss': 2.975, 'grad_norm': 5.652780055999756, 'learning_rate': 0.0001655679012345679, 'epoch': 0.52}
{'loss': 3.1288, 'grad_norm': 5.399834632873535, 'learning_rate': 0.0001655061728395062, 'epoch': 0.52}
{'loss': 2.7957, 'grad_norm': 4.642181873321533, 'learning_rate': 0.00016544444444444443, 'epoch': 0.52}
{'loss': 2.5104, 'grad_norm': 4.05488920211792, 'learning_rate': 0.0001653827160493827, 'epoch': 0.52}
{'loss': 4.4038, 'grad_norm': 4.910202980041504, 'learning_rate': 0.000165320987654321, 'epoch': 0.52}
{'loss': 2.9168, 'grad_norm': 11.196625709533691, 'learning_rate': 0.00016525925925925928, 'epoch': 0.52}
{'loss': 2.6822, 'grad_norm': 9.532586097717285, 'learning_rate': 0.00016519753086419753, 'epoch': 0.52}
{'loss': 3.1808, 'grad_norm': 4.117949962615967, 'learning_rate': 0.0001651358024691358, 'epoch': 0.52}
{'loss': 3.2897, 'grad_norm': 4.744349479675293, 'learning_rate': 0.0001650740740740741, 'epoch': 0.53}
{'loss': 3.4577, 'grad_norm': 4.842676639556885, 'learning_rate': 0.00016501234567901235, 'epoch': 0.53}
{'loss': 2.9186, 'grad_norm': 4.123828411102295, 'learning_rate': 0.00016495061728395062, 'epoch': 0.53}
{'loss': 2.6762, 'grad_norm': 6.218874931335449, 'learning_rate': 0.0001648888888888889, 'epoch': 0.53}
{'loss': 3.2387, 'grad_norm': 10.441277503967285, 'learning_rate': 0.00016482716049382717, 'epoch': 0.53}
{'loss': 3.628, 'grad_norm': 6.221649646759033, 'learning_rate': 0.00016476543209876544, 'epoch': 0.53}
{'loss': 3.8274, 'grad_norm': 9.677084922790527, 'learning_rate': 0.00016470370370370372, 'epoch': 0.53}
{'loss': 2.6057, 'grad_norm': 3.7734429836273193, 'learning_rate': 0.000164641975308642, 'epoch': 0.53}
{'loss': 3.9143, 'grad_norm': 6.199069976806641, 'learning_rate': 0.00016458024691358024, 'epoch': 0.53}
{'loss': 3.0224, 'grad_norm': 4.567345142364502, 'learning_rate': 0.0001645185185185185, 'epoch': 0.53}
{'loss': 3.09, 'grad_norm': 6.5607476234436035, 'learning_rate': 0.00016445679012345681, 'epoch': 0.53}
{'loss': 2.6773, 'grad_norm': 7.118706703186035, 'learning_rate': 0.00016439506172839506, 'epoch': 0.54}
{'loss': 2.9616, 'grad_norm': 4.217278480529785, 'learning_rate': 0.00016433333333333333, 'epoch': 0.54}
{'loss': 2.4422, 'grad_norm': 5.72900915145874, 'learning_rate': 0.0001642716049382716, 'epoch': 0.54}
{'loss': 3.1088, 'grad_norm': 5.221161365509033, 'learning_rate': 0.00016420987654320988, 'epoch': 0.54}
{'loss': 2.5845, 'grad_norm': 3.326308012008667, 'learning_rate': 0.00016414814814814816, 'epoch': 0.54}
{'loss': 3.6716, 'grad_norm': 4.7324090003967285, 'learning_rate': 0.00016408641975308643, 'epoch': 0.54}
{'loss': 2.471, 'grad_norm': 4.265687465667725, 'learning_rate': 0.0001640246913580247, 'epoch': 0.54}
{'loss': 3.6504, 'grad_norm': 5.634541988372803, 'learning_rate': 0.00016396296296296295, 'epoch': 0.54}
{'loss': 3.1447, 'grad_norm': 5.521821975708008, 'learning_rate': 0.00016390123456790125, 'epoch': 0.54}
{'loss': 2.8149, 'grad_norm': 4.713127136230469, 'learning_rate': 0.00016383950617283952, 'epoch': 0.54}
{'loss': 2.6932, 'grad_norm': 5.717653274536133, 'learning_rate': 0.0001637777777777778, 'epoch': 0.54}
{'loss': 3.1773, 'grad_norm': 4.859395980834961, 'learning_rate': 0.00016371604938271605, 'epoch': 0.55}
{'loss': 3.6884, 'grad_norm': 5.261291027069092, 'learning_rate': 0.00016365432098765432, 'epoch': 0.55}
{'loss': 2.5623, 'grad_norm': 3.6158218383789062, 'learning_rate': 0.00016359259259259262, 'epoch': 0.55}
{'loss': 2.3138, 'grad_norm': 4.609578609466553, 'learning_rate': 0.00016353086419753087, 'epoch': 0.55}
{'loss': 3.6156, 'grad_norm': 6.141046047210693, 'learning_rate': 0.00016346913580246914, 'epoch': 0.55}
{'loss': 2.9238, 'grad_norm': 8.22814655303955, 'learning_rate': 0.00016340740740740741, 'epoch': 0.55}
{'loss': 2.9221, 'grad_norm': 4.4517645835876465, 'learning_rate': 0.0001633456790123457, 'epoch': 0.55}
{'loss': 2.8462, 'grad_norm': 5.778818130493164, 'learning_rate': 0.00016328395061728396, 'epoch': 0.55}
{'loss': 2.8412, 'grad_norm': 5.501284122467041, 'learning_rate': 0.00016322222222222224, 'epoch': 0.55}
{'loss': 2.5434, 'grad_norm': 3.3213443756103516, 'learning_rate': 0.0001631604938271605, 'epoch': 0.55}
{'loss': 3.2979, 'grad_norm': 9.630114555358887, 'learning_rate': 0.00016309876543209876, 'epoch': 0.55}
{'loss': 2.8858, 'grad_norm': 11.511080741882324, 'learning_rate': 0.00016303703703703706, 'epoch': 0.56}
  warnings.warn(                                  
{'eval_loss': 2.844632625579834, 'eval_runtime': 412.1449, 'eval_samples_per_second': 1.456, 'eval_steps_per_second': 1.456, 'epoch': 0.56}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.1391, 'grad_norm': 4.953110694885254, 'learning_rate': 0.00016297530864197533, 'epoch': 0.56}
{'loss': 2.5553, 'grad_norm': 4.902477741241455, 'learning_rate': 0.00016291358024691358, 'epoch': 0.56}
{'loss': 2.6243, 'grad_norm': 4.80935001373291, 'learning_rate': 0.00016285185185185185, 'epoch': 0.56}
{'loss': 2.7839, 'grad_norm': 8.256823539733887, 'learning_rate': 0.00016279012345679012, 'epoch': 0.56}
{'loss': 2.7875, 'grad_norm': 7.0620551109313965, 'learning_rate': 0.0001627283950617284, 'epoch': 0.56}
{'loss': 2.3371, 'grad_norm': 5.424318790435791, 'learning_rate': 0.00016266666666666667, 'epoch': 0.56}
{'loss': 3.0057, 'grad_norm': 5.8774189949035645, 'learning_rate': 0.00016260493827160495, 'epoch': 0.56}
{'loss': 2.962, 'grad_norm': 6.229370594024658, 'learning_rate': 0.00016254320987654322, 'epoch': 0.56}
{'loss': 3.2245, 'grad_norm': 7.316385746002197, 'learning_rate': 0.00016248148148148147, 'epoch': 0.56}
{'loss': 2.6854, 'grad_norm': 7.525296688079834, 'learning_rate': 0.00016241975308641977, 'epoch': 0.56}
{'loss': 3.2047, 'grad_norm': 7.432187080383301, 'learning_rate': 0.00016235802469135804, 'epoch': 0.57}
{'loss': 3.2941, 'grad_norm': 4.658022880554199, 'learning_rate': 0.00016229629629629632, 'epoch': 0.57}
{'loss': 3.5941, 'grad_norm': 8.408882141113281, 'learning_rate': 0.00016223456790123456, 'epoch': 0.57}
{'loss': 3.2518, 'grad_norm': 6.926242828369141, 'learning_rate': 0.00016217283950617286, 'epoch': 0.57}
{'loss': 3.316, 'grad_norm': 4.451963424682617, 'learning_rate': 0.00016211111111111114, 'epoch': 0.57}
{'loss': 2.4348, 'grad_norm': 5.3809733390808105, 'learning_rate': 0.00016204938271604938, 'epoch': 0.57}
{'loss': 2.8113, 'grad_norm': 4.694788932800293, 'learning_rate': 0.00016198765432098766, 'epoch': 0.57}
{'loss': 3.1397, 'grad_norm': 6.002695560455322, 'learning_rate': 0.00016192592592592593, 'epoch': 0.57}
{'loss': 2.5973, 'grad_norm': 6.143688201904297, 'learning_rate': 0.0001618641975308642, 'epoch': 0.57}
{'loss': 2.6251, 'grad_norm': 4.26063871383667, 'learning_rate': 0.00016180246913580248, 'epoch': 0.57}
{'loss': 2.7754, 'grad_norm': 4.353486061096191, 'learning_rate': 0.00016174074074074075, 'epoch': 0.57}
{'loss': 3.1948, 'grad_norm': 5.283685684204102, 'learning_rate': 0.00016167901234567903, 'epoch': 0.58}
{'loss': 3.4634, 'grad_norm': 5.410098552703857, 'learning_rate': 0.00016161728395061727, 'epoch': 0.58}
{'loss': 2.9852, 'grad_norm': 5.781902313232422, 'learning_rate': 0.00016155555555555557, 'epoch': 0.58}
{'loss': 3.9324, 'grad_norm': 15.72486400604248, 'learning_rate': 0.00016149382716049385, 'epoch': 0.58}
{'loss': 3.4371, 'grad_norm': 4.056273460388184, 'learning_rate': 0.0001614320987654321, 'epoch': 0.58}
{'loss': 2.7791, 'grad_norm': 9.037456512451172, 'learning_rate': 0.00016137037037037037, 'epoch': 0.58}
{'loss': 2.8598, 'grad_norm': 3.9656970500946045, 'learning_rate': 0.00016130864197530864, 'epoch': 0.58}
{'loss': 2.7399, 'grad_norm': 3.5074117183685303, 'learning_rate': 0.00016124691358024692, 'epoch': 0.58}
{'loss': 2.9831, 'grad_norm': 4.8320183753967285, 'learning_rate': 0.0001611851851851852, 'epoch': 0.58}
{'loss': 3.4439, 'grad_norm': 10.599753379821777, 'learning_rate': 0.00016112345679012346, 'epoch': 0.58}
{'loss': 2.4972, 'grad_norm': 4.432032108306885, 'learning_rate': 0.00016106172839506174, 'epoch': 0.59}
{'loss': 3.2737, 'grad_norm': 7.9926910400390625, 'learning_rate': 0.000161, 'epoch': 0.59}
{'loss': 2.9239, 'grad_norm': 6.074434280395508, 'learning_rate': 0.00016093827160493828, 'epoch': 0.59}
{'loss': 3.1903, 'grad_norm': 3.5332283973693848, 'learning_rate': 0.00016087654320987656, 'epoch': 0.59}
{'loss': 3.2511, 'grad_norm': 5.084285736083984, 'learning_rate': 0.00016081481481481483, 'epoch': 0.59}
{'loss': 2.6234, 'grad_norm': 5.074741840362549, 'learning_rate': 0.00016075308641975308, 'epoch': 0.59}
{'loss': 3.0307, 'grad_norm': 4.272864818572998, 'learning_rate': 0.00016069135802469138, 'epoch': 0.59}
{'loss': 2.9749, 'grad_norm': 4.7882914543151855, 'learning_rate': 0.00016062962962962965, 'epoch': 0.59}
{'loss': 2.8413, 'grad_norm': 4.054689407348633, 'learning_rate': 0.0001605679012345679, 'epoch': 0.59}
{'loss': 2.7471, 'grad_norm': 4.886167049407959, 'learning_rate': 0.00016050617283950617, 'epoch': 0.59}
{'loss': 2.8299, 'grad_norm': 5.637815475463867, 'learning_rate': 0.00016044444444444445, 'epoch': 0.59}
{'loss': 3.4923, 'grad_norm': 3.6841418743133545, 'learning_rate': 0.00016038271604938272, 'epoch': 0.6}
{'loss': 3.1882, 'grad_norm': 5.278371810913086, 'learning_rate': 0.000160320987654321, 'epoch': 0.6}
{'loss': 2.7053, 'grad_norm': 4.647726058959961, 'learning_rate': 0.00016025925925925927, 'epoch': 0.6}
{'loss': 3.1567, 'grad_norm': 5.956851005554199, 'learning_rate': 0.00016019753086419754, 'epoch': 0.6}
{'loss': 2.8483, 'grad_norm': 5.202605247497559, 'learning_rate': 0.00016013580246913582, 'epoch': 0.6}
{'loss': 3.0412, 'grad_norm': 6.251481533050537, 'learning_rate': 0.0001600740740740741, 'epoch': 0.6}
{'loss': 2.8611, 'grad_norm': 7.1867756843566895, 'learning_rate': 0.00016001234567901236, 'epoch': 0.6}
{'loss': 3.1516, 'grad_norm': 7.085766315460205, 'learning_rate': 0.0001599506172839506, 'epoch': 0.6}
{'loss': 2.4172, 'grad_norm': 3.779430627822876, 'learning_rate': 0.00015988888888888888, 'epoch': 0.6}
{'loss': 2.4568, 'grad_norm': 3.364518404006958, 'learning_rate': 0.00015982716049382719, 'epoch': 0.6}
{'loss': 2.5408, 'grad_norm': 7.516426086425781, 'learning_rate': 0.00015976543209876543, 'epoch': 0.6}
{'loss': 3.0707, 'grad_norm': 6.378111362457275, 'learning_rate': 0.0001597037037037037, 'epoch': 0.61}
{'loss': 2.2186, 'grad_norm': 6.259640693664551, 'learning_rate': 0.00015964197530864198, 'epoch': 0.61}
{'loss': 2.507, 'grad_norm': 4.177558898925781, 'learning_rate': 0.00015958024691358025, 'epoch': 0.61}
{'loss': 3.2212, 'grad_norm': 7.4219465255737305, 'learning_rate': 0.00015951851851851853, 'epoch': 0.61}
{'loss': 3.3885, 'grad_norm': 9.310404777526855, 'learning_rate': 0.0001594567901234568, 'epoch': 0.61}
{'loss': 2.8153, 'grad_norm': 5.073850631713867, 'learning_rate': 0.00015939506172839507, 'epoch': 0.61}
{'loss': 3.2386, 'grad_norm': 6.602231979370117, 'learning_rate': 0.00015933333333333332, 'epoch': 0.61}
{'loss': 2.7759, 'grad_norm': 5.47509765625, 'learning_rate': 0.00015927160493827162, 'epoch': 0.61}
{'loss': 3.1096, 'grad_norm': 4.454647064208984, 'learning_rate': 0.0001592098765432099, 'epoch': 0.61}
{'loss': 2.5484, 'grad_norm': 7.957827568054199, 'learning_rate': 0.00015914814814814817, 'epoch': 0.61}
{'loss': 2.9744, 'grad_norm': 3.96185040473938, 'learning_rate': 0.00015908641975308642, 'epoch': 0.61}
{'loss': 2.8751, 'grad_norm': 3.903541326522827, 'learning_rate': 0.0001590246913580247, 'epoch': 0.62}
{'loss': 2.5307, 'grad_norm': 4.3643388748168945, 'learning_rate': 0.000158962962962963, 'epoch': 0.62}
{'loss': 3.3989, 'grad_norm': 11.085199356079102, 'learning_rate': 0.00015890123456790124, 'epoch': 0.62}
{'loss': 2.8477, 'grad_norm': 4.537784099578857, 'learning_rate': 0.0001588395061728395, 'epoch': 0.62}
{'loss': 2.5975, 'grad_norm': 3.6399641036987305, 'learning_rate': 0.00015877777777777779, 'epoch': 0.62}
{'loss': 3.3434, 'grad_norm': 9.132728576660156, 'learning_rate': 0.00015871604938271606, 'epoch': 0.62}
{'loss': 2.9604, 'grad_norm': 4.42687463760376, 'learning_rate': 0.00015865432098765433, 'epoch': 0.62}
{'loss': 2.9878, 'grad_norm': 3.911304235458374, 'learning_rate': 0.0001585925925925926, 'epoch': 0.62}
{'loss': 3.1306, 'grad_norm': 6.528487682342529, 'learning_rate': 0.00015853086419753088, 'epoch': 0.62}
{'loss': 2.9367, 'grad_norm': 9.56475830078125, 'learning_rate': 0.00015846913580246913, 'epoch': 0.62}
{'loss': 3.0158, 'grad_norm': 6.7899017333984375, 'learning_rate': 0.0001584074074074074, 'epoch': 0.62}
{'loss': 2.6343, 'grad_norm': 4.518325328826904, 'learning_rate': 0.0001583456790123457, 'epoch': 0.63}
{'loss': 3.4913, 'grad_norm': 10.968609809875488, 'learning_rate': 0.00015828395061728395, 'epoch': 0.63}
{'loss': 2.9221, 'grad_norm': 6.890040874481201, 'learning_rate': 0.00015822222222222222, 'epoch': 0.63}
{'loss': 3.4306, 'grad_norm': 5.818004131317139, 'learning_rate': 0.0001581604938271605, 'epoch': 0.63}
{'loss': 2.5364, 'grad_norm': 4.267908096313477, 'learning_rate': 0.00015809876543209877, 'epoch': 0.63}
{'loss': 2.3226, 'grad_norm': 5.453311920166016, 'learning_rate': 0.00015803703703703704, 'epoch': 0.63}
{'loss': 3.3885, 'grad_norm': 7.130499362945557, 'learning_rate': 0.00015797530864197532, 'epoch': 0.63}
{'loss': 2.1636, 'grad_norm': 3.508327007293701, 'learning_rate': 0.0001579135802469136, 'epoch': 0.63}
{'loss': 3.2369, 'grad_norm': 5.624485969543457, 'learning_rate': 0.00015785185185185184, 'epoch': 0.63}
{'loss': 2.9878, 'grad_norm': 5.060897350311279, 'learning_rate': 0.00015779012345679014, 'epoch': 0.63}
{'loss': 2.7198, 'grad_norm': 5.265519618988037, 'learning_rate': 0.0001577283950617284, 'epoch': 0.64}
{'loss': 3.0503, 'grad_norm': 3.8487136363983154, 'learning_rate': 0.00015766666666666669, 'epoch': 0.64}
{'loss': 2.6493, 'grad_norm': 4.2337727546691895, 'learning_rate': 0.00015760493827160493, 'epoch': 0.64}
{'loss': 2.7416, 'grad_norm': 4.091850757598877, 'learning_rate': 0.0001575432098765432, 'epoch': 0.64}
{'loss': 2.9158, 'grad_norm': 5.057206630706787, 'learning_rate': 0.0001574814814814815, 'epoch': 0.64}
{'loss': 2.0919, 'grad_norm': 5.607822895050049, 'learning_rate': 0.00015741975308641975, 'epoch': 0.64}
{'loss': 3.3416, 'grad_norm': 5.6955084800720215, 'learning_rate': 0.00015735802469135803, 'epoch': 0.64}
{'loss': 2.4931, 'grad_norm': 4.25512170791626, 'learning_rate': 0.0001572962962962963, 'epoch': 0.64}
{'loss': 3.3931, 'grad_norm': 9.226489067077637, 'learning_rate': 0.00015723456790123458, 'epoch': 0.64}
{'loss': 2.6812, 'grad_norm': 3.975039482116699, 'learning_rate': 0.00015717283950617285, 'epoch': 0.64}
{'loss': 2.5785, 'grad_norm': 5.689967632293701, 'learning_rate': 0.00015711111111111112, 'epoch': 0.64}
{'loss': 3.4346, 'grad_norm': 4.944441795349121, 'learning_rate': 0.0001570493827160494, 'epoch': 0.65}
{'loss': 3.0891, 'grad_norm': 7.591249465942383, 'learning_rate': 0.00015698765432098764, 'epoch': 0.65}
{'loss': 3.0798, 'grad_norm': 3.738339424133301, 'learning_rate': 0.00015692592592592594, 'epoch': 0.65}
{'loss': 2.7066, 'grad_norm': 7.214137554168701, 'learning_rate': 0.00015686419753086422, 'epoch': 0.65}
  warnings.warn(                                  
{'eval_loss': 2.827423095703125, 'eval_runtime': 413.8906, 'eval_samples_per_second': 1.45, 'eval_steps_per_second': 1.45, 'epoch': 0.65}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.5761, 'grad_norm': 5.875977993011475, 'learning_rate': 0.00015680246913580247, 'epoch': 0.65}
{'loss': 2.9286, 'grad_norm': 6.754230976104736, 'learning_rate': 0.00015674074074074074, 'epoch': 0.65}
{'loss': 3.3342, 'grad_norm': 3.795288562774658, 'learning_rate': 0.000156679012345679, 'epoch': 0.65}
{'loss': 2.5796, 'grad_norm': 3.520401954650879, 'learning_rate': 0.00015661728395061729, 'epoch': 0.65}
{'loss': 3.3697, 'grad_norm': 4.125729084014893, 'learning_rate': 0.00015655555555555556, 'epoch': 0.65}
{'loss': 2.769, 'grad_norm': 7.34197473526001, 'learning_rate': 0.00015649382716049383, 'epoch': 0.65}
{'loss': 3.1427, 'grad_norm': 5.281707763671875, 'learning_rate': 0.0001564320987654321, 'epoch': 0.65}
{'loss': 3.087, 'grad_norm': 5.079662322998047, 'learning_rate': 0.00015637037037037035, 'epoch': 0.66}
{'loss': 2.6246, 'grad_norm': 5.980893611907959, 'learning_rate': 0.00015630864197530866, 'epoch': 0.66}
{'loss': 2.9846, 'grad_norm': 4.721114158630371, 'learning_rate': 0.00015624691358024693, 'epoch': 0.66}
{'loss': 3.3839, 'grad_norm': 4.290327548980713, 'learning_rate': 0.0001561851851851852, 'epoch': 0.66}
{'loss': 2.8395, 'grad_norm': 6.899056434631348, 'learning_rate': 0.00015612345679012345, 'epoch': 0.66}
{'loss': 3.7768, 'grad_norm': 6.166828632354736, 'learning_rate': 0.00015606172839506175, 'epoch': 0.66}
{'loss': 3.3052, 'grad_norm': 4.974721908569336, 'learning_rate': 0.00015600000000000002, 'epoch': 0.66}
{'loss': 3.2422, 'grad_norm': 6.532190799713135, 'learning_rate': 0.00015593827160493827, 'epoch': 0.66}
{'loss': 3.4646, 'grad_norm': 8.5346097946167, 'learning_rate': 0.00015587654320987654, 'epoch': 0.66}
{'loss': 2.6281, 'grad_norm': 6.276358127593994, 'learning_rate': 0.00015581481481481482, 'epoch': 0.66}
{'loss': 3.184, 'grad_norm': 7.498420238494873, 'learning_rate': 0.0001557530864197531, 'epoch': 0.66}
{'loss': 2.992, 'grad_norm': 5.186938285827637, 'learning_rate': 0.00015569135802469137, 'epoch': 0.67}
{'loss': 2.741, 'grad_norm': 5.477980613708496, 'learning_rate': 0.00015562962962962964, 'epoch': 0.67}
{'loss': 2.8417, 'grad_norm': 10.457611083984375, 'learning_rate': 0.00015556790123456791, 'epoch': 0.67}
{'loss': 3.3962, 'grad_norm': 6.408233165740967, 'learning_rate': 0.00015550617283950616, 'epoch': 0.67}
{'loss': 3.1739, 'grad_norm': 6.066939353942871, 'learning_rate': 0.00015544444444444446, 'epoch': 0.67}
{'loss': 3.19, 'grad_norm': 5.350064277648926, 'learning_rate': 0.00015538271604938274, 'epoch': 0.67}
{'loss': 2.0951, 'grad_norm': 4.143048286437988, 'learning_rate': 0.00015532098765432098, 'epoch': 0.67}
{'loss': 2.6166, 'grad_norm': 4.959865570068359, 'learning_rate': 0.00015525925925925926, 'epoch': 0.67}
{'loss': 2.2208, 'grad_norm': 4.781478404998779, 'learning_rate': 0.00015519753086419756, 'epoch': 0.67}
{'loss': 2.8443, 'grad_norm': 4.581552505493164, 'learning_rate': 0.0001551358024691358, 'epoch': 0.67}
{'loss': 3.039, 'grad_norm': 7.397372722625732, 'learning_rate': 0.00015507407407407408, 'epoch': 0.68}
{'loss': 2.6204, 'grad_norm': 6.129358291625977, 'learning_rate': 0.00015501234567901235, 'epoch': 0.68}
{'loss': 3.2169, 'grad_norm': 4.695938587188721, 'learning_rate': 0.00015495061728395062, 'epoch': 0.68}
{'loss': 3.057, 'grad_norm': 5.589157581329346, 'learning_rate': 0.0001548888888888889, 'epoch': 0.68}
{'loss': 3.1569, 'grad_norm': 7.479938983917236, 'learning_rate': 0.00015482716049382717, 'epoch': 0.68}
{'loss': 3.2031, 'grad_norm': 9.904438018798828, 'learning_rate': 0.00015476543209876545, 'epoch': 0.68}
{'loss': 3.2236, 'grad_norm': 4.583178520202637, 'learning_rate': 0.00015470370370370372, 'epoch': 0.68}
{'loss': 3.1655, 'grad_norm': 6.101853847503662, 'learning_rate': 0.00015464197530864197, 'epoch': 0.68}
{'loss': 2.771, 'grad_norm': 5.438263893127441, 'learning_rate': 0.00015458024691358027, 'epoch': 0.68}
{'loss': 2.7422, 'grad_norm': 3.7961061000823975, 'learning_rate': 0.00015451851851851854, 'epoch': 0.68}
{'loss': 3.3721, 'grad_norm': 4.0535173416137695, 'learning_rate': 0.0001544567901234568, 'epoch': 0.68}
{'loss': 2.4987, 'grad_norm': 5.11932897567749, 'learning_rate': 0.00015439506172839506, 'epoch': 0.69}
{'loss': 3.1148, 'grad_norm': 6.019298553466797, 'learning_rate': 0.00015433333333333334, 'epoch': 0.69}
{'loss': 2.7817, 'grad_norm': 7.208858489990234, 'learning_rate': 0.0001542716049382716, 'epoch': 0.69}
{'loss': 3.5138, 'grad_norm': 3.9156675338745117, 'learning_rate': 0.00015420987654320988, 'epoch': 0.69}
{'loss': 3.8784, 'grad_norm': 6.14569091796875, 'learning_rate': 0.00015414814814814816, 'epoch': 0.69}
{'loss': 2.3948, 'grad_norm': 4.769265174865723, 'learning_rate': 0.00015408641975308643, 'epoch': 0.69}
{'loss': 2.3248, 'grad_norm': 7.1047163009643555, 'learning_rate': 0.0001540246913580247, 'epoch': 0.69}
{'loss': 3.0841, 'grad_norm': 6.141191005706787, 'learning_rate': 0.00015396296296296298, 'epoch': 0.69}
{'loss': 2.9722, 'grad_norm': 6.294900417327881, 'learning_rate': 0.00015390123456790125, 'epoch': 0.69}
{'loss': 3.1012, 'grad_norm': 5.375240802764893, 'learning_rate': 0.0001538395061728395, 'epoch': 0.69}
{'loss': 2.7546, 'grad_norm': 3.8989181518554688, 'learning_rate': 0.00015377777777777777, 'epoch': 0.69}
{'loss': 2.678, 'grad_norm': 7.4195780754089355, 'learning_rate': 0.00015371604938271607, 'epoch': 0.7}
{'loss': 3.1482, 'grad_norm': 5.446403503417969, 'learning_rate': 0.00015365432098765432, 'epoch': 0.7}
{'loss': 2.8862, 'grad_norm': 6.898123741149902, 'learning_rate': 0.0001535925925925926, 'epoch': 0.7}
{'loss': 3.2562, 'grad_norm': 4.566466331481934, 'learning_rate': 0.00015353086419753087, 'epoch': 0.7}
{'loss': 3.0386, 'grad_norm': 4.544894218444824, 'learning_rate': 0.00015346913580246914, 'epoch': 0.7}
{'loss': 2.2983, 'grad_norm': 8.054075241088867, 'learning_rate': 0.00015340740740740741, 'epoch': 0.7}
{'loss': 2.9373, 'grad_norm': 5.110659599304199, 'learning_rate': 0.0001533456790123457, 'epoch': 0.7}
{'loss': 3.2013, 'grad_norm': 4.64119291305542, 'learning_rate': 0.00015328395061728396, 'epoch': 0.7}
{'loss': 4.3739, 'grad_norm': 8.947532653808594, 'learning_rate': 0.0001532222222222222, 'epoch': 0.7}
{'loss': 3.4178, 'grad_norm': 5.510303020477295, 'learning_rate': 0.0001531604938271605, 'epoch': 0.7}
{'loss': 3.3413, 'grad_norm': 4.728631019592285, 'learning_rate': 0.00015309876543209878, 'epoch': 0.7}
{'loss': 2.92, 'grad_norm': 3.684675693511963, 'learning_rate': 0.00015303703703703706, 'epoch': 0.71}
{'loss': 3.0046, 'grad_norm': 9.86213207244873, 'learning_rate': 0.0001529753086419753, 'epoch': 0.71}
{'loss': 3.2046, 'grad_norm': 4.2021484375, 'learning_rate': 0.00015291358024691358, 'epoch': 0.71}
{'loss': 3.4066, 'grad_norm': 5.1225786209106445, 'learning_rate': 0.00015285185185185188, 'epoch': 0.71}
{'loss': 2.8679, 'grad_norm': 4.344208240509033, 'learning_rate': 0.00015279012345679013, 'epoch': 0.71}
{'loss': 3.3507, 'grad_norm': 10.524840354919434, 'learning_rate': 0.0001527283950617284, 'epoch': 0.71}
{'loss': 3.2829, 'grad_norm': 5.784821033477783, 'learning_rate': 0.00015266666666666667, 'epoch': 0.71}
{'loss': 2.5988, 'grad_norm': 9.00137710571289, 'learning_rate': 0.00015260493827160495, 'epoch': 0.71}
{'loss': 2.725, 'grad_norm': 4.879500865936279, 'learning_rate': 0.00015254320987654322, 'epoch': 0.71}
{'loss': 2.7902, 'grad_norm': 4.113624572753906, 'learning_rate': 0.0001524814814814815, 'epoch': 0.71}
{'loss': 3.1077, 'grad_norm': 9.898322105407715, 'learning_rate': 0.00015241975308641977, 'epoch': 0.71}
{'loss': 3.0454, 'grad_norm': 4.744999885559082, 'learning_rate': 0.00015235802469135801, 'epoch': 0.72}
{'loss': 2.7016, 'grad_norm': 6.451904296875, 'learning_rate': 0.00015229629629629632, 'epoch': 0.72}
{'loss': 3.3376, 'grad_norm': 6.2293243408203125, 'learning_rate': 0.0001522345679012346, 'epoch': 0.72}
{'loss': 3.1971, 'grad_norm': 7.203713417053223, 'learning_rate': 0.00015217283950617284, 'epoch': 0.72}
{'loss': 3.3912, 'grad_norm': 5.708430767059326, 'learning_rate': 0.0001521111111111111, 'epoch': 0.72}
{'loss': 3.758, 'grad_norm': 5.277132987976074, 'learning_rate': 0.00015204938271604938, 'epoch': 0.72}
{'loss': 2.9021, 'grad_norm': 5.692667484283447, 'learning_rate': 0.00015198765432098768, 'epoch': 0.72}
{'loss': 3.4152, 'grad_norm': 4.745474338531494, 'learning_rate': 0.00015192592592592593, 'epoch': 0.72}
{'loss': 2.5603, 'grad_norm': 3.708312749862671, 'learning_rate': 0.0001518641975308642, 'epoch': 0.72}
{'loss': 2.7683, 'grad_norm': 5.785922527313232, 'learning_rate': 0.00015180246913580248, 'epoch': 0.72}
{'loss': 3.1051, 'grad_norm': 6.251147270202637, 'learning_rate': 0.00015174074074074073, 'epoch': 0.72}
{'loss': 3.2485, 'grad_norm': 5.09977912902832, 'learning_rate': 0.00015167901234567903, 'epoch': 0.73}
{'loss': 2.8848, 'grad_norm': 8.942157745361328, 'learning_rate': 0.0001516172839506173, 'epoch': 0.73}
{'loss': 3.2697, 'grad_norm': 4.224001407623291, 'learning_rate': 0.00015155555555555557, 'epoch': 0.73}
{'loss': 2.9502, 'grad_norm': 7.131610870361328, 'learning_rate': 0.00015149382716049382, 'epoch': 0.73}
{'loss': 3.0441, 'grad_norm': 4.951596736907959, 'learning_rate': 0.0001514320987654321, 'epoch': 0.73}
{'loss': 2.7504, 'grad_norm': 20.226818084716797, 'learning_rate': 0.0001513703703703704, 'epoch': 0.73}
{'loss': 2.4747, 'grad_norm': 6.630990028381348, 'learning_rate': 0.00015130864197530864, 'epoch': 0.73}
{'loss': 3.0065, 'grad_norm': 3.0566887855529785, 'learning_rate': 0.00015124691358024692, 'epoch': 0.73}
{'loss': 3.3792, 'grad_norm': 7.962925434112549, 'learning_rate': 0.0001511851851851852, 'epoch': 0.73}
{'loss': 2.8507, 'grad_norm': 5.937767028808594, 'learning_rate': 0.00015112345679012346, 'epoch': 0.73}
{'loss': 3.3753, 'grad_norm': 7.740881443023682, 'learning_rate': 0.00015106172839506174, 'epoch': 0.74}
{'loss': 2.7678, 'grad_norm': 3.80126690864563, 'learning_rate': 0.000151, 'epoch': 0.74}
{'loss': 3.2926, 'grad_norm': 9.815840721130371, 'learning_rate': 0.00015093827160493828, 'epoch': 0.74}
{'loss': 2.9059, 'grad_norm': 4.163932800292969, 'learning_rate': 0.00015087654320987653, 'epoch': 0.74}
{'loss': 2.9663, 'grad_norm': 7.09673547744751, 'learning_rate': 0.00015081481481481483, 'epoch': 0.74}
{'loss': 2.8395, 'grad_norm': 5.70171594619751, 'learning_rate': 0.0001507530864197531, 'epoch': 0.74}
{'loss': 2.9094, 'grad_norm': 5.333834648132324, 'learning_rate': 0.00015069135802469135, 'epoch': 0.74}
  warnings.warn(                                  
{'eval_loss': 2.793897867202759, 'eval_runtime': 412.6896, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 1.454, 'epoch': 0.74}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.9624, 'grad_norm': 3.9960544109344482, 'learning_rate': 0.00015062962962962963, 'epoch': 0.74}
{'loss': 2.7664, 'grad_norm': 4.817371845245361, 'learning_rate': 0.0001505679012345679, 'epoch': 0.74}
{'loss': 3.3223, 'grad_norm': 4.597641468048096, 'learning_rate': 0.00015050617283950617, 'epoch': 0.74}
{'loss': 3.4194, 'grad_norm': 5.660142421722412, 'learning_rate': 0.00015044444444444445, 'epoch': 0.74}
{'loss': 3.1266, 'grad_norm': 4.588673114776611, 'learning_rate': 0.00015038271604938272, 'epoch': 0.75}
{'loss': 2.7011, 'grad_norm': 6.871037483215332, 'learning_rate': 0.000150320987654321, 'epoch': 0.75}
{'loss': 2.644, 'grad_norm': 8.824200630187988, 'learning_rate': 0.00015025925925925927, 'epoch': 0.75}
{'loss': 2.8342, 'grad_norm': 5.51090145111084, 'learning_rate': 0.00015019753086419754, 'epoch': 0.75}
{'loss': 2.3993, 'grad_norm': 4.217279434204102, 'learning_rate': 0.00015013580246913582, 'epoch': 0.75}
{'loss': 2.8032, 'grad_norm': 5.074184894561768, 'learning_rate': 0.0001500740740740741, 'epoch': 0.75}
{'loss': 2.7852, 'grad_norm': 4.174840927124023, 'learning_rate': 0.00015001234567901234, 'epoch': 0.75}
{'loss': 3.2449, 'grad_norm': 4.3625030517578125, 'learning_rate': 0.00014995061728395064, 'epoch': 0.75}
{'loss': 2.8741, 'grad_norm': 5.251242160797119, 'learning_rate': 0.0001498888888888889, 'epoch': 0.75}
{'loss': 3.0837, 'grad_norm': 6.926584243774414, 'learning_rate': 0.00014982716049382716, 'epoch': 0.75}
{'loss': 2.8078, 'grad_norm': 5.2098388671875, 'learning_rate': 0.00014976543209876543, 'epoch': 0.75}
{'loss': 2.1449, 'grad_norm': 10.919454574584961, 'learning_rate': 0.0001497037037037037, 'epoch': 0.76}
{'loss': 3.6843, 'grad_norm': 10.476770401000977, 'learning_rate': 0.00014964197530864198, 'epoch': 0.76}
{'loss': 3.7736, 'grad_norm': 9.158740043640137, 'learning_rate': 0.00014958024691358025, 'epoch': 0.76}
{'loss': 2.8198, 'grad_norm': 7.950628280639648, 'learning_rate': 0.00014951851851851853, 'epoch': 0.76}
{'loss': 2.4671, 'grad_norm': 4.651590347290039, 'learning_rate': 0.0001494567901234568, 'epoch': 0.76}
{'loss': 2.9922, 'grad_norm': 3.3779406547546387, 'learning_rate': 0.00014939506172839505, 'epoch': 0.76}
{'loss': 3.0171, 'grad_norm': 3.4413650035858154, 'learning_rate': 0.00014933333333333335, 'epoch': 0.76}
{'loss': 2.665, 'grad_norm': 4.234182834625244, 'learning_rate': 0.00014927160493827162, 'epoch': 0.76}
{'loss': 3.0845, 'grad_norm': 5.811956405639648, 'learning_rate': 0.00014920987654320987, 'epoch': 0.76}
{'loss': 3.24, 'grad_norm': 6.493916034698486, 'learning_rate': 0.00014914814814814814, 'epoch': 0.76}
{'loss': 3.3198, 'grad_norm': 7.229963302612305, 'learning_rate': 0.00014908641975308644, 'epoch': 0.76}
{'loss': 3.2384, 'grad_norm': 7.296105861663818, 'learning_rate': 0.0001490246913580247, 'epoch': 0.77}
{'loss': 3.0493, 'grad_norm': 4.789237976074219, 'learning_rate': 0.00014896296296296296, 'epoch': 0.77}
{'loss': 3.535, 'grad_norm': 4.235842704772949, 'learning_rate': 0.00014890123456790124, 'epoch': 0.77}
{'loss': 3.1082, 'grad_norm': 10.09853744506836, 'learning_rate': 0.0001488395061728395, 'epoch': 0.77}
{'loss': 3.5691, 'grad_norm': 15.420167922973633, 'learning_rate': 0.00014877777777777779, 'epoch': 0.77}
{'loss': 3.8413, 'grad_norm': 5.134814739227295, 'learning_rate': 0.00014871604938271606, 'epoch': 0.77}
{'loss': 3.0791, 'grad_norm': 4.659122467041016, 'learning_rate': 0.00014865432098765433, 'epoch': 0.77}
{'loss': 2.9567, 'grad_norm': 6.525450706481934, 'learning_rate': 0.0001485925925925926, 'epoch': 0.77}
{'loss': 3.8416, 'grad_norm': 4.853729248046875, 'learning_rate': 0.00014853086419753085, 'epoch': 0.77}
{'loss': 2.954, 'grad_norm': 9.241488456726074, 'learning_rate': 0.00014846913580246916, 'epoch': 0.77}
{'loss': 2.7546, 'grad_norm': 4.363748550415039, 'learning_rate': 0.00014840740740740743, 'epoch': 0.78}
{'loss': 2.5992, 'grad_norm': 3.443162679672241, 'learning_rate': 0.00014834567901234568, 'epoch': 0.78}
{'loss': 2.6642, 'grad_norm': 7.943073749542236, 'learning_rate': 0.00014828395061728395, 'epoch': 0.78}
{'loss': 2.9962, 'grad_norm': 8.638729095458984, 'learning_rate': 0.00014822222222222225, 'epoch': 0.78}
{'loss': 2.758, 'grad_norm': 5.51627254486084, 'learning_rate': 0.0001481604938271605, 'epoch': 0.78}
{'loss': 2.9971, 'grad_norm': 5.761147975921631, 'learning_rate': 0.00014809876543209877, 'epoch': 0.78}
{'loss': 2.8788, 'grad_norm': 5.092127799987793, 'learning_rate': 0.00014803703703703704, 'epoch': 0.78}
{'loss': 2.5407, 'grad_norm': 4.453933238983154, 'learning_rate': 0.00014797530864197532, 'epoch': 0.78}
{'loss': 3.2008, 'grad_norm': 3.5021755695343018, 'learning_rate': 0.0001479135802469136, 'epoch': 0.78}
{'loss': 2.4071, 'grad_norm': 8.25377082824707, 'learning_rate': 0.00014785185185185187, 'epoch': 0.78}
{'loss': 3.0336, 'grad_norm': 4.473169326782227, 'learning_rate': 0.00014779012345679014, 'epoch': 0.78}
{'loss': 3.4703, 'grad_norm': 9.188678741455078, 'learning_rate': 0.00014772839506172839, 'epoch': 0.79}
{'loss': 3.2038, 'grad_norm': 3.84399676322937, 'learning_rate': 0.00014766666666666666, 'epoch': 0.79}
{'loss': 3.3085, 'grad_norm': 7.911136627197266, 'learning_rate': 0.00014760493827160496, 'epoch': 0.79}
{'loss': 2.7099, 'grad_norm': 5.1042304039001465, 'learning_rate': 0.0001475432098765432, 'epoch': 0.79}
{'loss': 3.1766, 'grad_norm': 4.14184045791626, 'learning_rate': 0.00014748148148148148, 'epoch': 0.79}
{'loss': 3.0657, 'grad_norm': 4.02318811416626, 'learning_rate': 0.00014741975308641976, 'epoch': 0.79}
{'loss': 3.0221, 'grad_norm': 5.705940246582031, 'learning_rate': 0.00014735802469135803, 'epoch': 0.79}
{'loss': 2.6364, 'grad_norm': 6.049953937530518, 'learning_rate': 0.0001472962962962963, 'epoch': 0.79}
{'loss': 2.6009, 'grad_norm': 4.375154495239258, 'learning_rate': 0.00014723456790123458, 'epoch': 0.79}
{'loss': 3.4031, 'grad_norm': 4.7926506996154785, 'learning_rate': 0.00014717283950617285, 'epoch': 0.79}
{'loss': 2.8005, 'grad_norm': 3.9446887969970703, 'learning_rate': 0.00014711111111111112, 'epoch': 0.79}
{'loss': 3.4447, 'grad_norm': 6.316922664642334, 'learning_rate': 0.0001470493827160494, 'epoch': 0.8}
{'loss': 3.4508, 'grad_norm': 6.715431213378906, 'learning_rate': 0.00014698765432098767, 'epoch': 0.8}
{'loss': 3.1527, 'grad_norm': 9.266366958618164, 'learning_rate': 0.00014692592592592595, 'epoch': 0.8}
{'loss': 2.6952, 'grad_norm': 6.421245098114014, 'learning_rate': 0.0001468641975308642, 'epoch': 0.8}
{'loss': 2.8024, 'grad_norm': 3.945438861846924, 'learning_rate': 0.00014680246913580247, 'epoch': 0.8}
{'loss': 2.9477, 'grad_norm': 3.815415859222412, 'learning_rate': 0.00014674074074074077, 'epoch': 0.8}
{'loss': 2.9756, 'grad_norm': 9.368067741394043, 'learning_rate': 0.000146679012345679, 'epoch': 0.8}
{'loss': 2.9701, 'grad_norm': 5.244173526763916, 'learning_rate': 0.0001466172839506173, 'epoch': 0.8}
{'loss': 3.2256, 'grad_norm': 5.02579927444458, 'learning_rate': 0.00014655555555555556, 'epoch': 0.8}
{'loss': 3.9318, 'grad_norm': 5.35691499710083, 'learning_rate': 0.00014649382716049383, 'epoch': 0.8}
{'loss': 3.7381, 'grad_norm': 7.268657684326172, 'learning_rate': 0.0001464320987654321, 'epoch': 0.8}
{'loss': 2.8898, 'grad_norm': 3.983659267425537, 'learning_rate': 0.00014637037037037038, 'epoch': 0.81}
{'loss': 2.8772, 'grad_norm': 6.670384883880615, 'learning_rate': 0.00014630864197530866, 'epoch': 0.81}
{'loss': 2.8881, 'grad_norm': 3.853816509246826, 'learning_rate': 0.0001462469135802469, 'epoch': 0.81}
{'loss': 3.1727, 'grad_norm': 3.274322271347046, 'learning_rate': 0.0001461851851851852, 'epoch': 0.81}
{'loss': 3.0713, 'grad_norm': 8.30314826965332, 'learning_rate': 0.00014612345679012348, 'epoch': 0.81}
{'loss': 2.8244, 'grad_norm': 3.347440481185913, 'learning_rate': 0.00014606172839506172, 'epoch': 0.81}
{'loss': 2.5242, 'grad_norm': 4.377588272094727, 'learning_rate': 0.000146, 'epoch': 0.81}
{'loss': 2.9, 'grad_norm': 5.189154624938965, 'learning_rate': 0.00014593827160493827, 'epoch': 0.81}
{'loss': 2.4966, 'grad_norm': 6.624817371368408, 'learning_rate': 0.00014587654320987657, 'epoch': 0.81}
{'loss': 2.855, 'grad_norm': 7.2854905128479, 'learning_rate': 0.00014581481481481482, 'epoch': 0.81}
{'loss': 3.7111, 'grad_norm': 7.992575645446777, 'learning_rate': 0.0001457530864197531, 'epoch': 0.81}
{'loss': 3.075, 'grad_norm': 5.801377773284912, 'learning_rate': 0.00014569135802469137, 'epoch': 0.82}
{'loss': 2.6463, 'grad_norm': 5.2991180419921875, 'learning_rate': 0.00014562962962962961, 'epoch': 0.82}
{'loss': 2.7374, 'grad_norm': 6.566559791564941, 'learning_rate': 0.00014556790123456791, 'epoch': 0.82}
{'loss': 2.9646, 'grad_norm': 4.836554527282715, 'learning_rate': 0.0001455061728395062, 'epoch': 0.82}
{'loss': 2.6463, 'grad_norm': 12.990135192871094, 'learning_rate': 0.00014544444444444446, 'epoch': 0.82}
{'loss': 3.7119, 'grad_norm': 6.2302374839782715, 'learning_rate': 0.0001453827160493827, 'epoch': 0.82}
{'loss': 2.9709, 'grad_norm': 5.954987049102783, 'learning_rate': 0.00014532098765432098, 'epoch': 0.82}
{'loss': 2.7838, 'grad_norm': 12.31476879119873, 'learning_rate': 0.00014525925925925928, 'epoch': 0.82}
{'loss': 2.6996, 'grad_norm': 5.2461466789245605, 'learning_rate': 0.00014519753086419753, 'epoch': 0.82}
{'loss': 3.7107, 'grad_norm': 7.130992889404297, 'learning_rate': 0.0001451358024691358, 'epoch': 0.82}
{'loss': 2.6859, 'grad_norm': 4.5909013748168945, 'learning_rate': 0.00014507407407407408, 'epoch': 0.82}
{'loss': 3.1272, 'grad_norm': 6.467410564422607, 'learning_rate': 0.00014501234567901235, 'epoch': 0.83}
{'loss': 3.3032, 'grad_norm': 6.6839752197265625, 'learning_rate': 0.00014495061728395063, 'epoch': 0.83}
{'loss': 3.2862, 'grad_norm': 4.910985946655273, 'learning_rate': 0.0001448888888888889, 'epoch': 0.83}
{'loss': 2.7974, 'grad_norm': 5.324882984161377, 'learning_rate': 0.00014482716049382717, 'epoch': 0.83}
{'loss': 3.1225, 'grad_norm': 4.831127643585205, 'learning_rate': 0.00014476543209876542, 'epoch': 0.83}
{'loss': 2.6817, 'grad_norm': 6.955620765686035, 'learning_rate': 0.00014470370370370372, 'epoch': 0.83}
{'loss': 2.8244, 'grad_norm': 8.637871742248535, 'learning_rate': 0.000144641975308642, 'epoch': 0.83}
{'loss': 3.2215, 'grad_norm': 5.207119464874268, 'learning_rate': 0.00014458024691358024, 'epoch': 0.83}
{'loss': 3.2186, 'grad_norm': 7.290143966674805, 'learning_rate': 0.00014451851851851851, 'epoch': 0.83}
  warnings.warn(                                  
{'eval_loss': 2.797585964202881, 'eval_runtime': 412.0238, 'eval_samples_per_second': 1.456, 'eval_steps_per_second': 1.456, 'epoch': 0.83}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.613, 'grad_norm': 4.18711519241333, 'learning_rate': 0.0001444567901234568, 'epoch': 0.83}
{'loss': 3.1396, 'grad_norm': 4.897027969360352, 'learning_rate': 0.00014439506172839506, 'epoch': 0.84}
{'loss': 3.6916, 'grad_norm': 3.7478344440460205, 'learning_rate': 0.00014433333333333334, 'epoch': 0.84}
{'loss': 3.804, 'grad_norm': 6.050228118896484, 'learning_rate': 0.0001442716049382716, 'epoch': 0.84}
{'loss': 3.1942, 'grad_norm': 4.964255332946777, 'learning_rate': 0.00014420987654320988, 'epoch': 0.84}
{'loss': 3.478, 'grad_norm': 5.435728549957275, 'learning_rate': 0.00014414814814814816, 'epoch': 0.84}
{'loss': 3.013, 'grad_norm': 4.613772392272949, 'learning_rate': 0.00014408641975308643, 'epoch': 0.84}
{'loss': 2.2759, 'grad_norm': 3.9080018997192383, 'learning_rate': 0.0001440246913580247, 'epoch': 0.84}
{'loss': 2.7288, 'grad_norm': 5.439021587371826, 'learning_rate': 0.00014396296296296298, 'epoch': 0.84}
{'loss': 3.651, 'grad_norm': 9.732486724853516, 'learning_rate': 0.00014390123456790123, 'epoch': 0.84}
{'loss': 3.4563, 'grad_norm': 8.877402305603027, 'learning_rate': 0.00014383950617283953, 'epoch': 0.84}
{'loss': 3.361, 'grad_norm': 5.10040807723999, 'learning_rate': 0.0001437777777777778, 'epoch': 0.84}
{'loss': 2.481, 'grad_norm': 5.568819046020508, 'learning_rate': 0.00014371604938271605, 'epoch': 0.85}
{'loss': 3.4118, 'grad_norm': 3.9654810428619385, 'learning_rate': 0.00014365432098765432, 'epoch': 0.85}
{'loss': 2.9529, 'grad_norm': 6.642820358276367, 'learning_rate': 0.0001435925925925926, 'epoch': 0.85}
{'loss': 3.4074, 'grad_norm': 8.139374732971191, 'learning_rate': 0.00014353086419753087, 'epoch': 0.85}
{'loss': 3.0889, 'grad_norm': 6.810214519500732, 'learning_rate': 0.00014346913580246914, 'epoch': 0.85}
{'loss': 2.5928, 'grad_norm': 4.977262020111084, 'learning_rate': 0.00014340740740740742, 'epoch': 0.85}
{'loss': 3.0736, 'grad_norm': 7.297285079956055, 'learning_rate': 0.0001433456790123457, 'epoch': 0.85}
{'loss': 3.1379, 'grad_norm': 9.755061149597168, 'learning_rate': 0.00014328395061728396, 'epoch': 0.85}
{'loss': 3.5006, 'grad_norm': 5.17436408996582, 'learning_rate': 0.00014322222222222224, 'epoch': 0.85}
{'loss': 2.6997, 'grad_norm': 4.6622843742370605, 'learning_rate': 0.0001431604938271605, 'epoch': 0.85}
{'loss': 3.0997, 'grad_norm': 4.651973724365234, 'learning_rate': 0.00014309876543209876, 'epoch': 0.85}
{'loss': 3.3321, 'grad_norm': 5.414169788360596, 'learning_rate': 0.00014303703703703703, 'epoch': 0.86}
{'loss': 2.8844, 'grad_norm': 5.981006145477295, 'learning_rate': 0.00014297530864197533, 'epoch': 0.86}
{'loss': 2.8813, 'grad_norm': 4.669342517852783, 'learning_rate': 0.00014291358024691358, 'epoch': 0.86}
{'loss': 3.3209, 'grad_norm': 4.890692710876465, 'learning_rate': 0.00014285185185185185, 'epoch': 0.86}
{'loss': 3.3088, 'grad_norm': 8.190815925598145, 'learning_rate': 0.00014279012345679013, 'epoch': 0.86}
{'loss': 2.886, 'grad_norm': 4.904009819030762, 'learning_rate': 0.0001427283950617284, 'epoch': 0.86}
{'loss': 3.5749, 'grad_norm': 5.0273118019104, 'learning_rate': 0.00014266666666666667, 'epoch': 0.86}
{'loss': 3.3631, 'grad_norm': 3.9915976524353027, 'learning_rate': 0.00014260493827160495, 'epoch': 0.86}
{'loss': 2.6448, 'grad_norm': 3.489968776702881, 'learning_rate': 0.00014254320987654322, 'epoch': 0.86}
{'loss': 2.9891, 'grad_norm': 5.510855197906494, 'learning_rate': 0.0001424814814814815, 'epoch': 0.86}
{'loss': 2.4692, 'grad_norm': 5.454300880432129, 'learning_rate': 0.00014241975308641974, 'epoch': 0.86}
{'loss': 2.627, 'grad_norm': 5.453793525695801, 'learning_rate': 0.00014235802469135804, 'epoch': 0.87}
{'loss': 2.5344, 'grad_norm': 4.104012966156006, 'learning_rate': 0.00014229629629629632, 'epoch': 0.87}
{'loss': 3.4402, 'grad_norm': 8.856088638305664, 'learning_rate': 0.00014223456790123456, 'epoch': 0.87}
{'loss': 3.1696, 'grad_norm': 3.823575019836426, 'learning_rate': 0.00014217283950617284, 'epoch': 0.87}
{'loss': 3.5866, 'grad_norm': 4.712802410125732, 'learning_rate': 0.00014211111111111114, 'epoch': 0.87}
{'loss': 3.121, 'grad_norm': 6.8478312492370605, 'learning_rate': 0.00014204938271604938, 'epoch': 0.87}
{'loss': 3.2568, 'grad_norm': 5.682523727416992, 'learning_rate': 0.00014198765432098766, 'epoch': 0.87}
{'loss': 2.925, 'grad_norm': 5.069509506225586, 'learning_rate': 0.00014192592592592593, 'epoch': 0.87}
{'loss': 2.956, 'grad_norm': 7.791549205780029, 'learning_rate': 0.0001418641975308642, 'epoch': 0.87}
{'loss': 3.4764, 'grad_norm': 4.6881232261657715, 'learning_rate': 0.00014180246913580248, 'epoch': 0.87}
{'loss': 3.2237, 'grad_norm': 3.771629810333252, 'learning_rate': 0.00014174074074074075, 'epoch': 0.88}
{'loss': 2.8772, 'grad_norm': 4.75786018371582, 'learning_rate': 0.00014167901234567903, 'epoch': 0.88}
{'loss': 2.5842, 'grad_norm': 6.031387805938721, 'learning_rate': 0.00014161728395061727, 'epoch': 0.88}
{'loss': 3.2239, 'grad_norm': 8.682493209838867, 'learning_rate': 0.00014155555555555555, 'epoch': 0.88}
{'loss': 3.4391, 'grad_norm': 6.706668853759766, 'learning_rate': 0.00014149382716049385, 'epoch': 0.88}
{'loss': 2.672, 'grad_norm': 6.0623064041137695, 'learning_rate': 0.0001414320987654321, 'epoch': 0.88}
{'loss': 2.9059, 'grad_norm': 5.117400646209717, 'learning_rate': 0.00014137037037037037, 'epoch': 0.88}
{'loss': 3.4087, 'grad_norm': 6.456783294677734, 'learning_rate': 0.00014130864197530864, 'epoch': 0.88}
{'loss': 4.0476, 'grad_norm': 8.680681228637695, 'learning_rate': 0.00014124691358024694, 'epoch': 0.88}
{'loss': 3.9001, 'grad_norm': 12.570939064025879, 'learning_rate': 0.0001411851851851852, 'epoch': 0.88}
{'loss': 2.4945, 'grad_norm': 5.768484592437744, 'learning_rate': 0.00014112345679012346, 'epoch': 0.88}
{'loss': 3.3186, 'grad_norm': 6.7482008934021, 'learning_rate': 0.00014106172839506174, 'epoch': 0.89}
{'loss': 2.5416, 'grad_norm': 4.769829273223877, 'learning_rate': 0.000141, 'epoch': 0.89}
{'loss': 3.1196, 'grad_norm': 8.001009941101074, 'learning_rate': 0.00014093827160493829, 'epoch': 0.89}
{'loss': 2.6332, 'grad_norm': 5.529943466186523, 'learning_rate': 0.00014087654320987656, 'epoch': 0.89}
{'loss': 3.0969, 'grad_norm': 5.332167148590088, 'learning_rate': 0.00014081481481481483, 'epoch': 0.89}
{'loss': 3.5716, 'grad_norm': 6.3939032554626465, 'learning_rate': 0.00014075308641975308, 'epoch': 0.89}
{'loss': 2.4454, 'grad_norm': 6.091679573059082, 'learning_rate': 0.00014069135802469135, 'epoch': 0.89}
{'loss': 3.3866, 'grad_norm': 6.8925065994262695, 'learning_rate': 0.00014062962962962965, 'epoch': 0.89}
{'loss': 2.9894, 'grad_norm': 4.979087829589844, 'learning_rate': 0.0001405679012345679, 'epoch': 0.89}
{'loss': 2.6683, 'grad_norm': 5.358528137207031, 'learning_rate': 0.00014050617283950618, 'epoch': 0.89}
{'loss': 2.7071, 'grad_norm': 4.765856742858887, 'learning_rate': 0.00014044444444444445, 'epoch': 0.89}
{'loss': 3.2554, 'grad_norm': 5.1320929527282715, 'learning_rate': 0.00014038271604938272, 'epoch': 0.9}
{'loss': 2.3115, 'grad_norm': 4.922436237335205, 'learning_rate': 0.000140320987654321, 'epoch': 0.9}
{'loss': 2.8775, 'grad_norm': 4.842088222503662, 'learning_rate': 0.00014025925925925927, 'epoch': 0.9}
{'loss': 2.6552, 'grad_norm': 4.875040531158447, 'learning_rate': 0.00014019753086419754, 'epoch': 0.9}
{'loss': 3.4443, 'grad_norm': 6.222746849060059, 'learning_rate': 0.0001401358024691358, 'epoch': 0.9}
{'loss': 2.8625, 'grad_norm': 8.657583236694336, 'learning_rate': 0.0001400740740740741, 'epoch': 0.9}
{'loss': 3.0431, 'grad_norm': 5.441398620605469, 'learning_rate': 0.00014001234567901237, 'epoch': 0.9}
{'loss': 2.9654, 'grad_norm': 4.142420768737793, 'learning_rate': 0.0001399506172839506, 'epoch': 0.9}
{'loss': 2.5024, 'grad_norm': 6.055872917175293, 'learning_rate': 0.00013988888888888889, 'epoch': 0.9}
{'loss': 2.3286, 'grad_norm': 4.162294387817383, 'learning_rate': 0.00013982716049382716, 'epoch': 0.9}
{'loss': 2.7278, 'grad_norm': 3.643026113510132, 'learning_rate': 0.00013976543209876546, 'epoch': 0.9}
{'loss': 2.7754, 'grad_norm': 4.124515056610107, 'learning_rate': 0.0001397037037037037, 'epoch': 0.91}
{'loss': 2.9962, 'grad_norm': 5.571829319000244, 'learning_rate': 0.00013964197530864198, 'epoch': 0.91}
{'loss': 3.2962, 'grad_norm': 8.017061233520508, 'learning_rate': 0.00013958024691358025, 'epoch': 0.91}
{'loss': 2.7093, 'grad_norm': 13.985363960266113, 'learning_rate': 0.0001395185185185185, 'epoch': 0.91}
{'loss': 2.4139, 'grad_norm': 5.358932018280029, 'learning_rate': 0.0001394567901234568, 'epoch': 0.91}
{'loss': 3.53, 'grad_norm': 8.737218856811523, 'learning_rate': 0.00013939506172839508, 'epoch': 0.91}
{'loss': 3.0537, 'grad_norm': 8.117655754089355, 'learning_rate': 0.00013933333333333335, 'epoch': 0.91}
{'loss': 2.7102, 'grad_norm': 6.814184188842773, 'learning_rate': 0.0001392716049382716, 'epoch': 0.91}
{'loss': 3.5778, 'grad_norm': 12.46120834350586, 'learning_rate': 0.0001392098765432099, 'epoch': 0.91}
{'loss': 2.9078, 'grad_norm': 10.18100643157959, 'learning_rate': 0.00013914814814814817, 'epoch': 0.91}
{'loss': 3.0329, 'grad_norm': 5.020604133605957, 'learning_rate': 0.00013908641975308642, 'epoch': 0.91}
{'loss': 2.8452, 'grad_norm': 5.584394454956055, 'learning_rate': 0.0001390246913580247, 'epoch': 0.92}
{'loss': 2.0846, 'grad_norm': 4.658310890197754, 'learning_rate': 0.00013896296296296297, 'epoch': 0.92}
{'loss': 2.7795, 'grad_norm': 5.723416328430176, 'learning_rate': 0.00013890123456790124, 'epoch': 0.92}
{'loss': 3.0575, 'grad_norm': 3.2896721363067627, 'learning_rate': 0.0001388395061728395, 'epoch': 0.92}
{'loss': 3.1881, 'grad_norm': 5.6506171226501465, 'learning_rate': 0.0001387777777777778, 'epoch': 0.92}
{'loss': 3.3307, 'grad_norm': 4.109691143035889, 'learning_rate': 0.00013871604938271606, 'epoch': 0.92}
{'loss': 2.592, 'grad_norm': 3.4271304607391357, 'learning_rate': 0.0001386543209876543, 'epoch': 0.92}
{'loss': 3.1946, 'grad_norm': 10.297228813171387, 'learning_rate': 0.0001385925925925926, 'epoch': 0.92}
{'loss': 2.2458, 'grad_norm': 4.041224479675293, 'learning_rate': 0.00013853086419753088, 'epoch': 0.92}
{'loss': 2.9998, 'grad_norm': 6.1531476974487305, 'learning_rate': 0.00013846913580246913, 'epoch': 0.92}
{'loss': 3.3716, 'grad_norm': 7.277291297912598, 'learning_rate': 0.0001384074074074074, 'epoch': 0.93}
{'loss': 3.0154, 'grad_norm': 6.424679756164551, 'learning_rate': 0.00013834567901234568, 'epoch': 0.93}
  warnings.warn(                                  
{'eval_loss': 2.7753002643585205, 'eval_runtime': 412.7848, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 1.454, 'epoch': 0.93}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.5522, 'grad_norm': 4.929193019866943, 'learning_rate': 0.00013828395061728398, 'epoch': 0.93}
{'loss': 2.8771, 'grad_norm': 10.0648775100708, 'learning_rate': 0.00013822222222222222, 'epoch': 0.93}
{'loss': 2.9193, 'grad_norm': 6.90368127822876, 'learning_rate': 0.0001381604938271605, 'epoch': 0.93}
{'loss': 3.074, 'grad_norm': 5.212262153625488, 'learning_rate': 0.00013809876543209877, 'epoch': 0.93}
{'loss': 2.8772, 'grad_norm': 6.206539630889893, 'learning_rate': 0.00013803703703703705, 'epoch': 0.93}
{'loss': 2.4178, 'grad_norm': 3.030203342437744, 'learning_rate': 0.00013797530864197532, 'epoch': 0.93}
{'loss': 2.9775, 'grad_norm': 4.3394951820373535, 'learning_rate': 0.0001379135802469136, 'epoch': 0.93}
{'loss': 2.8206, 'grad_norm': 7.78535270690918, 'learning_rate': 0.00013785185185185187, 'epoch': 0.93}
{'loss': 3.0049, 'grad_norm': 5.197419166564941, 'learning_rate': 0.0001377901234567901, 'epoch': 0.93}
{'loss': 3.3138, 'grad_norm': 9.360462188720703, 'learning_rate': 0.00013772839506172841, 'epoch': 0.94}
{'loss': 2.9465, 'grad_norm': 11.781441688537598, 'learning_rate': 0.0001376666666666667, 'epoch': 0.94}
{'loss': 2.8875, 'grad_norm': 5.2277631759643555, 'learning_rate': 0.00013760493827160493, 'epoch': 0.94}
{'loss': 2.8366, 'grad_norm': 3.3084092140197754, 'learning_rate': 0.0001375432098765432, 'epoch': 0.94}
{'loss': 2.9936, 'grad_norm': 5.979989051818848, 'learning_rate': 0.00013748148148148148, 'epoch': 0.94}
{'loss': 2.9009, 'grad_norm': 3.965432643890381, 'learning_rate': 0.00013741975308641976, 'epoch': 0.94}
{'loss': 3.582, 'grad_norm': 4.837417125701904, 'learning_rate': 0.00013735802469135803, 'epoch': 0.94}
{'loss': 3.1473, 'grad_norm': 4.450867176055908, 'learning_rate': 0.0001372962962962963, 'epoch': 0.94}
{'loss': 3.168, 'grad_norm': 5.067830562591553, 'learning_rate': 0.00013723456790123458, 'epoch': 0.94}
{'loss': 2.3709, 'grad_norm': 5.917957305908203, 'learning_rate': 0.00013717283950617285, 'epoch': 0.94}
{'loss': 2.9735, 'grad_norm': 4.699795246124268, 'learning_rate': 0.00013711111111111113, 'epoch': 0.94}
{'loss': 2.468, 'grad_norm': 5.047936916351318, 'learning_rate': 0.0001370493827160494, 'epoch': 0.95}
{'loss': 3.4319, 'grad_norm': 9.212477684020996, 'learning_rate': 0.00013698765432098765, 'epoch': 0.95}
{'loss': 2.8124, 'grad_norm': 6.397042274475098, 'learning_rate': 0.00013692592592592592, 'epoch': 0.95}
{'loss': 2.7497, 'grad_norm': 4.300695419311523, 'learning_rate': 0.00013686419753086422, 'epoch': 0.95}
{'loss': 3.2144, 'grad_norm': 7.570245265960693, 'learning_rate': 0.00013680246913580247, 'epoch': 0.95}
{'loss': 3.1235, 'grad_norm': 7.719054222106934, 'learning_rate': 0.00013674074074074074, 'epoch': 0.95}
{'loss': 2.7437, 'grad_norm': 11.032346725463867, 'learning_rate': 0.00013667901234567901, 'epoch': 0.95}
{'loss': 2.8451, 'grad_norm': 6.4779486656188965, 'learning_rate': 0.0001366172839506173, 'epoch': 0.95}
{'loss': 3.5085, 'grad_norm': 5.167367935180664, 'learning_rate': 0.00013655555555555556, 'epoch': 0.95}
{'loss': 3.0974, 'grad_norm': 3.440171241760254, 'learning_rate': 0.00013649382716049384, 'epoch': 0.95}
{'loss': 3.1145, 'grad_norm': 3.970534563064575, 'learning_rate': 0.0001364320987654321, 'epoch': 0.95}
{'loss': 2.7051, 'grad_norm': 8.102171897888184, 'learning_rate': 0.00013637037037037038, 'epoch': 0.96}
{'loss': 3.544, 'grad_norm': 7.829708099365234, 'learning_rate': 0.00013630864197530863, 'epoch': 0.96}
{'loss': 2.555, 'grad_norm': 3.6027767658233643, 'learning_rate': 0.00013624691358024693, 'epoch': 0.96}
{'loss': 2.9615, 'grad_norm': 11.131255149841309, 'learning_rate': 0.0001361851851851852, 'epoch': 0.96}
{'loss': 3.2265, 'grad_norm': 6.271632671356201, 'learning_rate': 0.00013612345679012345, 'epoch': 0.96}
{'loss': 3.5124, 'grad_norm': 4.3713788986206055, 'learning_rate': 0.00013606172839506173, 'epoch': 0.96}
{'loss': 2.585, 'grad_norm': 4.555140972137451, 'learning_rate': 0.00013600000000000003, 'epoch': 0.96}
{'loss': 2.5972, 'grad_norm': 6.27398157119751, 'learning_rate': 0.00013593827160493827, 'epoch': 0.96}
{'loss': 2.7289, 'grad_norm': 4.368262767791748, 'learning_rate': 0.00013587654320987655, 'epoch': 0.96}
{'loss': 2.8686, 'grad_norm': 4.073123931884766, 'learning_rate': 0.00013581481481481482, 'epoch': 0.96}
{'loss': 3.7154, 'grad_norm': 10.24350643157959, 'learning_rate': 0.0001357530864197531, 'epoch': 0.96}
{'loss': 2.8603, 'grad_norm': 5.937849998474121, 'learning_rate': 0.00013569135802469137, 'epoch': 0.97}
{'loss': 2.608, 'grad_norm': 4.790041923522949, 'learning_rate': 0.00013562962962962964, 'epoch': 0.97}
{'loss': 2.8545, 'grad_norm': 3.7715132236480713, 'learning_rate': 0.00013556790123456792, 'epoch': 0.97}
{'loss': 3.2658, 'grad_norm': 9.360920906066895, 'learning_rate': 0.00013550617283950616, 'epoch': 0.97}
{'loss': 3.1573, 'grad_norm': 4.386181831359863, 'learning_rate': 0.00013544444444444444, 'epoch': 0.97}
{'loss': 2.8521, 'grad_norm': 5.027073383331299, 'learning_rate': 0.00013538271604938274, 'epoch': 0.97}
{'loss': 2.7881, 'grad_norm': 12.0707368850708, 'learning_rate': 0.00013532098765432098, 'epoch': 0.97}
{'loss': 3.4888, 'grad_norm': 6.737088680267334, 'learning_rate': 0.00013525925925925926, 'epoch': 0.97}
{'loss': 3.5632, 'grad_norm': 6.251590251922607, 'learning_rate': 0.00013519753086419753, 'epoch': 0.97}
{'loss': 2.8087, 'grad_norm': 5.414275169372559, 'learning_rate': 0.00013513580246913583, 'epoch': 0.97}
{'loss': 3.2215, 'grad_norm': 4.745291709899902, 'learning_rate': 0.00013507407407407408, 'epoch': 0.97}
{'loss': 4.2521, 'grad_norm': 12.216216087341309, 'learning_rate': 0.00013501234567901235, 'epoch': 0.98}
{'loss': 2.7317, 'grad_norm': 3.861368417739868, 'learning_rate': 0.00013495061728395063, 'epoch': 0.98}
{'loss': 3.1246, 'grad_norm': 11.00317096710205, 'learning_rate': 0.0001348888888888889, 'epoch': 0.98}
{'loss': 3.209, 'grad_norm': 4.067930221557617, 'learning_rate': 0.00013482716049382717, 'epoch': 0.98}
{'loss': 2.8283, 'grad_norm': 4.901063442230225, 'learning_rate': 0.00013476543209876545, 'epoch': 0.98}
{'loss': 2.8674, 'grad_norm': 4.70626974105835, 'learning_rate': 0.00013470370370370372, 'epoch': 0.98}
{'loss': 2.5841, 'grad_norm': 3.8736014366149902, 'learning_rate': 0.00013464197530864197, 'epoch': 0.98}
{'loss': 3.8509, 'grad_norm': 4.33689546585083, 'learning_rate': 0.00013458024691358024, 'epoch': 0.98}
{'loss': 3.0089, 'grad_norm': 4.859792232513428, 'learning_rate': 0.00013451851851851854, 'epoch': 0.98}
{'loss': 2.8049, 'grad_norm': 6.913721561431885, 'learning_rate': 0.0001344567901234568, 'epoch': 0.98}
{'loss': 3.1067, 'grad_norm': 5.537696361541748, 'learning_rate': 0.00013439506172839506, 'epoch': 0.99}
{'loss': 2.7698, 'grad_norm': 4.277905464172363, 'learning_rate': 0.00013433333333333334, 'epoch': 0.99}
{'loss': 2.4868, 'grad_norm': 5.857649803161621, 'learning_rate': 0.0001342716049382716, 'epoch': 0.99}
{'loss': 2.9191, 'grad_norm': 3.747299909591675, 'learning_rate': 0.00013420987654320988, 'epoch': 0.99}
{'loss': 2.7278, 'grad_norm': 4.593921661376953, 'learning_rate': 0.00013414814814814816, 'epoch': 0.99}
{'loss': 2.6675, 'grad_norm': 4.813292503356934, 'learning_rate': 0.00013408641975308643, 'epoch': 0.99}
{'loss': 2.8171, 'grad_norm': 5.6723833084106445, 'learning_rate': 0.00013402469135802468, 'epoch': 0.99}
{'loss': 3.0122, 'grad_norm': 12.291019439697266, 'learning_rate': 0.00013396296296296298, 'epoch': 0.99}
{'loss': 3.2449, 'grad_norm': 5.813091278076172, 'learning_rate': 0.00013390123456790125, 'epoch': 0.99}
{'loss': 2.6908, 'grad_norm': 9.517964363098145, 'learning_rate': 0.0001338395061728395, 'epoch': 0.99}
{'loss': 2.3422, 'grad_norm': 4.321295261383057, 'learning_rate': 0.00013377777777777777, 'epoch': 0.99}
{'loss': 3.5112, 'grad_norm': 5.611319065093994, 'learning_rate': 0.00013371604938271605, 'epoch': 1.0}
{'loss': 2.9062, 'grad_norm': 7.254283428192139, 'learning_rate': 0.00013365432098765435, 'epoch': 1.0}
{'loss': 3.5908, 'grad_norm': 5.511903285980225, 'learning_rate': 0.0001335925925925926, 'epoch': 1.0}
{'loss': 3.3925, 'grad_norm': 6.417106628417969, 'learning_rate': 0.00013353086419753087, 'epoch': 1.0}
{'loss': 3.0694, 'grad_norm': 5.580140113830566, 'learning_rate': 0.00013346913580246914, 'epoch': 1.0}
{'loss': 2.7761, 'grad_norm': 4.639519691467285, 'learning_rate': 0.00013340740740740742, 'epoch': 1.0}
{'loss': 2.5984, 'grad_norm': 4.596829891204834, 'learning_rate': 0.0001333456790123457, 'epoch': 1.0}
{'loss': 2.5718, 'grad_norm': 4.970406532287598, 'learning_rate': 0.00013328395061728396, 'epoch': 1.0}
{'loss': 2.9112, 'grad_norm': 10.147337913513184, 'learning_rate': 0.00013322222222222224, 'epoch': 1.0}
{'loss': 3.4517, 'grad_norm': 4.544493675231934, 'learning_rate': 0.00013316049382716048, 'epoch': 1.0}
{'loss': 3.004, 'grad_norm': 11.242039680480957, 'learning_rate': 0.00013309876543209879, 'epoch': 1.0}
{'loss': 2.6328, 'grad_norm': 5.9310197830200195, 'learning_rate': 0.00013303703703703706, 'epoch': 1.01}
{'loss': 3.1182, 'grad_norm': 8.346236228942871, 'learning_rate': 0.0001329753086419753, 'epoch': 1.01}
{'loss': 2.5422, 'grad_norm': 4.0504889488220215, 'learning_rate': 0.00013291358024691358, 'epoch': 1.01}
{'loss': 3.047, 'grad_norm': 5.804905891418457, 'learning_rate': 0.00013285185185185185, 'epoch': 1.01}
{'loss': 3.2903, 'grad_norm': 3.6482093334198, 'learning_rate': 0.00013279012345679013, 'epoch': 1.01}
{'loss': 2.7219, 'grad_norm': 3.826184034347534, 'learning_rate': 0.0001327283950617284, 'epoch': 1.01}
{'loss': 3.0501, 'grad_norm': 5.447853088378906, 'learning_rate': 0.00013266666666666667, 'epoch': 1.01}
{'loss': 3.3814, 'grad_norm': 5.789559841156006, 'learning_rate': 0.00013260493827160495, 'epoch': 1.01}
{'loss': 2.7774, 'grad_norm': 3.8445041179656982, 'learning_rate': 0.0001325432098765432, 'epoch': 1.01}
{'loss': 3.3306, 'grad_norm': 6.273751258850098, 'learning_rate': 0.0001324814814814815, 'epoch': 1.01}
{'loss': 3.3505, 'grad_norm': 5.218961238861084, 'learning_rate': 0.00013241975308641977, 'epoch': 1.01}
{'loss': 2.7918, 'grad_norm': 5.047347545623779, 'learning_rate': 0.00013235802469135802, 'epoch': 1.02}
{'loss': 3.0977, 'grad_norm': 4.6034417152404785, 'learning_rate': 0.0001322962962962963, 'epoch': 1.02}
{'loss': 2.5542, 'grad_norm': 3.8351104259490967, 'learning_rate': 0.0001322345679012346, 'epoch': 1.02}
{'loss': 2.8661, 'grad_norm': 3.9929401874542236, 'learning_rate': 0.00013217283950617287, 'epoch': 1.02}
  warnings.warn(                                  
{'eval_loss': 2.764618396759033, 'eval_runtime': 412.3046, 'eval_samples_per_second': 1.455, 'eval_steps_per_second': 1.455, 'epoch': 1.02}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.4141, 'grad_norm': 6.3363847732543945, 'learning_rate': 0.0001321111111111111, 'epoch': 1.02}
{'loss': 2.7061, 'grad_norm': 6.798190593719482, 'learning_rate': 0.00013204938271604939, 'epoch': 1.02}
{'loss': 3.3112, 'grad_norm': 3.7782411575317383, 'learning_rate': 0.00013198765432098766, 'epoch': 1.02}
{'loss': 2.9885, 'grad_norm': 4.985571384429932, 'learning_rate': 0.00013192592592592593, 'epoch': 1.02}
{'loss': 2.389, 'grad_norm': 4.7519354820251465, 'learning_rate': 0.0001318641975308642, 'epoch': 1.02}
{'loss': 2.9489, 'grad_norm': 4.71165132522583, 'learning_rate': 0.00013180246913580248, 'epoch': 1.02}
{'loss': 2.6025, 'grad_norm': 4.536322116851807, 'learning_rate': 0.00013174074074074075, 'epoch': 1.02}
{'loss': 2.4275, 'grad_norm': 4.567592620849609, 'learning_rate': 0.000131679012345679, 'epoch': 1.03}
{'loss': 2.3106, 'grad_norm': 4.498507022857666, 'learning_rate': 0.0001316172839506173, 'epoch': 1.03}
{'loss': 2.4498, 'grad_norm': 7.533823490142822, 'learning_rate': 0.00013155555555555558, 'epoch': 1.03}
{'loss': 2.517, 'grad_norm': 4.192122459411621, 'learning_rate': 0.00013149382716049382, 'epoch': 1.03}
{'loss': 2.9368, 'grad_norm': 4.769320487976074, 'learning_rate': 0.0001314320987654321, 'epoch': 1.03}
{'loss': 2.2954, 'grad_norm': 4.156675338745117, 'learning_rate': 0.00013137037037037037, 'epoch': 1.03}
{'loss': 2.789, 'grad_norm': 5.070184230804443, 'learning_rate': 0.00013130864197530864, 'epoch': 1.03}
{'loss': 2.4839, 'grad_norm': 6.740492820739746, 'learning_rate': 0.00013124691358024692, 'epoch': 1.03}
{'loss': 2.9284, 'grad_norm': 5.931964874267578, 'learning_rate': 0.0001311851851851852, 'epoch': 1.03}
{'loss': 2.4503, 'grad_norm': 8.261327743530273, 'learning_rate': 0.00013112345679012347, 'epoch': 1.03}
{'loss': 2.3025, 'grad_norm': 5.3946309089660645, 'learning_rate': 0.00013106172839506174, 'epoch': 1.04}
{'loss': 2.749, 'grad_norm': 5.956405162811279, 'learning_rate': 0.000131, 'epoch': 1.04}
{'loss': 2.8015, 'grad_norm': 4.305123329162598, 'learning_rate': 0.0001309382716049383, 'epoch': 1.04}
{'loss': 2.617, 'grad_norm': 9.316519737243652, 'learning_rate': 0.00013087654320987653, 'epoch': 1.04}
{'loss': 3.3221, 'grad_norm': 6.2135090827941895, 'learning_rate': 0.0001308148148148148, 'epoch': 1.04}
{'loss': 2.4492, 'grad_norm': 3.8860723972320557, 'learning_rate': 0.0001307530864197531, 'epoch': 1.04}
{'loss': 2.5795, 'grad_norm': 5.3983917236328125, 'learning_rate': 0.00013069135802469135, 'epoch': 1.04}
{'loss': 2.7221, 'grad_norm': 5.259133815765381, 'learning_rate': 0.00013062962962962963, 'epoch': 1.04}
{'loss': 2.8725, 'grad_norm': 6.021335601806641, 'learning_rate': 0.0001305679012345679, 'epoch': 1.04}
{'loss': 3.6166, 'grad_norm': 9.937885284423828, 'learning_rate': 0.00013050617283950618, 'epoch': 1.04}
{'loss': 2.9797, 'grad_norm': 6.185091972351074, 'learning_rate': 0.00013044444444444445, 'epoch': 1.04}
{'loss': 2.8844, 'grad_norm': 4.887503147125244, 'learning_rate': 0.00013038271604938272, 'epoch': 1.05}
{'loss': 2.8831, 'grad_norm': 4.857191562652588, 'learning_rate': 0.000130320987654321, 'epoch': 1.05}
{'loss': 2.1287, 'grad_norm': 3.7839999198913574, 'learning_rate': 0.00013025925925925927, 'epoch': 1.05}
{'loss': 2.6185, 'grad_norm': 4.6961541175842285, 'learning_rate': 0.00013019753086419755, 'epoch': 1.05}
{'loss': 2.7145, 'grad_norm': 5.247591495513916, 'learning_rate': 0.00013013580246913582, 'epoch': 1.05}
{'loss': 2.8192, 'grad_norm': 6.517244815826416, 'learning_rate': 0.0001300740740740741, 'epoch': 1.05}
{'loss': 3.1838, 'grad_norm': 6.568953990936279, 'learning_rate': 0.00013001234567901234, 'epoch': 1.05}
{'loss': 2.4934, 'grad_norm': 6.446143627166748, 'learning_rate': 0.0001299506172839506, 'epoch': 1.05}
{'loss': 2.5623, 'grad_norm': 5.460818767547607, 'learning_rate': 0.00012988888888888891, 'epoch': 1.05}
{'loss': 3.1425, 'grad_norm': 4.009670257568359, 'learning_rate': 0.00012982716049382716, 'epoch': 1.05}
{'loss': 3.3764, 'grad_norm': 5.024749755859375, 'learning_rate': 0.00012976543209876543, 'epoch': 1.05}
{'loss': 2.9169, 'grad_norm': 4.514994144439697, 'learning_rate': 0.0001297037037037037, 'epoch': 1.06}
{'loss': 2.5003, 'grad_norm': 3.986384153366089, 'learning_rate': 0.00012964197530864198, 'epoch': 1.06}
{'loss': 2.3621, 'grad_norm': 5.6036882400512695, 'learning_rate': 0.00012958024691358026, 'epoch': 1.06}
{'loss': 2.8129, 'grad_norm': 4.681436538696289, 'learning_rate': 0.00012951851851851853, 'epoch': 1.06}
{'loss': 2.9896, 'grad_norm': 3.0877397060394287, 'learning_rate': 0.0001294567901234568, 'epoch': 1.06}
{'loss': 2.6054, 'grad_norm': 17.491422653198242, 'learning_rate': 0.00012939506172839505, 'epoch': 1.06}
{'loss': 2.9096, 'grad_norm': 4.284832954406738, 'learning_rate': 0.00012933333333333332, 'epoch': 1.06}
{'loss': 2.7133, 'grad_norm': 3.8425941467285156, 'learning_rate': 0.00012927160493827162, 'epoch': 1.06}
{'loss': 2.7174, 'grad_norm': 4.560996055603027, 'learning_rate': 0.00012920987654320987, 'epoch': 1.06}
{'loss': 3.1382, 'grad_norm': 4.538171768188477, 'learning_rate': 0.00012914814814814815, 'epoch': 1.06}
{'loss': 2.5354, 'grad_norm': 3.5296928882598877, 'learning_rate': 0.00012908641975308642, 'epoch': 1.06}
{'loss': 3.5611, 'grad_norm': 6.5827226638793945, 'learning_rate': 0.00012902469135802472, 'epoch': 1.07}
{'loss': 2.5744, 'grad_norm': 3.4661171436309814, 'learning_rate': 0.00012896296296296297, 'epoch': 1.07}
{'loss': 3.5361, 'grad_norm': 8.868112564086914, 'learning_rate': 0.00012890123456790124, 'epoch': 1.07}
{'loss': 2.9383, 'grad_norm': 3.7794148921966553, 'learning_rate': 0.00012883950617283951, 'epoch': 1.07}
{'loss': 2.5625, 'grad_norm': 4.472708225250244, 'learning_rate': 0.0001287777777777778, 'epoch': 1.07}
{'loss': 3.7165, 'grad_norm': 4.377107620239258, 'learning_rate': 0.00012871604938271606, 'epoch': 1.07}
{'loss': 3.0722, 'grad_norm': 8.405773162841797, 'learning_rate': 0.00012865432098765434, 'epoch': 1.07}
{'loss': 2.3983, 'grad_norm': 4.217696666717529, 'learning_rate': 0.0001285925925925926, 'epoch': 1.07}
{'loss': 3.1918, 'grad_norm': 6.320676326751709, 'learning_rate': 0.00012853086419753086, 'epoch': 1.07}
{'loss': 2.8338, 'grad_norm': 4.903084754943848, 'learning_rate': 0.00012846913580246913, 'epoch': 1.07}
{'loss': 2.3882, 'grad_norm': 4.424328804016113, 'learning_rate': 0.00012840740740740743, 'epoch': 1.07}
{'loss': 2.6684, 'grad_norm': 4.376224517822266, 'learning_rate': 0.00012834567901234568, 'epoch': 1.08}
{'loss': 2.5889, 'grad_norm': 4.055755615234375, 'learning_rate': 0.00012828395061728395, 'epoch': 1.08}
{'loss': 2.8706, 'grad_norm': 4.961483955383301, 'learning_rate': 0.00012822222222222222, 'epoch': 1.08}
{'loss': 2.836, 'grad_norm': 6.2895894050598145, 'learning_rate': 0.0001281604938271605, 'epoch': 1.08}
{'loss': 3.125, 'grad_norm': 6.377967357635498, 'learning_rate': 0.00012809876543209877, 'epoch': 1.08}
{'loss': 2.699, 'grad_norm': 4.943554401397705, 'learning_rate': 0.00012803703703703705, 'epoch': 1.08}
{'loss': 3.2242, 'grad_norm': 11.4605131149292, 'learning_rate': 0.00012797530864197532, 'epoch': 1.08}
{'loss': 2.7704, 'grad_norm': 3.7396080493927, 'learning_rate': 0.00012791358024691357, 'epoch': 1.08}
{'loss': 2.9854, 'grad_norm': 12.93986988067627, 'learning_rate': 0.00012785185185185187, 'epoch': 1.08}
{'loss': 3.061, 'grad_norm': 5.505040168762207, 'learning_rate': 0.00012779012345679014, 'epoch': 1.08}
{'loss': 2.7024, 'grad_norm': 4.294811725616455, 'learning_rate': 0.0001277283950617284, 'epoch': 1.09}
{'loss': 2.3293, 'grad_norm': 17.156408309936523, 'learning_rate': 0.00012766666666666666, 'epoch': 1.09}
{'loss': 3.4837, 'grad_norm': 7.0889763832092285, 'learning_rate': 0.00012760493827160494, 'epoch': 1.09}
{'loss': 2.6845, 'grad_norm': 7.008948802947998, 'learning_rate': 0.00012754320987654324, 'epoch': 1.09}
{'loss': 2.4507, 'grad_norm': 5.482565879821777, 'learning_rate': 0.00012748148148148148, 'epoch': 1.09}
{'loss': 2.9994, 'grad_norm': 7.384850025177002, 'learning_rate': 0.00012741975308641976, 'epoch': 1.09}
{'loss': 2.6448, 'grad_norm': 4.943121433258057, 'learning_rate': 0.00012735802469135803, 'epoch': 1.09}
{'loss': 2.8252, 'grad_norm': 9.741801261901855, 'learning_rate': 0.0001272962962962963, 'epoch': 1.09}
{'loss': 3.3106, 'grad_norm': 8.679849624633789, 'learning_rate': 0.00012723456790123458, 'epoch': 1.09}
{'loss': 2.7345, 'grad_norm': 5.3584794998168945, 'learning_rate': 0.00012717283950617285, 'epoch': 1.09}
{'loss': 2.7366, 'grad_norm': 7.232043743133545, 'learning_rate': 0.00012711111111111113, 'epoch': 1.09}
{'loss': 2.274, 'grad_norm': 11.363574981689453, 'learning_rate': 0.00012704938271604937, 'epoch': 1.1}
{'loss': 2.5856, 'grad_norm': 4.856405735015869, 'learning_rate': 0.00012698765432098767, 'epoch': 1.1}
{'loss': 3.2868, 'grad_norm': 7.798404693603516, 'learning_rate': 0.00012692592592592595, 'epoch': 1.1}
{'loss': 2.5334, 'grad_norm': 4.882857322692871, 'learning_rate': 0.0001268641975308642, 'epoch': 1.1}
{'loss': 2.5135, 'grad_norm': 3.4826014041900635, 'learning_rate': 0.00012680246913580247, 'epoch': 1.1}
{'loss': 2.981, 'grad_norm': 5.151905059814453, 'learning_rate': 0.00012674074074074074, 'epoch': 1.1}
{'loss': 2.757, 'grad_norm': 7.598877429962158, 'learning_rate': 0.00012667901234567902, 'epoch': 1.1}
{'loss': 2.993, 'grad_norm': 5.851440906524658, 'learning_rate': 0.0001266172839506173, 'epoch': 1.1}
{'loss': 2.201, 'grad_norm': 3.9678192138671875, 'learning_rate': 0.00012655555555555556, 'epoch': 1.1}
{'loss': 2.6364, 'grad_norm': 5.17921257019043, 'learning_rate': 0.00012649382716049384, 'epoch': 1.1}
{'loss': 3.0767, 'grad_norm': 9.139985084533691, 'learning_rate': 0.00012643209876543208, 'epoch': 1.1}
{'loss': 2.8142, 'grad_norm': 5.426150321960449, 'learning_rate': 0.00012637037037037038, 'epoch': 1.11}
{'loss': 2.5641, 'grad_norm': 4.954465866088867, 'learning_rate': 0.00012630864197530866, 'epoch': 1.11}
{'loss': 3.3764, 'grad_norm': 4.597005844116211, 'learning_rate': 0.0001262469135802469, 'epoch': 1.11}
{'loss': 2.4676, 'grad_norm': 4.321470260620117, 'learning_rate': 0.00012618518518518518, 'epoch': 1.11}
{'loss': 3.0957, 'grad_norm': 6.264866352081299, 'learning_rate': 0.00012612345679012348, 'epoch': 1.11}
{'loss': 2.4811, 'grad_norm': 6.296106338500977, 'learning_rate': 0.00012606172839506175, 'epoch': 1.11}
{'loss': 3.7915, 'grad_norm': 6.656477451324463, 'learning_rate': 0.000126, 'epoch': 1.11}
  warnings.warn(                                  
{'eval_loss': 2.7673866748809814, 'eval_runtime': 412.4548, 'eval_samples_per_second': 1.455, 'eval_steps_per_second': 1.455, 'epoch': 1.11}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.8934, 'grad_norm': 5.155009746551514, 'learning_rate': 0.00012593827160493827, 'epoch': 1.11}
{'loss': 2.3881, 'grad_norm': 6.203547954559326, 'learning_rate': 0.00012587654320987655, 'epoch': 1.11}
{'loss': 2.6917, 'grad_norm': 5.69082498550415, 'learning_rate': 0.00012581481481481482, 'epoch': 1.11}
{'loss': 2.5219, 'grad_norm': 4.577427387237549, 'learning_rate': 0.0001257530864197531, 'epoch': 1.11}
{'loss': 2.5224, 'grad_norm': 5.0820417404174805, 'learning_rate': 0.00012569135802469137, 'epoch': 1.12}
{'loss': 3.2722, 'grad_norm': 4.933923244476318, 'learning_rate': 0.00012562962962962964, 'epoch': 1.12}
{'loss': 2.9984, 'grad_norm': 5.487542629241943, 'learning_rate': 0.0001255679012345679, 'epoch': 1.12}
{'loss': 2.9807, 'grad_norm': 5.511381149291992, 'learning_rate': 0.0001255061728395062, 'epoch': 1.12}
{'loss': 2.6952, 'grad_norm': 4.984970569610596, 'learning_rate': 0.00012544444444444446, 'epoch': 1.12}
{'loss': 2.7482, 'grad_norm': 4.388258457183838, 'learning_rate': 0.0001253827160493827, 'epoch': 1.12}
{'loss': 2.6826, 'grad_norm': 6.213352203369141, 'learning_rate': 0.00012532098765432098, 'epoch': 1.12}
{'loss': 2.9858, 'grad_norm': 11.379194259643555, 'learning_rate': 0.00012525925925925929, 'epoch': 1.12}
{'loss': 3.2713, 'grad_norm': 8.993492126464844, 'learning_rate': 0.00012519753086419753, 'epoch': 1.12}
{'loss': 2.6455, 'grad_norm': 4.210080146789551, 'learning_rate': 0.0001251358024691358, 'epoch': 1.12}
{'loss': 3.1847, 'grad_norm': 5.928145408630371, 'learning_rate': 0.00012507407407407408, 'epoch': 1.12}
{'loss': 2.6699, 'grad_norm': 4.210380554199219, 'learning_rate': 0.00012501234567901235, 'epoch': 1.13}
{'loss': 2.7779, 'grad_norm': 7.181911468505859, 'learning_rate': 0.00012495061728395063, 'epoch': 1.13}
{'loss': 2.3865, 'grad_norm': 6.099572658538818, 'learning_rate': 0.0001248888888888889, 'epoch': 1.13}
{'loss': 3.1687, 'grad_norm': 4.936030864715576, 'learning_rate': 0.00012482716049382717, 'epoch': 1.13}
{'loss': 2.9819, 'grad_norm': 12.055975914001465, 'learning_rate': 0.00012476543209876542, 'epoch': 1.13}
{'loss': 2.4607, 'grad_norm': 4.876532077789307, 'learning_rate': 0.0001247037037037037, 'epoch': 1.13}
{'loss': 3.0861, 'grad_norm': 5.289126873016357, 'learning_rate': 0.000124641975308642, 'epoch': 1.13}
{'loss': 3.0695, 'grad_norm': 5.159422397613525, 'learning_rate': 0.00012458024691358027, 'epoch': 1.13}
{'loss': 2.8983, 'grad_norm': 5.0341339111328125, 'learning_rate': 0.00012451851851851852, 'epoch': 1.13}
{'loss': 2.428, 'grad_norm': 5.9041428565979, 'learning_rate': 0.0001244567901234568, 'epoch': 1.13}
{'loss': 2.2431, 'grad_norm': 3.8995957374572754, 'learning_rate': 0.00012439506172839506, 'epoch': 1.14}
{'loss': 2.7409, 'grad_norm': 4.640994548797607, 'learning_rate': 0.00012433333333333334, 'epoch': 1.14}
{'loss': 3.1147, 'grad_norm': 11.216383934020996, 'learning_rate': 0.0001242716049382716, 'epoch': 1.14}
{'loss': 2.7348, 'grad_norm': 5.869344234466553, 'learning_rate': 0.00012420987654320989, 'epoch': 1.14}
{'loss': 3.4108, 'grad_norm': 6.128626823425293, 'learning_rate': 0.00012414814814814816, 'epoch': 1.14}
{'loss': 2.6986, 'grad_norm': 4.889739036560059, 'learning_rate': 0.00012408641975308643, 'epoch': 1.14}
{'loss': 2.4439, 'grad_norm': 5.066452503204346, 'learning_rate': 0.0001240246913580247, 'epoch': 1.14}
{'loss': 2.9983, 'grad_norm': 4.396365642547607, 'learning_rate': 0.00012396296296296298, 'epoch': 1.14}
{'loss': 3.0257, 'grad_norm': 5.192996978759766, 'learning_rate': 0.00012390123456790123, 'epoch': 1.14}
{'loss': 3.5952, 'grad_norm': 6.067643165588379, 'learning_rate': 0.0001238395061728395, 'epoch': 1.14}
{'loss': 3.058, 'grad_norm': 4.868331432342529, 'learning_rate': 0.0001237777777777778, 'epoch': 1.14}
{'loss': 3.0931, 'grad_norm': 4.460557460784912, 'learning_rate': 0.00012371604938271605, 'epoch': 1.15}
{'loss': 2.9998, 'grad_norm': 4.296310901641846, 'learning_rate': 0.00012365432098765432, 'epoch': 1.15}
{'loss': 2.5965, 'grad_norm': 5.167713165283203, 'learning_rate': 0.0001235925925925926, 'epoch': 1.15}
{'loss': 2.5787, 'grad_norm': 7.253588676452637, 'learning_rate': 0.00012353086419753087, 'epoch': 1.15}
{'loss': 3.0081, 'grad_norm': 5.1223015785217285, 'learning_rate': 0.00012346913580246914, 'epoch': 1.15}
{'loss': 2.9691, 'grad_norm': nan, 'learning_rate': 0.00012340740740740742, 'epoch': 1.15}
{'loss': 3.3175, 'grad_norm': 7.398138999938965, 'learning_rate': 0.00012335802469135801, 'epoch': 1.15}
{'loss': 2.7059, 'grad_norm': 3.340245485305786, 'learning_rate': 0.00012329629629629632, 'epoch': 1.15}
{'loss': 2.9776, 'grad_norm': 12.28267765045166, 'learning_rate': 0.0001232345679012346, 'epoch': 1.15}
{'loss': 3.009, 'grad_norm': 4.6936140060424805, 'learning_rate': 0.00012317283950617284, 'epoch': 1.15}
{'loss': 2.4508, 'grad_norm': 5.003667831420898, 'learning_rate': 0.0001231111111111111, 'epoch': 1.15}
{'loss': 3.0265, 'grad_norm': 6.327855587005615, 'learning_rate': 0.0001230493827160494, 'epoch': 1.16}
{'loss': 2.8988, 'grad_norm': 3.8144283294677734, 'learning_rate': 0.00012298765432098766, 'epoch': 1.16}
{'loss': 2.9572, 'grad_norm': 6.57953405380249, 'learning_rate': 0.00012292592592592593, 'epoch': 1.16}
{'loss': 3.3305, 'grad_norm': 3.396787405014038, 'learning_rate': 0.0001228641975308642, 'epoch': 1.16}
{'loss': 3.2446, 'grad_norm': 5.096120834350586, 'learning_rate': 0.00012280246913580248, 'epoch': 1.16}
{'loss': 2.7206, 'grad_norm': 5.3332014083862305, 'learning_rate': 0.00012274074074074075, 'epoch': 1.16}
{'loss': 3.151, 'grad_norm': 6.717229843139648, 'learning_rate': 0.00012267901234567903, 'epoch': 1.16}
{'loss': 3.4068, 'grad_norm': 8.221832275390625, 'learning_rate': 0.0001226172839506173, 'epoch': 1.16}
{'loss': 3.3414, 'grad_norm': 4.313227653503418, 'learning_rate': 0.00012255555555555555, 'epoch': 1.16}
{'loss': 3.1229, 'grad_norm': 5.586184978485107, 'learning_rate': 0.00012249382716049382, 'epoch': 1.16}
{'loss': 2.9429, 'grad_norm': 5.062484264373779, 'learning_rate': 0.00012243209876543212, 'epoch': 1.16}
{'loss': 3.0392, 'grad_norm': 5.890458106994629, 'learning_rate': 0.00012237037037037037, 'epoch': 1.17}
{'loss': 3.0781, 'grad_norm': 7.019794940948486, 'learning_rate': 0.00012230864197530864, 'epoch': 1.17}
{'loss': 3.3603, 'grad_norm': 5.231752395629883, 'learning_rate': 0.00012224691358024692, 'epoch': 1.17}
{'loss': 3.1192, 'grad_norm': 6.435535430908203, 'learning_rate': 0.0001221851851851852, 'epoch': 1.17}
{'loss': 2.7722, 'grad_norm': 2.776763439178467, 'learning_rate': 0.00012212345679012346, 'epoch': 1.17}
{'loss': 3.1169, 'grad_norm': 7.167818546295166, 'learning_rate': 0.00012206172839506174, 'epoch': 1.17}
{'loss': 2.892, 'grad_norm': 8.089978218078613, 'learning_rate': 0.000122, 'epoch': 1.17}
{'loss': 2.7475, 'grad_norm': 6.242933750152588, 'learning_rate': 0.00012193827160493827, 'epoch': 1.17}
{'loss': 2.5837, 'grad_norm': 6.636911869049072, 'learning_rate': 0.00012187654320987656, 'epoch': 1.17}
{'loss': 2.9829, 'grad_norm': 5.10159969329834, 'learning_rate': 0.00012181481481481483, 'epoch': 1.17}
{'loss': 2.0496, 'grad_norm': 5.619296550750732, 'learning_rate': 0.00012175308641975309, 'epoch': 1.18}
{'loss': 2.6397, 'grad_norm': 4.7321367263793945, 'learning_rate': 0.00012169135802469137, 'epoch': 1.18}
{'loss': 2.7897, 'grad_norm': 4.795454502105713, 'learning_rate': 0.00012162962962962963, 'epoch': 1.18}
{'loss': 2.7866, 'grad_norm': 5.062082767486572, 'learning_rate': 0.00012156790123456791, 'epoch': 1.18}
{'loss': 3.5213, 'grad_norm': 5.911881446838379, 'learning_rate': 0.00012150617283950619, 'epoch': 1.18}
{'loss': 3.2493, 'grad_norm': 7.236544609069824, 'learning_rate': 0.00012144444444444445, 'epoch': 1.18}
{'loss': 3.0071, 'grad_norm': 6.8127899169921875, 'learning_rate': 0.00012138271604938272, 'epoch': 1.18}
{'loss': 2.801, 'grad_norm': 3.3127267360687256, 'learning_rate': 0.00012132098765432098, 'epoch': 1.18}
{'loss': 3.0597, 'grad_norm': 8.81524658203125, 'learning_rate': 0.00012125925925925927, 'epoch': 1.18}
{'loss': 2.7043, 'grad_norm': 4.613575458526611, 'learning_rate': 0.00012119753086419754, 'epoch': 1.18}
{'loss': 2.9787, 'grad_norm': 3.2881805896759033, 'learning_rate': 0.0001211358024691358, 'epoch': 1.18}
{'loss': 3.8244, 'grad_norm': 6.463468551635742, 'learning_rate': 0.00012107407407407408, 'epoch': 1.19}
{'loss': 2.9566, 'grad_norm': 4.537844181060791, 'learning_rate': 0.00012101234567901236, 'epoch': 1.19}
{'loss': 3.1953, 'grad_norm': 5.635930061340332, 'learning_rate': 0.00012095061728395062, 'epoch': 1.19}
{'loss': 3.1298, 'grad_norm': 5.701106071472168, 'learning_rate': 0.0001208888888888889, 'epoch': 1.19}
{'loss': 2.7795, 'grad_norm': 7.647531032562256, 'learning_rate': 0.00012082716049382716, 'epoch': 1.19}
{'loss': 2.6168, 'grad_norm': 6.622318267822266, 'learning_rate': 0.00012076543209876543, 'epoch': 1.19}
{'loss': 2.3979, 'grad_norm': 4.938936233520508, 'learning_rate': 0.00012070370370370372, 'epoch': 1.19}
{'loss': 3.1139, 'grad_norm': 5.2952446937561035, 'learning_rate': 0.00012064197530864198, 'epoch': 1.19}
{'loss': 2.2037, 'grad_norm': 5.106762886047363, 'learning_rate': 0.00012058024691358025, 'epoch': 1.19}
{'loss': 2.4356, 'grad_norm': 8.025805473327637, 'learning_rate': 0.00012051851851851851, 'epoch': 1.19}
{'loss': 2.6492, 'grad_norm': 6.888979434967041, 'learning_rate': 0.00012045679012345679, 'epoch': 1.19}
{'loss': 3.3297, 'grad_norm': 6.5598273277282715, 'learning_rate': 0.00012039506172839508, 'epoch': 1.2}
{'loss': 2.5154, 'grad_norm': 5.023137092590332, 'learning_rate': 0.00012033333333333335, 'epoch': 1.2}
{'loss': 2.3391, 'grad_norm': 5.025289535522461, 'learning_rate': 0.00012027160493827161, 'epoch': 1.2}
{'loss': 3.0331, 'grad_norm': 6.018956661224365, 'learning_rate': 0.00012020987654320987, 'epoch': 1.2}
{'loss': 3.7814, 'grad_norm': 4.416308879852295, 'learning_rate': 0.00012014814814814814, 'epoch': 1.2}
{'loss': 3.1597, 'grad_norm': 8.430418014526367, 'learning_rate': 0.00012008641975308643, 'epoch': 1.2}
{'loss': 2.3975, 'grad_norm': 5.6732635498046875, 'learning_rate': 0.0001200246913580247, 'epoch': 1.2}
{'loss': 2.801, 'grad_norm': 6.3685712814331055, 'learning_rate': 0.00011996296296296296, 'epoch': 1.2}
{'loss': 2.8957, 'grad_norm': 4.223400115966797, 'learning_rate': 0.00011990123456790124, 'epoch': 1.2}
{'loss': 2.8849, 'grad_norm': 3.908745527267456, 'learning_rate': 0.00011983950617283953, 'epoch': 1.2}
  warnings.warn(                                  
{'eval_loss': 2.753653049468994, 'eval_runtime': 412.6992, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 1.454, 'epoch': 1.2}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.4892, 'grad_norm': 7.710258960723877, 'learning_rate': 0.00011977777777777779, 'epoch': 1.2}
{'loss': 2.9213, 'grad_norm': 6.062934875488281, 'learning_rate': 0.00011971604938271606, 'epoch': 1.21}
{'loss': 2.845, 'grad_norm': 4.830548286437988, 'learning_rate': 0.00011965432098765432, 'epoch': 1.21}
{'loss': 2.7782, 'grad_norm': 4.310309886932373, 'learning_rate': 0.0001195925925925926, 'epoch': 1.21}
{'loss': 2.5644, 'grad_norm': 4.986969947814941, 'learning_rate': 0.00011953086419753088, 'epoch': 1.21}
{'loss': 2.4524, 'grad_norm': 6.655799388885498, 'learning_rate': 0.00011946913580246914, 'epoch': 1.21}
{'loss': 2.5719, 'grad_norm': 7.990471363067627, 'learning_rate': 0.00011940740740740742, 'epoch': 1.21}
{'loss': 3.2005, 'grad_norm': 5.199170112609863, 'learning_rate': 0.00011934567901234568, 'epoch': 1.21}
{'loss': 3.9306, 'grad_norm': 4.991022109985352, 'learning_rate': 0.00011928395061728395, 'epoch': 1.21}
{'loss': 3.6471, 'grad_norm': 5.316286087036133, 'learning_rate': 0.00011922222222222224, 'epoch': 1.21}
{'loss': 2.6355, 'grad_norm': 6.018586158752441, 'learning_rate': 0.0001191604938271605, 'epoch': 1.21}
{'loss': 2.9365, 'grad_norm': 4.849008083343506, 'learning_rate': 0.00011909876543209877, 'epoch': 1.21}
{'loss': 2.992, 'grad_norm': 6.497365474700928, 'learning_rate': 0.00011903703703703703, 'epoch': 1.22}
{'loss': 2.5781, 'grad_norm': 5.535061359405518, 'learning_rate': 0.00011897530864197533, 'epoch': 1.22}
{'loss': 2.4005, 'grad_norm': 4.501823425292969, 'learning_rate': 0.00011891358024691359, 'epoch': 1.22}
{'loss': 3.0859, 'grad_norm': 8.367337226867676, 'learning_rate': 0.00011885185185185185, 'epoch': 1.22}
{'loss': 3.3347, 'grad_norm': 4.377227783203125, 'learning_rate': 0.00011879012345679013, 'epoch': 1.22}
{'loss': 2.6442, 'grad_norm': 5.7691874504089355, 'learning_rate': 0.00011872839506172839, 'epoch': 1.22}
{'loss': 3.3977, 'grad_norm': 6.801534175872803, 'learning_rate': 0.00011866666666666669, 'epoch': 1.22}
{'loss': 2.8655, 'grad_norm': 3.8170013427734375, 'learning_rate': 0.00011860493827160495, 'epoch': 1.22}
{'loss': 2.7942, 'grad_norm': 4.94915246963501, 'learning_rate': 0.00011854320987654322, 'epoch': 1.22}
{'loss': 3.6081, 'grad_norm': 13.542702674865723, 'learning_rate': 0.00011848148148148148, 'epoch': 1.22}
{'loss': 2.5301, 'grad_norm': 4.718916893005371, 'learning_rate': 0.00011841975308641976, 'epoch': 1.23}
{'loss': 2.4106, 'grad_norm': 6.6008782386779785, 'learning_rate': 0.00011835802469135804, 'epoch': 1.23}
{'loss': 2.9761, 'grad_norm': 5.543396949768066, 'learning_rate': 0.0001182962962962963, 'epoch': 1.23}
{'loss': 3.0983, 'grad_norm': 4.142580986022949, 'learning_rate': 0.00011823456790123458, 'epoch': 1.23}
{'loss': 3.5389, 'grad_norm': 4.125502586364746, 'learning_rate': 0.00011817283950617284, 'epoch': 1.23}
{'loss': 2.6071, 'grad_norm': 5.324550151824951, 'learning_rate': 0.00011811111111111111, 'epoch': 1.23}
{'loss': 3.4294, 'grad_norm': 7.707825183868408, 'learning_rate': 0.0001180493827160494, 'epoch': 1.23}
{'loss': 3.0336, 'grad_norm': 9.735894203186035, 'learning_rate': 0.00011798765432098766, 'epoch': 1.23}
{'loss': 3.114, 'grad_norm': 6.520545959472656, 'learning_rate': 0.00011792592592592593, 'epoch': 1.23}
{'loss': 2.6463, 'grad_norm': 5.49100923538208, 'learning_rate': 0.00011786419753086419, 'epoch': 1.23}
{'loss': 2.9386, 'grad_norm': 5.400496959686279, 'learning_rate': 0.00011780246913580248, 'epoch': 1.23}
{'loss': 2.8579, 'grad_norm': 4.496227264404297, 'learning_rate': 0.00011774074074074075, 'epoch': 1.24}
{'loss': 2.5567, 'grad_norm': 6.395915985107422, 'learning_rate': 0.00011767901234567901, 'epoch': 1.24}
{'loss': 3.7948, 'grad_norm': 7.019949913024902, 'learning_rate': 0.00011761728395061729, 'epoch': 1.24}
{'loss': 2.5349, 'grad_norm': 4.568688869476318, 'learning_rate': 0.00011755555555555555, 'epoch': 1.24}
{'loss': 2.4716, 'grad_norm': 7.530618667602539, 'learning_rate': 0.00011749382716049383, 'epoch': 1.24}
{'loss': 2.8954, 'grad_norm': 5.937447547912598, 'learning_rate': 0.00011743209876543211, 'epoch': 1.24}
{'loss': 2.4205, 'grad_norm': 5.56420373916626, 'learning_rate': 0.00011737037037037037, 'epoch': 1.24}
{'loss': 2.643, 'grad_norm': 4.788768291473389, 'learning_rate': 0.00011730864197530864, 'epoch': 1.24}
{'loss': 3.3021, 'grad_norm': 6.464276313781738, 'learning_rate': 0.0001172469135802469, 'epoch': 1.24}
{'loss': 2.7441, 'grad_norm': 4.910634994506836, 'learning_rate': 0.0001171851851851852, 'epoch': 1.24}
{'loss': 2.6477, 'grad_norm': 4.615391254425049, 'learning_rate': 0.00011712345679012346, 'epoch': 1.24}
{'loss': 2.8588, 'grad_norm': 7.110569953918457, 'learning_rate': 0.00011706172839506174, 'epoch': 1.25}
{'loss': 2.9398, 'grad_norm': 4.5809645652771, 'learning_rate': 0.000117, 'epoch': 1.25}
{'loss': 3.0147, 'grad_norm': 6.0987348556518555, 'learning_rate': 0.00011693827160493829, 'epoch': 1.25}
{'loss': 3.0694, 'grad_norm': 8.234905242919922, 'learning_rate': 0.00011687654320987656, 'epoch': 1.25}
{'loss': 2.864, 'grad_norm': 4.447389125823975, 'learning_rate': 0.00011681481481481482, 'epoch': 1.25}
{'loss': 3.8138, 'grad_norm': 8.240595817565918, 'learning_rate': 0.00011675308641975309, 'epoch': 1.25}
{'loss': 2.7413, 'grad_norm': 5.4283952713012695, 'learning_rate': 0.00011669135802469135, 'epoch': 1.25}
{'loss': 2.4795, 'grad_norm': 5.217905521392822, 'learning_rate': 0.00011662962962962964, 'epoch': 1.25}
{'loss': 2.8922, 'grad_norm': 3.9195218086242676, 'learning_rate': 0.00011656790123456791, 'epoch': 1.25}
{'loss': 2.3536, 'grad_norm': 5.753921031951904, 'learning_rate': 0.00011650617283950617, 'epoch': 1.25}
{'loss': 3.3364, 'grad_norm': 5.199613571166992, 'learning_rate': 0.00011644444444444445, 'epoch': 1.25}
{'loss': 3.1179, 'grad_norm': 5.268481731414795, 'learning_rate': 0.00011638271604938271, 'epoch': 1.26}
{'loss': 2.8197, 'grad_norm': 6.356109619140625, 'learning_rate': 0.000116320987654321, 'epoch': 1.26}
{'loss': 2.5526, 'grad_norm': 7.311467170715332, 'learning_rate': 0.00011625925925925927, 'epoch': 1.26}
{'loss': 2.6634, 'grad_norm': 5.1820855140686035, 'learning_rate': 0.00011619753086419753, 'epoch': 1.26}
{'loss': 3.193, 'grad_norm': 5.868142604827881, 'learning_rate': 0.0001161358024691358, 'epoch': 1.26}
{'loss': 3.2483, 'grad_norm': 4.336532115936279, 'learning_rate': 0.00011607407407407409, 'epoch': 1.26}
{'loss': 2.1638, 'grad_norm': 5.742249965667725, 'learning_rate': 0.00011601234567901235, 'epoch': 1.26}
{'loss': 2.8173, 'grad_norm': 4.429267406463623, 'learning_rate': 0.00011595061728395063, 'epoch': 1.26}
{'loss': 2.8588, 'grad_norm': 4.720630168914795, 'learning_rate': 0.00011588888888888889, 'epoch': 1.26}
{'loss': 2.5813, 'grad_norm': 5.0408148765563965, 'learning_rate': 0.00011582716049382716, 'epoch': 1.26}
{'loss': 3.0083, 'grad_norm': 6.314742565155029, 'learning_rate': 0.00011576543209876545, 'epoch': 1.26}
{'loss': 2.7945, 'grad_norm': 4.4726080894470215, 'learning_rate': 0.00011570370370370372, 'epoch': 1.27}
{'loss': 2.605, 'grad_norm': 7.468185901641846, 'learning_rate': 0.00011564197530864198, 'epoch': 1.27}
{'loss': 2.5501, 'grad_norm': 5.618171215057373, 'learning_rate': 0.00011558024691358025, 'epoch': 1.27}
{'loss': 3.0858, 'grad_norm': 7.028726577758789, 'learning_rate': 0.00011551851851851851, 'epoch': 1.27}
{'loss': 2.6495, 'grad_norm': 8.327268600463867, 'learning_rate': 0.0001154567901234568, 'epoch': 1.27}
{'loss': 2.819, 'grad_norm': 7.192134857177734, 'learning_rate': 0.00011539506172839508, 'epoch': 1.27}
{'loss': 2.6979, 'grad_norm': 7.015320301055908, 'learning_rate': 0.00011533333333333334, 'epoch': 1.27}
{'loss': 2.7605, 'grad_norm': 6.844057559967041, 'learning_rate': 0.00011527160493827161, 'epoch': 1.27}
{'loss': 3.4061, 'grad_norm': 8.902921676635742, 'learning_rate': 0.00011520987654320987, 'epoch': 1.27}
{'loss': 3.1335, 'grad_norm': 4.871196269989014, 'learning_rate': 0.00011514814814814816, 'epoch': 1.27}
{'loss': 3.1608, 'grad_norm': 4.3971428871154785, 'learning_rate': 0.00011508641975308643, 'epoch': 1.27}
{'loss': 3.3262, 'grad_norm': 6.4737701416015625, 'learning_rate': 0.00011502469135802469, 'epoch': 1.28}
{'loss': 3.3303, 'grad_norm': 7.8163557052612305, 'learning_rate': 0.00011496296296296297, 'epoch': 1.28}
{'loss': 2.796, 'grad_norm': 16.862478256225586, 'learning_rate': 0.00011490123456790125, 'epoch': 1.28}
{'loss': 3.1549, 'grad_norm': 5.374831676483154, 'learning_rate': 0.00011483950617283951, 'epoch': 1.28}
{'loss': 2.3339, 'grad_norm': 4.925633907318115, 'learning_rate': 0.00011477777777777779, 'epoch': 1.28}
{'loss': 3.0487, 'grad_norm': 5.237983226776123, 'learning_rate': 0.00011471604938271605, 'epoch': 1.28}
{'loss': 3.1091, 'grad_norm': 4.274326324462891, 'learning_rate': 0.00011465432098765432, 'epoch': 1.28}
{'loss': 2.7578, 'grad_norm': 4.787829399108887, 'learning_rate': 0.00011459259259259261, 'epoch': 1.28}
{'loss': 2.9505, 'grad_norm': 7.74447774887085, 'learning_rate': 0.00011453086419753087, 'epoch': 1.28}
{'loss': 2.9311, 'grad_norm': 4.8124003410339355, 'learning_rate': 0.00011446913580246914, 'epoch': 1.28}
{'loss': 2.3516, 'grad_norm': 3.6602909564971924, 'learning_rate': 0.0001144074074074074, 'epoch': 1.29}
{'loss': 2.4445, 'grad_norm': 5.825507164001465, 'learning_rate': 0.00011434567901234568, 'epoch': 1.29}
{'loss': 2.6207, 'grad_norm': 5.364184856414795, 'learning_rate': 0.00011428395061728396, 'epoch': 1.29}
{'loss': 2.851, 'grad_norm': 6.879342555999756, 'learning_rate': 0.00011422222222222224, 'epoch': 1.29}
{'loss': 2.3416, 'grad_norm': 6.362977027893066, 'learning_rate': 0.0001141604938271605, 'epoch': 1.29}
{'loss': 2.5996, 'grad_norm': 4.965247631072998, 'learning_rate': 0.00011409876543209877, 'epoch': 1.29}
{'loss': 2.7201, 'grad_norm': 6.146298408508301, 'learning_rate': 0.00011403703703703706, 'epoch': 1.29}
{'loss': 2.8258, 'grad_norm': 8.9128999710083, 'learning_rate': 0.00011397530864197532, 'epoch': 1.29}
{'loss': 3.7844, 'grad_norm': 6.186609268188477, 'learning_rate': 0.00011391358024691359, 'epoch': 1.29}
{'loss': 2.5226, 'grad_norm': 4.888931751251221, 'learning_rate': 0.00011385185185185185, 'epoch': 1.29}
{'loss': 3.2796, 'grad_norm': 6.716485023498535, 'learning_rate': 0.00011379012345679013, 'epoch': 1.29}
{'loss': 3.4358, 'grad_norm': 4.96286678314209, 'learning_rate': 0.00011372839506172841, 'epoch': 1.3}
{'loss': 3.2931, 'grad_norm': 7.859227657318115, 'learning_rate': 0.00011366666666666667, 'epoch': 1.3}
  warnings.warn(                                  
{'eval_loss': 2.757641553878784, 'eval_runtime': 413.1674, 'eval_samples_per_second': 1.452, 'eval_steps_per_second': 1.452, 'epoch': 1.3}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.2214, 'grad_norm': 6.508372783660889, 'learning_rate': 0.00011360493827160495, 'epoch': 1.3}
{'loss': 2.5555, 'grad_norm': 3.8882124423980713, 'learning_rate': 0.00011354320987654321, 'epoch': 1.3}
{'loss': 2.3305, 'grad_norm': 5.488385200500488, 'learning_rate': 0.00011348148148148148, 'epoch': 1.3}
{'loss': 2.7871, 'grad_norm': 7.451428413391113, 'learning_rate': 0.00011341975308641977, 'epoch': 1.3}
{'loss': 2.7158, 'grad_norm': 8.105524063110352, 'learning_rate': 0.00011335802469135803, 'epoch': 1.3}
{'loss': 2.8544, 'grad_norm': 6.8633222579956055, 'learning_rate': 0.0001132962962962963, 'epoch': 1.3}
{'loss': 3.2986, 'grad_norm': 5.3521342277526855, 'learning_rate': 0.00011323456790123456, 'epoch': 1.3}
{'loss': 2.3322, 'grad_norm': 4.412693500518799, 'learning_rate': 0.00011317283950617284, 'epoch': 1.3}
{'loss': 2.6934, 'grad_norm': 5.4080657958984375, 'learning_rate': 0.00011311111111111112, 'epoch': 1.3}
{'loss': 3.1694, 'grad_norm': 6.740055084228516, 'learning_rate': 0.00011304938271604938, 'epoch': 1.31}
{'loss': 3.5098, 'grad_norm': 10.712843894958496, 'learning_rate': 0.00011298765432098766, 'epoch': 1.31}
{'loss': 3.7738, 'grad_norm': 18.794925689697266, 'learning_rate': 0.00011292592592592592, 'epoch': 1.31}
{'loss': 2.7385, 'grad_norm': 3.9464688301086426, 'learning_rate': 0.00011286419753086422, 'epoch': 1.31}
{'loss': 2.4665, 'grad_norm': 3.7211172580718994, 'learning_rate': 0.00011280246913580248, 'epoch': 1.31}
{'loss': 2.8256, 'grad_norm': 5.36487340927124, 'learning_rate': 0.00011274074074074074, 'epoch': 1.31}
{'loss': 2.8038, 'grad_norm': 3.9738245010375977, 'learning_rate': 0.00011267901234567901, 'epoch': 1.31}
{'loss': 2.9009, 'grad_norm': 4.620434761047363, 'learning_rate': 0.00011261728395061727, 'epoch': 1.31}
{'loss': 3.485, 'grad_norm': 13.434114456176758, 'learning_rate': 0.00011255555555555557, 'epoch': 1.31}
{'loss': 2.4922, 'grad_norm': 5.852077960968018, 'learning_rate': 0.00011249382716049384, 'epoch': 1.31}
{'loss': 2.7974, 'grad_norm': 6.689511775970459, 'learning_rate': 0.00011243209876543211, 'epoch': 1.31}
{'loss': 3.6189, 'grad_norm': 4.781833648681641, 'learning_rate': 0.00011237037037037037, 'epoch': 1.32}
{'loss': 3.6338, 'grad_norm': 4.407285690307617, 'learning_rate': 0.00011230864197530864, 'epoch': 1.32}
{'loss': 2.5455, 'grad_norm': 4.4594597816467285, 'learning_rate': 0.00011224691358024693, 'epoch': 1.32}
{'loss': 2.8632, 'grad_norm': 4.55999755859375, 'learning_rate': 0.00011218518518518519, 'epoch': 1.32}
{'loss': 3.0789, 'grad_norm': 11.682376861572266, 'learning_rate': 0.00011212345679012346, 'epoch': 1.32}
{'loss': 2.3979, 'grad_norm': 3.83974552154541, 'learning_rate': 0.00011206172839506172, 'epoch': 1.32}
{'loss': 2.9158, 'grad_norm': 6.013816833496094, 'learning_rate': 0.00011200000000000001, 'epoch': 1.32}
{'loss': 3.1486, 'grad_norm': 6.8623738288879395, 'learning_rate': 0.00011193827160493829, 'epoch': 1.32}
{'loss': 2.7982, 'grad_norm': 5.101633548736572, 'learning_rate': 0.00011187654320987655, 'epoch': 1.32}
{'loss': 2.7596, 'grad_norm': 4.756069183349609, 'learning_rate': 0.00011181481481481482, 'epoch': 1.32}
{'loss': 2.8919, 'grad_norm': 4.46480655670166, 'learning_rate': 0.00011175308641975308, 'epoch': 1.32}
{'loss': 3.1495, 'grad_norm': 5.128878593444824, 'learning_rate': 0.00011169135802469137, 'epoch': 1.33}
{'loss': 2.8295, 'grad_norm': 6.376893520355225, 'learning_rate': 0.00011162962962962964, 'epoch': 1.33}
{'loss': 2.2862, 'grad_norm': 4.1728363037109375, 'learning_rate': 0.0001115679012345679, 'epoch': 1.33}
{'loss': 3.2289, 'grad_norm': 4.775488376617432, 'learning_rate': 0.00011150617283950618, 'epoch': 1.33}
{'loss': 2.2489, 'grad_norm': 4.772634506225586, 'learning_rate': 0.00011144444444444444, 'epoch': 1.33}
{'loss': 2.7118, 'grad_norm': 5.02881383895874, 'learning_rate': 0.00011138271604938272, 'epoch': 1.33}
{'loss': 2.3725, 'grad_norm': 4.861889839172363, 'learning_rate': 0.000111320987654321, 'epoch': 1.33}
{'loss': 2.6071, 'grad_norm': 4.059642791748047, 'learning_rate': 0.00011125925925925926, 'epoch': 1.33}
{'loss': 2.1229, 'grad_norm': 3.932173013687134, 'learning_rate': 0.00011119753086419753, 'epoch': 1.33}
{'loss': 3.5052, 'grad_norm': 7.3952813148498535, 'learning_rate': 0.00011113580246913579, 'epoch': 1.33}
{'loss': 3.0024, 'grad_norm': 7.599935054779053, 'learning_rate': 0.00011107407407407409, 'epoch': 1.34}
{'loss': 2.8046, 'grad_norm': 9.061247825622559, 'learning_rate': 0.00011101234567901235, 'epoch': 1.34}
{'loss': 2.8149, 'grad_norm': 4.797201156616211, 'learning_rate': 0.00011095061728395063, 'epoch': 1.34}
{'loss': 3.0038, 'grad_norm': 5.980061054229736, 'learning_rate': 0.00011088888888888889, 'epoch': 1.34}
{'loss': 2.6581, 'grad_norm': 6.192116737365723, 'learning_rate': 0.00011082716049382717, 'epoch': 1.34}
{'loss': 3.7344, 'grad_norm': 6.608138561248779, 'learning_rate': 0.00011076543209876545, 'epoch': 1.34}
{'loss': 2.9234, 'grad_norm': 7.139713764190674, 'learning_rate': 0.00011070370370370371, 'epoch': 1.34}
{'loss': 2.6754, 'grad_norm': 5.1539506912231445, 'learning_rate': 0.00011064197530864198, 'epoch': 1.34}
{'loss': 3.2286, 'grad_norm': 7.0390706062316895, 'learning_rate': 0.00011058024691358024, 'epoch': 1.34}
{'loss': 3.0618, 'grad_norm': 5.312830924987793, 'learning_rate': 0.00011051851851851853, 'epoch': 1.34}
{'loss': 2.4927, 'grad_norm': 7.304208755493164, 'learning_rate': 0.0001104567901234568, 'epoch': 1.34}
{'loss': 2.3674, 'grad_norm': 4.822638511657715, 'learning_rate': 0.00011039506172839506, 'epoch': 1.35}
{'loss': 3.5308, 'grad_norm': 9.909815788269043, 'learning_rate': 0.00011033333333333334, 'epoch': 1.35}
{'loss': 3.0615, 'grad_norm': 4.465532302856445, 'learning_rate': 0.0001102716049382716, 'epoch': 1.35}
{'loss': 2.8137, 'grad_norm': 5.842231273651123, 'learning_rate': 0.00011020987654320988, 'epoch': 1.35}
{'loss': 2.9597, 'grad_norm': 4.079471588134766, 'learning_rate': 0.00011014814814814816, 'epoch': 1.35}
{'loss': 2.5658, 'grad_norm': 3.9455583095550537, 'learning_rate': 0.00011008641975308642, 'epoch': 1.35}
{'loss': 2.6581, 'grad_norm': 5.803159713745117, 'learning_rate': 0.00011002469135802469, 'epoch': 1.35}
{'loss': 2.8023, 'grad_norm': 3.959970712661743, 'learning_rate': 0.00010996296296296298, 'epoch': 1.35}
{'loss': 2.7407, 'grad_norm': 4.588097095489502, 'learning_rate': 0.00010990123456790124, 'epoch': 1.35}
{'loss': 2.4944, 'grad_norm': 6.985432147979736, 'learning_rate': 0.00010983950617283951, 'epoch': 1.35}
{'loss': 2.9769, 'grad_norm': 5.405830383300781, 'learning_rate': 0.00010977777777777777, 'epoch': 1.35}
{'loss': 3.4694, 'grad_norm': 6.577681064605713, 'learning_rate': 0.00010971604938271605, 'epoch': 1.36}
{'loss': 2.6907, 'grad_norm': 6.414079666137695, 'learning_rate': 0.00010965432098765433, 'epoch': 1.36}
{'loss': 2.5421, 'grad_norm': 4.002150535583496, 'learning_rate': 0.00010959259259259261, 'epoch': 1.36}
{'loss': 2.9323, 'grad_norm': 4.901289939880371, 'learning_rate': 0.00010953086419753087, 'epoch': 1.36}
{'loss': 2.8145, 'grad_norm': 12.97323226928711, 'learning_rate': 0.00010946913580246914, 'epoch': 1.36}
{'loss': 2.6364, 'grad_norm': 7.493730068206787, 'learning_rate': 0.0001094074074074074, 'epoch': 1.36}
{'loss': 2.6327, 'grad_norm': 4.413530349731445, 'learning_rate': 0.00010934567901234569, 'epoch': 1.36}
{'loss': 3.652, 'grad_norm': 4.018875598907471, 'learning_rate': 0.00010928395061728396, 'epoch': 1.36}
{'loss': 2.9283, 'grad_norm': 4.030287742614746, 'learning_rate': 0.00010922222222222222, 'epoch': 1.36}
{'loss': 3.0534, 'grad_norm': 4.755824089050293, 'learning_rate': 0.0001091604938271605, 'epoch': 1.36}
{'loss': 2.5584, 'grad_norm': 4.490635395050049, 'learning_rate': 0.00010909876543209878, 'epoch': 1.36}
{'loss': 3.0304, 'grad_norm': 3.510399341583252, 'learning_rate': 0.00010903703703703705, 'epoch': 1.37}
{'loss': 2.9217, 'grad_norm': 6.373254299163818, 'learning_rate': 0.00010897530864197532, 'epoch': 1.37}
{'loss': 3.2393, 'grad_norm': 18.190698623657227, 'learning_rate': 0.00010891358024691358, 'epoch': 1.37}
{'loss': 2.8771, 'grad_norm': 5.576689720153809, 'learning_rate': 0.00010885185185185185, 'epoch': 1.37}
{'loss': 2.439, 'grad_norm': 4.014941692352295, 'learning_rate': 0.00010879012345679014, 'epoch': 1.37}
{'loss': 2.4891, 'grad_norm': 5.80916690826416, 'learning_rate': 0.0001087283950617284, 'epoch': 1.37}
{'loss': 2.6993, 'grad_norm': 6.824540138244629, 'learning_rate': 0.00010866666666666667, 'epoch': 1.37}
{'loss': 2.5603, 'grad_norm': 12.203055381774902, 'learning_rate': 0.00010860493827160493, 'epoch': 1.37}
{'loss': 2.6787, 'grad_norm': 5.2561259269714355, 'learning_rate': 0.00010854320987654321, 'epoch': 1.37}
{'loss': 2.8102, 'grad_norm': 4.210850238800049, 'learning_rate': 0.0001084814814814815, 'epoch': 1.37}
{'loss': 2.6504, 'grad_norm': 4.630173206329346, 'learning_rate': 0.00010841975308641976, 'epoch': 1.38}
{'loss': 3.0456, 'grad_norm': 5.7290754318237305, 'learning_rate': 0.00010835802469135803, 'epoch': 1.38}
{'loss': 2.6226, 'grad_norm': 3.970344305038452, 'learning_rate': 0.00010829629629629629, 'epoch': 1.38}
{'loss': 2.8495, 'grad_norm': 6.157504081726074, 'learning_rate': 0.00010823456790123456, 'epoch': 1.38}
{'loss': 2.6526, 'grad_norm': 5.888565540313721, 'learning_rate': 0.00010817283950617285, 'epoch': 1.38}
{'loss': 3.1206, 'grad_norm': 6.443511009216309, 'learning_rate': 0.00010811111111111112, 'epoch': 1.38}
{'loss': 2.5063, 'grad_norm': 5.918099403381348, 'learning_rate': 0.00010804938271604939, 'epoch': 1.38}
{'loss': 2.5256, 'grad_norm': 4.563322067260742, 'learning_rate': 0.00010798765432098766, 'epoch': 1.38}
{'loss': 2.8885, 'grad_norm': 5.13684606552124, 'learning_rate': 0.00010792592592592595, 'epoch': 1.38}
{'loss': 2.5562, 'grad_norm': 4.498524188995361, 'learning_rate': 0.0001078641975308642, 'epoch': 1.38}
{'loss': 2.7263, 'grad_norm': 6.378891944885254, 'learning_rate': 0.00010780246913580248, 'epoch': 1.38}
{'loss': 3.3165, 'grad_norm': 18.011247634887695, 'learning_rate': 0.00010774074074074074, 'epoch': 1.39}
{'loss': 2.2068, 'grad_norm': 4.8541741371154785, 'learning_rate': 0.00010767901234567901, 'epoch': 1.39}
{'loss': 2.9009, 'grad_norm': 5.525298595428467, 'learning_rate': 0.0001076172839506173, 'epoch': 1.39}
{'loss': 2.7587, 'grad_norm': 3.9567673206329346, 'learning_rate': 0.00010755555555555556, 'epoch': 1.39}
{'loss': 2.9879, 'grad_norm': 5.465761184692383, 'learning_rate': 0.00010749382716049384, 'epoch': 1.39}
  warnings.warn(                                  
{'eval_loss': 2.743602752685547, 'eval_runtime': 412.8385, 'eval_samples_per_second': 1.453, 'eval_steps_per_second': 1.453, 'epoch': 1.39}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.1959, 'grad_norm': 3.387216806411743, 'learning_rate': 0.0001074320987654321, 'epoch': 1.39}
{'loss': 3.0738, 'grad_norm': 7.398501873016357, 'learning_rate': 0.00010737037037037037, 'epoch': 1.39}
{'loss': 2.9618, 'grad_norm': 5.102400779724121, 'learning_rate': 0.00010730864197530866, 'epoch': 1.39}
{'loss': 2.5189, 'grad_norm': 8.925535202026367, 'learning_rate': 0.00010724691358024692, 'epoch': 1.39}
{'loss': 2.3575, 'grad_norm': 9.096946716308594, 'learning_rate': 0.00010718518518518519, 'epoch': 1.39}
{'loss': 3.3072, 'grad_norm': 7.552216053009033, 'learning_rate': 0.00010712345679012345, 'epoch': 1.39}
{'loss': 2.4021, 'grad_norm': 9.100491523742676, 'learning_rate': 0.00010706172839506174, 'epoch': 1.4}
{'loss': 3.0163, 'grad_norm': 5.621785640716553, 'learning_rate': 0.00010700000000000001, 'epoch': 1.4}
{'loss': 3.4507, 'grad_norm': 6.659276008605957, 'learning_rate': 0.00010693827160493827, 'epoch': 1.4}
{'loss': 3.1879, 'grad_norm': 6.0401458740234375, 'learning_rate': 0.00010687654320987655, 'epoch': 1.4}
{'loss': 2.6432, 'grad_norm': 5.118752479553223, 'learning_rate': 0.0001068148148148148, 'epoch': 1.4}
{'loss': 2.9304, 'grad_norm': 8.050430297851562, 'learning_rate': 0.00010675308641975311, 'epoch': 1.4}
{'loss': 2.9304, 'grad_norm': 4.188756465911865, 'learning_rate': 0.00010669135802469137, 'epoch': 1.4}
{'loss': 3.0183, 'grad_norm': 6.167237758636475, 'learning_rate': 0.00010662962962962964, 'epoch': 1.4}
{'loss': 3.626, 'grad_norm': 4.534696102142334, 'learning_rate': 0.0001065679012345679, 'epoch': 1.4}
{'loss': 2.548, 'grad_norm': 3.8604447841644287, 'learning_rate': 0.00010650617283950616, 'epoch': 1.4}
{'loss': 3.1791, 'grad_norm': 7.0282392501831055, 'learning_rate': 0.00010644444444444446, 'epoch': 1.4}
{'loss': 2.7047, 'grad_norm': 6.01727294921875, 'learning_rate': 0.00010638271604938272, 'epoch': 1.41}
{'loss': 2.5922, 'grad_norm': 4.4654011726379395, 'learning_rate': 0.000106320987654321, 'epoch': 1.41}
{'loss': 2.6957, 'grad_norm': 8.725998878479004, 'learning_rate': 0.00010625925925925926, 'epoch': 1.41}
{'loss': 3.3163, 'grad_norm': 6.3091912269592285, 'learning_rate': 0.00010619753086419753, 'epoch': 1.41}
{'loss': 2.9581, 'grad_norm': 7.72885274887085, 'learning_rate': 0.00010613580246913582, 'epoch': 1.41}
{'loss': 2.8486, 'grad_norm': 5.300862789154053, 'learning_rate': 0.00010607407407407408, 'epoch': 1.41}
{'loss': 2.9549, 'grad_norm': 8.964607238769531, 'learning_rate': 0.00010601234567901235, 'epoch': 1.41}
{'loss': 2.6171, 'grad_norm': 5.247945785522461, 'learning_rate': 0.00010595061728395061, 'epoch': 1.41}
{'loss': 2.6168, 'grad_norm': 3.96821928024292, 'learning_rate': 0.0001058888888888889, 'epoch': 1.41}
{'loss': 2.8254, 'grad_norm': 6.731603145599365, 'learning_rate': 0.00010582716049382717, 'epoch': 1.41}
{'loss': 2.5338, 'grad_norm': 7.032258987426758, 'learning_rate': 0.00010576543209876543, 'epoch': 1.41}
{'loss': 2.5158, 'grad_norm': 4.065962791442871, 'learning_rate': 0.00010570370370370371, 'epoch': 1.42}
{'loss': 2.577, 'grad_norm': 4.282586097717285, 'learning_rate': 0.00010564197530864197, 'epoch': 1.42}
{'loss': 3.017, 'grad_norm': 7.0423383712768555, 'learning_rate': 0.00010558024691358026, 'epoch': 1.42}
{'loss': 2.5892, 'grad_norm': 5.636713981628418, 'learning_rate': 0.00010551851851851853, 'epoch': 1.42}
{'loss': 2.8787, 'grad_norm': 4.291885852813721, 'learning_rate': 0.00010545679012345679, 'epoch': 1.42}
{'loss': 2.91, 'grad_norm': 6.619272232055664, 'learning_rate': 0.00010539506172839506, 'epoch': 1.42}
{'loss': 2.661, 'grad_norm': 5.430596828460693, 'learning_rate': 0.00010533333333333332, 'epoch': 1.42}
{'loss': 3.6074, 'grad_norm': 5.3380126953125, 'learning_rate': 0.00010527160493827162, 'epoch': 1.42}
{'loss': 3.2978, 'grad_norm': 6.026320934295654, 'learning_rate': 0.00010520987654320988, 'epoch': 1.42}
{'loss': 2.6797, 'grad_norm': 3.6427996158599854, 'learning_rate': 0.00010514814814814814, 'epoch': 1.42}
{'loss': 2.5961, 'grad_norm': 3.68554425239563, 'learning_rate': 0.00010508641975308642, 'epoch': 1.43}
{'loss': 2.3814, 'grad_norm': 3.970388174057007, 'learning_rate': 0.0001050246913580247, 'epoch': 1.43}
{'loss': 3.0909, 'grad_norm': 5.774307727813721, 'learning_rate': 0.00010496296296296298, 'epoch': 1.43}
{'loss': 2.4805, 'grad_norm': 4.363312721252441, 'learning_rate': 0.00010490123456790124, 'epoch': 1.43}
{'loss': 3.0174, 'grad_norm': 4.624432563781738, 'learning_rate': 0.00010483950617283951, 'epoch': 1.43}
{'loss': 2.981, 'grad_norm': 7.402371406555176, 'learning_rate': 0.00010477777777777777, 'epoch': 1.43}
{'loss': 3.0335, 'grad_norm': 5.053544521331787, 'learning_rate': 0.00010471604938271606, 'epoch': 1.43}
{'loss': 2.8807, 'grad_norm': 3.9186995029449463, 'learning_rate': 0.00010465432098765433, 'epoch': 1.43}
{'loss': 2.4123, 'grad_norm': 5.454326152801514, 'learning_rate': 0.0001045925925925926, 'epoch': 1.43}
{'loss': 2.8255, 'grad_norm': 4.758546829223633, 'learning_rate': 0.00010453086419753087, 'epoch': 1.43}
{'loss': 2.6366, 'grad_norm': 8.78990364074707, 'learning_rate': 0.00010446913580246913, 'epoch': 1.43}
{'loss': 2.7453, 'grad_norm': 6.023383140563965, 'learning_rate': 0.00010440740740740742, 'epoch': 1.44}
{'loss': 3.1222, 'grad_norm': 3.9360148906707764, 'learning_rate': 0.00010434567901234569, 'epoch': 1.44}
{'loss': 2.4312, 'grad_norm': 6.881426811218262, 'learning_rate': 0.00010428395061728395, 'epoch': 1.44}
{'loss': 2.8556, 'grad_norm': 9.901697158813477, 'learning_rate': 0.00010422222222222222, 'epoch': 1.44}
{'loss': 2.4073, 'grad_norm': 5.669589996337891, 'learning_rate': 0.00010416049382716048, 'epoch': 1.44}
{'loss': 2.1826, 'grad_norm': 6.309995651245117, 'learning_rate': 0.00010409876543209877, 'epoch': 1.44}
{'loss': 2.2655, 'grad_norm': 6.081586837768555, 'learning_rate': 0.00010403703703703705, 'epoch': 1.44}
{'loss': 3.0184, 'grad_norm': 6.987484455108643, 'learning_rate': 0.0001039753086419753, 'epoch': 1.44}
{'loss': 2.4113, 'grad_norm': 4.633395195007324, 'learning_rate': 0.00010391358024691358, 'epoch': 1.44}
{'loss': 2.6341, 'grad_norm': 7.092130661010742, 'learning_rate': 0.00010385185185185187, 'epoch': 1.44}
{'loss': 3.3316, 'grad_norm': 7.855164527893066, 'learning_rate': 0.00010379012345679013, 'epoch': 1.44}
{'loss': 2.9778, 'grad_norm': 5.927936553955078, 'learning_rate': 0.0001037283950617284, 'epoch': 1.45}
{'loss': 2.6399, 'grad_norm': 5.029228687286377, 'learning_rate': 0.00010366666666666666, 'epoch': 1.45}
{'loss': 2.9745, 'grad_norm': 5.362082004547119, 'learning_rate': 0.00010360493827160493, 'epoch': 1.45}
{'loss': 3.0137, 'grad_norm': 5.744594573974609, 'learning_rate': 0.00010354320987654322, 'epoch': 1.45}
{'loss': 2.8341, 'grad_norm': 3.8637609481811523, 'learning_rate': 0.0001034814814814815, 'epoch': 1.45}
{'loss': 2.2706, 'grad_norm': 4.970106601715088, 'learning_rate': 0.00010341975308641976, 'epoch': 1.45}
{'loss': 2.9491, 'grad_norm': 5.1493988037109375, 'learning_rate': 0.00010335802469135803, 'epoch': 1.45}
{'loss': 2.5887, 'grad_norm': 4.154726982116699, 'learning_rate': 0.00010329629629629629, 'epoch': 1.45}
{'loss': 2.9489, 'grad_norm': 7.182861804962158, 'learning_rate': 0.00010323456790123458, 'epoch': 1.45}
{'loss': 3.2079, 'grad_norm': 7.043064594268799, 'learning_rate': 0.00010317283950617285, 'epoch': 1.45}
{'loss': 2.5876, 'grad_norm': 3.806750774383545, 'learning_rate': 0.00010311111111111111, 'epoch': 1.45}
{'loss': 2.6353, 'grad_norm': 3.6839654445648193, 'learning_rate': 0.00010304938271604939, 'epoch': 1.46}
{'loss': 3.2603, 'grad_norm': 6.240415096282959, 'learning_rate': 0.00010298765432098767, 'epoch': 1.46}
{'loss': 2.7659, 'grad_norm': 5.3943562507629395, 'learning_rate': 0.00010292592592592593, 'epoch': 1.46}
{'loss': 3.0476, 'grad_norm': 8.925590515136719, 'learning_rate': 0.00010286419753086421, 'epoch': 1.46}
{'loss': 2.8241, 'grad_norm': 5.6234941482543945, 'learning_rate': 0.00010280246913580247, 'epoch': 1.46}
{'loss': 2.4727, 'grad_norm': 5.140562057495117, 'learning_rate': 0.00010274074074074074, 'epoch': 1.46}
{'loss': 3.0605, 'grad_norm': 4.916968822479248, 'learning_rate': 0.00010267901234567903, 'epoch': 1.46}
{'loss': 2.6473, 'grad_norm': 4.9809465408325195, 'learning_rate': 0.00010261728395061729, 'epoch': 1.46}
{'loss': 2.5778, 'grad_norm': 5.251540660858154, 'learning_rate': 0.00010255555555555556, 'epoch': 1.46}
{'loss': 3.0593, 'grad_norm': 9.22990608215332, 'learning_rate': 0.00010249382716049382, 'epoch': 1.46}
{'loss': 2.5665, 'grad_norm': 4.022624492645264, 'learning_rate': 0.0001024320987654321, 'epoch': 1.46}
{'loss': 3.039, 'grad_norm': 5.676851749420166, 'learning_rate': 0.00010237037037037038, 'epoch': 1.47}
{'loss': 2.9275, 'grad_norm': 4.027693271636963, 'learning_rate': 0.00010230864197530864, 'epoch': 1.47}
{'loss': 3.6182, 'grad_norm': 4.017908573150635, 'learning_rate': 0.00010224691358024692, 'epoch': 1.47}
{'loss': 2.9852, 'grad_norm': 6.2643280029296875, 'learning_rate': 0.00010218518518518518, 'epoch': 1.47}
{'loss': 3.2096, 'grad_norm': 7.774261951446533, 'learning_rate': 0.00010212345679012345, 'epoch': 1.47}
{'loss': 2.8776, 'grad_norm': 4.638683319091797, 'learning_rate': 0.00010206172839506174, 'epoch': 1.47}
{'loss': 2.6481, 'grad_norm': 3.425109386444092, 'learning_rate': 0.00010200000000000001, 'epoch': 1.47}
{'loss': 2.3615, 'grad_norm': 5.079549789428711, 'learning_rate': 0.00010193827160493827, 'epoch': 1.47}
{'loss': 2.8631, 'grad_norm': 6.25885534286499, 'learning_rate': 0.00010187654320987655, 'epoch': 1.47}
{'loss': 2.7365, 'grad_norm': 4.311738014221191, 'learning_rate': 0.00010181481481481483, 'epoch': 1.47}
{'loss': 2.4851, 'grad_norm': 4.944666385650635, 'learning_rate': 0.0001017530864197531, 'epoch': 1.48}
{'loss': 3.2065, 'grad_norm': 9.268774032592773, 'learning_rate': 0.00010169135802469137, 'epoch': 1.48}
{'loss': 3.3269, 'grad_norm': 7.280538082122803, 'learning_rate': 0.00010162962962962963, 'epoch': 1.48}
{'loss': 2.4873, 'grad_norm': 5.450625419616699, 'learning_rate': 0.0001015679012345679, 'epoch': 1.48}
{'loss': 3.1303, 'grad_norm': 6.111621856689453, 'learning_rate': 0.00010150617283950619, 'epoch': 1.48}
{'loss': 3.2329, 'grad_norm': 7.7882819175720215, 'learning_rate': 0.00010144444444444445, 'epoch': 1.48}
{'loss': 3.6735, 'grad_norm': 8.266863822937012, 'learning_rate': 0.00010138271604938272, 'epoch': 1.48}
{'loss': 3.13, 'grad_norm': 5.4510955810546875, 'learning_rate': 0.00010132098765432098, 'epoch': 1.48}
  warnings.warn(                                  
{'eval_loss': 2.7335164546966553, 'eval_runtime': 413.2211, 'eval_samples_per_second': 1.452, 'eval_steps_per_second': 1.452, 'epoch': 1.48}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.7436, 'grad_norm': 6.134002685546875, 'learning_rate': 0.00010125925925925926, 'epoch': 1.48}
{'loss': 2.119, 'grad_norm': 3.390202283859253, 'learning_rate': 0.00010119753086419754, 'epoch': 1.48}
{'loss': 2.882, 'grad_norm': 6.191496849060059, 'learning_rate': 0.0001011358024691358, 'epoch': 1.48}
{'loss': 2.6227, 'grad_norm': 4.004942893981934, 'learning_rate': 0.00010107407407407408, 'epoch': 1.49}
{'loss': 2.6077, 'grad_norm': 5.253493785858154, 'learning_rate': 0.00010101234567901234, 'epoch': 1.49}
{'loss': 2.8184, 'grad_norm': 3.365100860595703, 'learning_rate': 0.00010095061728395063, 'epoch': 1.49}
{'loss': 3.1224, 'grad_norm': 6.663740634918213, 'learning_rate': 0.0001008888888888889, 'epoch': 1.49}
{'loss': 3.1104, 'grad_norm': 7.367244243621826, 'learning_rate': 0.00010082716049382716, 'epoch': 1.49}
{'loss': 2.5911, 'grad_norm': 4.490209102630615, 'learning_rate': 0.00010076543209876543, 'epoch': 1.49}
{'loss': 3.7898, 'grad_norm': 5.928553104400635, 'learning_rate': 0.0001007037037037037, 'epoch': 1.49}
{'loss': 2.8219, 'grad_norm': 5.874213695526123, 'learning_rate': 0.000100641975308642, 'epoch': 1.49}
{'loss': 2.8715, 'grad_norm': 6.089649677276611, 'learning_rate': 0.00010058024691358026, 'epoch': 1.49}
{'loss': 3.5831, 'grad_norm': 11.673712730407715, 'learning_rate': 0.00010051851851851853, 'epoch': 1.49}
{'loss': 2.4849, 'grad_norm': 5.005722999572754, 'learning_rate': 0.00010045679012345679, 'epoch': 1.49}
{'loss': 2.918, 'grad_norm': 3.868197202682495, 'learning_rate': 0.00010039506172839506, 'epoch': 1.5}
{'loss': 3.2269, 'grad_norm': 4.535712242126465, 'learning_rate': 0.00010033333333333335, 'epoch': 1.5}
{'loss': 3.1073, 'grad_norm': 4.655774116516113, 'learning_rate': 0.00010027160493827161, 'epoch': 1.5}
{'loss': 2.7021, 'grad_norm': 5.361936092376709, 'learning_rate': 0.00010020987654320988, 'epoch': 1.5}
{'loss': 3.1455, 'grad_norm': 3.978445291519165, 'learning_rate': 0.00010014814814814814, 'epoch': 1.5}
{'loss': 2.4634, 'grad_norm': 3.980351209640503, 'learning_rate': 0.00010008641975308643, 'epoch': 1.5}
{'loss': 2.9611, 'grad_norm': 8.380892753601074, 'learning_rate': 0.0001000246913580247, 'epoch': 1.5}
{'loss': 3.1852, 'grad_norm': 9.451781272888184, 'learning_rate': 9.996296296296297e-05, 'epoch': 1.5}
{'loss': 2.1869, 'grad_norm': 6.898054599761963, 'learning_rate': 9.990123456790124e-05, 'epoch': 1.5}
{'loss': 2.3464, 'grad_norm': 3.7054502964019775, 'learning_rate': 9.983950617283951e-05, 'epoch': 1.5}
{'loss': 2.1278, 'grad_norm': 4.064563274383545, 'learning_rate': 9.977777777777779e-05, 'epoch': 1.5}
{'loss': 2.8048, 'grad_norm': 4.656005382537842, 'learning_rate': 9.971604938271606e-05, 'epoch': 1.51}
{'loss': 2.8796, 'grad_norm': 6.661139965057373, 'learning_rate': 9.965432098765432e-05, 'epoch': 1.51}
{'loss': 2.5664, 'grad_norm': 6.667051315307617, 'learning_rate': 9.95925925925926e-05, 'epoch': 1.51}
{'loss': 2.4812, 'grad_norm': 7.6701154708862305, 'learning_rate': 9.954320987654322e-05, 'epoch': 1.51}
{'loss': 1.9544, 'grad_norm': 4.076060771942139, 'learning_rate': 9.948148148148148e-05, 'epoch': 1.51}
{'loss': 3.1333, 'grad_norm': 8.42214298248291, 'learning_rate': 9.941975308641975e-05, 'epoch': 1.51}
{'loss': 2.8865, 'grad_norm': 5.35686731338501, 'learning_rate': 9.935802469135803e-05, 'epoch': 1.51}
{'loss': 3.5848, 'grad_norm': 8.013984680175781, 'learning_rate': 9.92962962962963e-05, 'epoch': 1.51}
{'loss': 3.3789, 'grad_norm': 6.231503486633301, 'learning_rate': 9.923456790123458e-05, 'epoch': 1.51}
{'loss': 2.8514, 'grad_norm': 7.615835666656494, 'learning_rate': 9.917283950617285e-05, 'epoch': 1.51}
{'loss': 2.3314, 'grad_norm': 5.381460666656494, 'learning_rate': 9.911111111111112e-05, 'epoch': 1.51}
{'loss': 3.0525, 'grad_norm': 4.393453598022461, 'learning_rate': 9.904938271604938e-05, 'epoch': 1.52}
{'loss': 2.6705, 'grad_norm': 5.301303863525391, 'learning_rate': 9.898765432098766e-05, 'epoch': 1.52}
{'loss': 3.3041, 'grad_norm': 12.200675010681152, 'learning_rate': 9.892592592592593e-05, 'epoch': 1.52}
{'loss': 2.925, 'grad_norm': 4.95964241027832, 'learning_rate': 9.88641975308642e-05, 'epoch': 1.52}
{'loss': 2.9427, 'grad_norm': 4.5863447189331055, 'learning_rate': 9.880246913580248e-05, 'epoch': 1.52}
{'loss': 2.9919, 'grad_norm': 5.1499528884887695, 'learning_rate': 9.874074074074074e-05, 'epoch': 1.52}
{'loss': 2.9476, 'grad_norm': 5.17536735534668, 'learning_rate': 9.867901234567901e-05, 'epoch': 1.52}
{'loss': 2.1243, 'grad_norm': 4.376731872558594, 'learning_rate': 9.861728395061729e-05, 'epoch': 1.52}
{'loss': 2.662, 'grad_norm': 5.262794017791748, 'learning_rate': 9.855555555555556e-05, 'epoch': 1.52}
{'loss': 2.9639, 'grad_norm': 5.25671911239624, 'learning_rate': 9.849382716049383e-05, 'epoch': 1.52}
{'loss': 2.776, 'grad_norm': 4.594629287719727, 'learning_rate': 9.843209876543211e-05, 'epoch': 1.52}
{'loss': 2.7395, 'grad_norm': 5.054198265075684, 'learning_rate': 9.837037037037038e-05, 'epoch': 1.53}
{'loss': 2.5437, 'grad_norm': 4.318439960479736, 'learning_rate': 9.830864197530864e-05, 'epoch': 1.53}
{'loss': 2.8575, 'grad_norm': 4.503694534301758, 'learning_rate': 9.824691358024692e-05, 'epoch': 1.53}
{'loss': 3.3964, 'grad_norm': 6.290829658508301, 'learning_rate': 9.818518518518519e-05, 'epoch': 1.53}
{'loss': 2.4208, 'grad_norm': 5.8201165199279785, 'learning_rate': 9.812345679012346e-05, 'epoch': 1.53}
{'loss': 2.4347, 'grad_norm': 4.256030082702637, 'learning_rate': 9.806172839506174e-05, 'epoch': 1.53}
{'loss': 2.8057, 'grad_norm': 6.033120155334473, 'learning_rate': 9.8e-05, 'epoch': 1.53}
{'loss': 2.878, 'grad_norm': 8.515059471130371, 'learning_rate': 9.793827160493828e-05, 'epoch': 1.53}
{'loss': 3.0854, 'grad_norm': 5.343039035797119, 'learning_rate': 9.787654320987654e-05, 'epoch': 1.53}
{'loss': 2.9636, 'grad_norm': 3.5790956020355225, 'learning_rate': 9.781481481481482e-05, 'epoch': 1.53}
{'loss': 2.462, 'grad_norm': 4.393999099731445, 'learning_rate': 9.775308641975309e-05, 'epoch': 1.54}
{'loss': 2.7319, 'grad_norm': 5.34478759765625, 'learning_rate': 9.769135802469137e-05, 'epoch': 1.54}
{'loss': 2.9086, 'grad_norm': 5.790796756744385, 'learning_rate': 9.762962962962964e-05, 'epoch': 1.54}
{'loss': 3.439, 'grad_norm': 7.8890557289123535, 'learning_rate': 9.75679012345679e-05, 'epoch': 1.54}
{'loss': 3.0328, 'grad_norm': 4.268129348754883, 'learning_rate': 9.750617283950619e-05, 'epoch': 1.54}
{'loss': 3.0443, 'grad_norm': 6.001567363739014, 'learning_rate': 9.744444444444445e-05, 'epoch': 1.54}
{'loss': 3.0434, 'grad_norm': 5.386470794677734, 'learning_rate': 9.738271604938272e-05, 'epoch': 1.54}
{'loss': 3.3582, 'grad_norm': 5.069033145904541, 'learning_rate': 9.7320987654321e-05, 'epoch': 1.54}
{'loss': 2.8481, 'grad_norm': 4.001532077789307, 'learning_rate': 9.725925925925926e-05, 'epoch': 1.54}
{'loss': 3.0401, 'grad_norm': 9.831332206726074, 'learning_rate': 9.719753086419754e-05, 'epoch': 1.54}
{'loss': 3.1081, 'grad_norm': 4.741168022155762, 'learning_rate': 9.71358024691358e-05, 'epoch': 1.54}
{'loss': 2.5868, 'grad_norm': 4.71490478515625, 'learning_rate': 9.707407407407409e-05, 'epoch': 1.55}
{'loss': 3.0205, 'grad_norm': 4.066258907318115, 'learning_rate': 9.701234567901235e-05, 'epoch': 1.55}
{'loss': 3.0769, 'grad_norm': 6.503936290740967, 'learning_rate': 9.695061728395062e-05, 'epoch': 1.55}
{'loss': 2.5256, 'grad_norm': 5.0252909660339355, 'learning_rate': 9.68888888888889e-05, 'epoch': 1.55}
{'loss': 2.8927, 'grad_norm': 5.368664264678955, 'learning_rate': 9.682716049382716e-05, 'epoch': 1.55}
{'loss': 2.9403, 'grad_norm': 4.022599697113037, 'learning_rate': 9.676543209876545e-05, 'epoch': 1.55}
{'loss': 2.7457, 'grad_norm': 3.5631232261657715, 'learning_rate': 9.67037037037037e-05, 'epoch': 1.55}
{'loss': 3.0223, 'grad_norm': 4.767518520355225, 'learning_rate': 9.664197530864198e-05, 'epoch': 1.55}
{'loss': 2.9003, 'grad_norm': 5.9318132400512695, 'learning_rate': 9.658024691358025e-05, 'epoch': 1.55}
{'loss': 3.9107, 'grad_norm': 4.00067663192749, 'learning_rate': 9.651851851851851e-05, 'epoch': 1.55}
{'loss': 2.7622, 'grad_norm': 5.223605632781982, 'learning_rate': 9.64567901234568e-05, 'epoch': 1.55}
{'loss': 2.0477, 'grad_norm': 3.0932250022888184, 'learning_rate': 9.639506172839506e-05, 'epoch': 1.56}
{'loss': 3.0535, 'grad_norm': 7.901895999908447, 'learning_rate': 9.633333333333335e-05, 'epoch': 1.56}
{'loss': 2.9297, 'grad_norm': 6.4299092292785645, 'learning_rate': 9.627160493827161e-05, 'epoch': 1.56}
{'loss': 3.0703, 'grad_norm': 9.82766342163086, 'learning_rate': 9.620987654320988e-05, 'epoch': 1.56}
{'loss': 3.0511, 'grad_norm': 4.878182411193848, 'learning_rate': 9.614814814814816e-05, 'epoch': 1.56}
{'loss': 2.728, 'grad_norm': 5.1068549156188965, 'learning_rate': 9.608641975308642e-05, 'epoch': 1.56}
{'loss': 2.6121, 'grad_norm': 4.518481254577637, 'learning_rate': 9.60246913580247e-05, 'epoch': 1.56}
{'loss': 2.894, 'grad_norm': 5.080348491668701, 'learning_rate': 9.596296296296296e-05, 'epoch': 1.56}
{'loss': 2.6731, 'grad_norm': 6.923649787902832, 'learning_rate': 9.590123456790124e-05, 'epoch': 1.56}
{'loss': 3.4192, 'grad_norm': 5.944960594177246, 'learning_rate': 9.583950617283951e-05, 'epoch': 1.56}
{'loss': 2.9248, 'grad_norm': 7.135940074920654, 'learning_rate': 9.577777777777777e-05, 'epoch': 1.56}
{'loss': 2.8745, 'grad_norm': 5.599444389343262, 'learning_rate': 9.571604938271606e-05, 'epoch': 1.57}
{'loss': 2.5759, 'grad_norm': 5.680606842041016, 'learning_rate': 9.565432098765432e-05, 'epoch': 1.57}
{'loss': 3.0974, 'grad_norm': 4.421505928039551, 'learning_rate': 9.55925925925926e-05, 'epoch': 1.57}
{'loss': 2.8954, 'grad_norm': 4.694873809814453, 'learning_rate': 9.553086419753087e-05, 'epoch': 1.57}
{'loss': 2.9393, 'grad_norm': 5.451996326446533, 'learning_rate': 9.546913580246914e-05, 'epoch': 1.57}
{'loss': 2.672, 'grad_norm': 6.021378040313721, 'learning_rate': 9.540740740740741e-05, 'epoch': 1.57}
{'loss': 3.2511, 'grad_norm': 6.004204750061035, 'learning_rate': 9.534567901234567e-05, 'epoch': 1.57}
{'loss': 2.5029, 'grad_norm': 10.419918060302734, 'learning_rate': 9.528395061728396e-05, 'epoch': 1.57}
{'loss': 2.3829, 'grad_norm': 7.493120193481445, 'learning_rate': 9.522222222222222e-05, 'epoch': 1.57}
{'loss': 3.4791, 'grad_norm': 6.051782131195068, 'learning_rate': 9.51604938271605e-05, 'epoch': 1.57}
  warnings.warn(                                  
{'eval_loss': 2.7310755252838135, 'eval_runtime': 413.3296, 'eval_samples_per_second': 1.452, 'eval_steps_per_second': 1.452, 'epoch': 1.57}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.0824, 'grad_norm': 4.181055545806885, 'learning_rate': 9.509876543209877e-05, 'epoch': 1.57}
{'loss': 2.2693, 'grad_norm': 6.675682544708252, 'learning_rate': 9.503703703703704e-05, 'epoch': 1.58}
{'loss': 2.689, 'grad_norm': 5.390361309051514, 'learning_rate': 9.497530864197532e-05, 'epoch': 1.58}
{'loss': 2.636, 'grad_norm': 3.8234004974365234, 'learning_rate': 9.491358024691358e-05, 'epoch': 1.58}
{'loss': 3.3889, 'grad_norm': 9.33552074432373, 'learning_rate': 9.485185185185187e-05, 'epoch': 1.58}
{'loss': 2.7693, 'grad_norm': 3.600109100341797, 'learning_rate': 9.479012345679013e-05, 'epoch': 1.58}
{'loss': 2.7713, 'grad_norm': 4.690422058105469, 'learning_rate': 9.47283950617284e-05, 'epoch': 1.58}
{'loss': 3.2117, 'grad_norm': 5.44545841217041, 'learning_rate': 9.466666666666667e-05, 'epoch': 1.58}
{'loss': 3.1906, 'grad_norm': 4.4838786125183105, 'learning_rate': 9.460493827160495e-05, 'epoch': 1.58}
{'loss': 3.2517, 'grad_norm': 4.761634826660156, 'learning_rate': 9.454320987654322e-05, 'epoch': 1.58}
{'loss': 2.416, 'grad_norm': 4.831271648406982, 'learning_rate': 9.448148148148148e-05, 'epoch': 1.58}
{'loss': 2.4209, 'grad_norm': 5.4223313331604, 'learning_rate': 9.441975308641975e-05, 'epoch': 1.59}
{'loss': 2.9732, 'grad_norm': 5.037059307098389, 'learning_rate': 9.435802469135803e-05, 'epoch': 1.59}
{'loss': 2.6062, 'grad_norm': 3.696164131164551, 'learning_rate': 9.42962962962963e-05, 'epoch': 1.59}
{'loss': 3.1977, 'grad_norm': 4.831305980682373, 'learning_rate': 9.423456790123458e-05, 'epoch': 1.59}
{'loss': 2.9358, 'grad_norm': 9.262514114379883, 'learning_rate': 9.417283950617284e-05, 'epoch': 1.59}
{'loss': 3.1855, 'grad_norm': 4.4839043617248535, 'learning_rate': 9.411111111111111e-05, 'epoch': 1.59}
{'loss': 3.0085, 'grad_norm': 6.485188961029053, 'learning_rate': 9.404938271604938e-05, 'epoch': 1.59}
{'loss': 2.6689, 'grad_norm': 4.4732985496521, 'learning_rate': 9.398765432098766e-05, 'epoch': 1.59}
{'loss': 3.1959, 'grad_norm': 4.069614887237549, 'learning_rate': 9.392592592592593e-05, 'epoch': 1.59}
{'loss': 2.9822, 'grad_norm': 6.547396659851074, 'learning_rate': 9.38641975308642e-05, 'epoch': 1.59}
{'loss': 3.2673, 'grad_norm': 5.032817363739014, 'learning_rate': 9.380246913580248e-05, 'epoch': 1.59}
{'loss': 3.3008, 'grad_norm': 3.9359028339385986, 'learning_rate': 9.374074074074074e-05, 'epoch': 1.6}
{'loss': 2.6041, 'grad_norm': 6.272861480712891, 'learning_rate': 9.367901234567901e-05, 'epoch': 1.6}
{'loss': 3.7164, 'grad_norm': 9.193404197692871, 'learning_rate': 9.361728395061729e-05, 'epoch': 1.6}
{'loss': 2.78, 'grad_norm': 7.724045276641846, 'learning_rate': 9.355555555555556e-05, 'epoch': 1.6}
{'loss': 2.9029, 'grad_norm': 6.066655158996582, 'learning_rate': 9.349382716049383e-05, 'epoch': 1.6}
{'loss': 3.2737, 'grad_norm': 4.795244216918945, 'learning_rate': 9.343209876543211e-05, 'epoch': 1.6}
{'loss': 3.4026, 'grad_norm': 32.369651794433594, 'learning_rate': 9.337037037037037e-05, 'epoch': 1.6}
{'loss': 2.7857, 'grad_norm': 4.244955062866211, 'learning_rate': 9.330864197530864e-05, 'epoch': 1.6}
{'loss': 3.0209, 'grad_norm': 7.400398254394531, 'learning_rate': 9.324691358024692e-05, 'epoch': 1.6}
{'loss': 2.8201, 'grad_norm': 5.042361259460449, 'learning_rate': 9.318518518518519e-05, 'epoch': 1.6}
{'loss': 3.149, 'grad_norm': 5.601799488067627, 'learning_rate': 9.312345679012346e-05, 'epoch': 1.6}
{'loss': 2.5652, 'grad_norm': 5.0186872482299805, 'learning_rate': 9.306172839506174e-05, 'epoch': 1.61}
{'loss': 2.6358, 'grad_norm': 7.304718494415283, 'learning_rate': 9.300000000000001e-05, 'epoch': 1.61}
{'loss': 2.7031, 'grad_norm': 8.917459487915039, 'learning_rate': 9.293827160493827e-05, 'epoch': 1.61}
{'loss': 2.5523, 'grad_norm': 4.850688457489014, 'learning_rate': 9.287654320987654e-05, 'epoch': 1.61}
{'loss': 2.9725, 'grad_norm': 6.843498229980469, 'learning_rate': 9.281481481481482e-05, 'epoch': 1.61}
{'loss': 3.3991, 'grad_norm': 5.0911970138549805, 'learning_rate': 9.275308641975309e-05, 'epoch': 1.61}
{'loss': 3.1414, 'grad_norm': 6.305418491363525, 'learning_rate': 9.269135802469137e-05, 'epoch': 1.61}
{'loss': 2.5126, 'grad_norm': 5.243459224700928, 'learning_rate': 9.262962962962963e-05, 'epoch': 1.61}
{'loss': 3.1137, 'grad_norm': 5.244443416595459, 'learning_rate': 9.256790123456791e-05, 'epoch': 1.61}
{'loss': 2.8941, 'grad_norm': 6.111240386962891, 'learning_rate': 9.250617283950617e-05, 'epoch': 1.61}
{'loss': 3.1475, 'grad_norm': 4.940302848815918, 'learning_rate': 9.244444444444445e-05, 'epoch': 1.61}
{'loss': 2.4692, 'grad_norm': 7.881680011749268, 'learning_rate': 9.238271604938272e-05, 'epoch': 1.62}
{'loss': 2.4609, 'grad_norm': 4.3132710456848145, 'learning_rate': 9.2320987654321e-05, 'epoch': 1.62}
{'loss': 2.5294, 'grad_norm': 4.485293865203857, 'learning_rate': 9.225925925925927e-05, 'epoch': 1.62}
{'loss': 2.7917, 'grad_norm': 5.635692596435547, 'learning_rate': 9.219753086419753e-05, 'epoch': 1.62}
{'loss': 3.1382, 'grad_norm': 5.038393020629883, 'learning_rate': 9.213580246913582e-05, 'epoch': 1.62}
{'loss': 3.3588, 'grad_norm': 8.705188751220703, 'learning_rate': 9.207407407407408e-05, 'epoch': 1.62}
{'loss': 2.8526, 'grad_norm': 4.309686183929443, 'learning_rate': 9.201234567901235e-05, 'epoch': 1.62}
{'loss': 3.2428, 'grad_norm': 5.303527355194092, 'learning_rate': 9.195061728395062e-05, 'epoch': 1.62}
{'loss': 2.9877, 'grad_norm': 5.70243501663208, 'learning_rate': 9.188888888888888e-05, 'epoch': 1.62}
{'loss': 2.443, 'grad_norm': 5.087866306304932, 'learning_rate': 9.182716049382717e-05, 'epoch': 1.62}
{'loss': 2.6647, 'grad_norm': 4.353459358215332, 'learning_rate': 9.176543209876543e-05, 'epoch': 1.62}
{'loss': 3.8098, 'grad_norm': 8.797650337219238, 'learning_rate': 9.17037037037037e-05, 'epoch': 1.63}
{'loss': 2.7579, 'grad_norm': 6.001735210418701, 'learning_rate': 9.164197530864198e-05, 'epoch': 1.63}
{'loss': 2.5777, 'grad_norm': 7.7805681228637695, 'learning_rate': 9.158024691358025e-05, 'epoch': 1.63}
{'loss': 3.0734, 'grad_norm': 5.685632705688477, 'learning_rate': 9.151851851851853e-05, 'epoch': 1.63}
{'loss': 2.9647, 'grad_norm': 4.1368865966796875, 'learning_rate': 9.145679012345679e-05, 'epoch': 1.63}
{'loss': 2.6636, 'grad_norm': 3.3984580039978027, 'learning_rate': 9.139506172839508e-05, 'epoch': 1.63}
{'loss': 2.1938, 'grad_norm': 5.3962202072143555, 'learning_rate': 9.133333333333334e-05, 'epoch': 1.63}
{'loss': 2.9272, 'grad_norm': 3.922544240951538, 'learning_rate': 9.127160493827161e-05, 'epoch': 1.63}
{'loss': 2.5599, 'grad_norm': 5.884434223175049, 'learning_rate': 9.120987654320988e-05, 'epoch': 1.63}
{'loss': 3.0422, 'grad_norm': 4.4453911781311035, 'learning_rate': 9.114814814814814e-05, 'epoch': 1.63}
{'loss': 2.5715, 'grad_norm': 9.046330451965332, 'learning_rate': 9.108641975308643e-05, 'epoch': 1.64}
{'loss': 2.4413, 'grad_norm': 6.169969081878662, 'learning_rate': 9.102469135802469e-05, 'epoch': 1.64}
{'loss': 3.2527, 'grad_norm': 6.632936000823975, 'learning_rate': 9.096296296296298e-05, 'epoch': 1.64}
{'loss': 2.7189, 'grad_norm': 7.0682878494262695, 'learning_rate': 9.090123456790124e-05, 'epoch': 1.64}
{'loss': 2.824, 'grad_norm': 4.396236896514893, 'learning_rate': 9.083950617283951e-05, 'epoch': 1.64}
{'loss': 2.6759, 'grad_norm': 5.213594913482666, 'learning_rate': 9.077777777777779e-05, 'epoch': 1.64}
{'loss': 2.4305, 'grad_norm': 7.373874187469482, 'learning_rate': 9.071604938271605e-05, 'epoch': 1.64}
{'loss': 2.3225, 'grad_norm': 4.950918197631836, 'learning_rate': 9.065432098765433e-05, 'epoch': 1.64}
{'loss': 3.1587, 'grad_norm': 4.767470359802246, 'learning_rate': 9.05925925925926e-05, 'epoch': 1.64}
{'loss': 3.7788, 'grad_norm': 10.5382719039917, 'learning_rate': 9.053086419753087e-05, 'epoch': 1.64}
{'loss': 3.3818, 'grad_norm': 5.161168098449707, 'learning_rate': 9.046913580246914e-05, 'epoch': 1.64}
{'loss': 2.9432, 'grad_norm': 4.257346153259277, 'learning_rate': 9.04074074074074e-05, 'epoch': 1.65}
{'loss': 3.073, 'grad_norm': 6.355772495269775, 'learning_rate': 9.034567901234569e-05, 'epoch': 1.65}
{'loss': 2.6502, 'grad_norm': 5.635276794433594, 'learning_rate': 9.028395061728395e-05, 'epoch': 1.65}
{'loss': 2.7476, 'grad_norm': 4.293457984924316, 'learning_rate': 9.022222222222224e-05, 'epoch': 1.65}
{'loss': 3.0829, 'grad_norm': 5.08678674697876, 'learning_rate': 9.01604938271605e-05, 'epoch': 1.65}
{'loss': 2.8048, 'grad_norm': 6.449868679046631, 'learning_rate': 9.009876543209877e-05, 'epoch': 1.65}
{'loss': 3.2261, 'grad_norm': 8.194217681884766, 'learning_rate': 9.003703703703704e-05, 'epoch': 1.65}
{'loss': 2.3642, 'grad_norm': 3.6655795574188232, 'learning_rate': 8.99753086419753e-05, 'epoch': 1.65}
{'loss': 3.2975, 'grad_norm': 4.515170574188232, 'learning_rate': 8.991358024691359e-05, 'epoch': 1.65}
{'loss': 3.3272, 'grad_norm': 8.188777923583984, 'learning_rate': 8.985185185185185e-05, 'epoch': 1.65}
{'loss': 2.7615, 'grad_norm': 5.5625104904174805, 'learning_rate': 8.979012345679013e-05, 'epoch': 1.65}
{'loss': 2.9828, 'grad_norm': 5.724912166595459, 'learning_rate': 8.97283950617284e-05, 'epoch': 1.66}
{'loss': 2.7814, 'grad_norm': 3.657858371734619, 'learning_rate': 8.966666666666666e-05, 'epoch': 1.66}
{'loss': 2.3039, 'grad_norm': 5.882882118225098, 'learning_rate': 8.960493827160495e-05, 'epoch': 1.66}
{'loss': 3.0268, 'grad_norm': 5.180421352386475, 'learning_rate': 8.954320987654321e-05, 'epoch': 1.66}
{'loss': 2.9543, 'grad_norm': 6.190537929534912, 'learning_rate': 8.94814814814815e-05, 'epoch': 1.66}
{'loss': 3.0431, 'grad_norm': 4.326825141906738, 'learning_rate': 8.941975308641975e-05, 'epoch': 1.66}
{'loss': 2.7306, 'grad_norm': 5.474342346191406, 'learning_rate': 8.935802469135803e-05, 'epoch': 1.66}
{'loss': 2.6372, 'grad_norm': 3.7912588119506836, 'learning_rate': 8.92962962962963e-05, 'epoch': 1.66}
{'loss': 2.8797, 'grad_norm': 5.447854518890381, 'learning_rate': 8.923456790123456e-05, 'epoch': 1.66}
{'loss': 2.9538, 'grad_norm': 10.658883094787598, 'learning_rate': 8.917283950617285e-05, 'epoch': 1.66}
{'loss': 2.6526, 'grad_norm': 5.099297523498535, 'learning_rate': 8.911111111111111e-05, 'epoch': 1.66}
{'loss': 2.6338, 'grad_norm': 7.108786582946777, 'learning_rate': 8.904938271604938e-05, 'epoch': 1.67}
{'loss': 2.8077, 'grad_norm': 4.27125883102417, 'learning_rate': 8.898765432098766e-05, 'epoch': 1.67}
  warnings.warn(                                  
{'eval_loss': 2.7207019329071045, 'eval_runtime': 413.9856, 'eval_samples_per_second': 1.449, 'eval_steps_per_second': 1.449, 'epoch': 1.67}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 3.2319, 'grad_norm': 4.537532329559326, 'learning_rate': 8.892592592592593e-05, 'epoch': 1.67}
{'loss': 2.9967, 'grad_norm': 4.892092227935791, 'learning_rate': 8.88641975308642e-05, 'epoch': 1.67}
{'loss': 2.5015, 'grad_norm': 4.947994232177734, 'learning_rate': 8.880246913580247e-05, 'epoch': 1.67}
{'loss': 3.0393, 'grad_norm': 5.599898338317871, 'learning_rate': 8.874074074074075e-05, 'epoch': 1.67}
{'loss': 3.0211, 'grad_norm': 5.985084533691406, 'learning_rate': 8.867901234567901e-05, 'epoch': 1.67}
{'loss': 2.4987, 'grad_norm': 5.837924003601074, 'learning_rate': 8.861728395061729e-05, 'epoch': 1.67}
{'loss': 3.1225, 'grad_norm': 6.664599418640137, 'learning_rate': 8.855555555555556e-05, 'epoch': 1.67}
{'loss': 2.648, 'grad_norm': 4.297895908355713, 'learning_rate': 8.849382716049383e-05, 'epoch': 1.67}
{'loss': 2.8459, 'grad_norm': 9.037592887878418, 'learning_rate': 8.843209876543211e-05, 'epoch': 1.68}
{'loss': 2.4244, 'grad_norm': 8.489897727966309, 'learning_rate': 8.837037037037037e-05, 'epoch': 1.68}
{'loss': 2.9243, 'grad_norm': 5.612354755401611, 'learning_rate': 8.830864197530864e-05, 'epoch': 1.68}
{'loss': 2.8357, 'grad_norm': 4.104187965393066, 'learning_rate': 8.824691358024692e-05, 'epoch': 1.68}
{'loss': 2.7709, 'grad_norm': 5.285262584686279, 'learning_rate': 8.818518518518519e-05, 'epoch': 1.68}
{'loss': 2.9872, 'grad_norm': 3.903337001800537, 'learning_rate': 8.812345679012346e-05, 'epoch': 1.68}
{'loss': 2.7752, 'grad_norm': 4.184730052947998, 'learning_rate': 8.806172839506174e-05, 'epoch': 1.68}
{'loss': 3.1589, 'grad_norm': 6.955784797668457, 'learning_rate': 8.800000000000001e-05, 'epoch': 1.68}
{'loss': 2.7756, 'grad_norm': 3.485327959060669, 'learning_rate': 8.793827160493827e-05, 'epoch': 1.68}
{'loss': 3.3064, 'grad_norm': 12.41352367401123, 'learning_rate': 8.787654320987655e-05, 'epoch': 1.68}
{'loss': 2.4818, 'grad_norm': 5.271559715270996, 'learning_rate': 8.781481481481482e-05, 'epoch': 1.68}
{'loss': 3.3362, 'grad_norm': 4.90527868270874, 'learning_rate': 8.775308641975309e-05, 'epoch': 1.69}
{'loss': 2.9106, 'grad_norm': 4.584277153015137, 'learning_rate': 8.769135802469137e-05, 'epoch': 1.69}
{'loss': 3.1053, 'grad_norm': 5.891316890716553, 'learning_rate': 8.762962962962964e-05, 'epoch': 1.69}
{'loss': 2.5834, 'grad_norm': 5.47948694229126, 'learning_rate': 8.75679012345679e-05, 'epoch': 1.69}
{'loss': 3.5255, 'grad_norm': 6.376174449920654, 'learning_rate': 8.750617283950617e-05, 'epoch': 1.69}
{'loss': 2.3625, 'grad_norm': 3.6583480834960938, 'learning_rate': 8.744444444444445e-05, 'epoch': 1.69}
{'loss': 2.2197, 'grad_norm': 3.9162867069244385, 'learning_rate': 8.738271604938272e-05, 'epoch': 1.69}
{'loss': 3.119, 'grad_norm': 6.4276533126831055, 'learning_rate': 8.7320987654321e-05, 'epoch': 1.69}
{'loss': 3.4554, 'grad_norm': 6.32008695602417, 'learning_rate': 8.725925925925927e-05, 'epoch': 1.69}
{'loss': 2.8398, 'grad_norm': 5.576916694641113, 'learning_rate': 8.719753086419753e-05, 'epoch': 1.69}
{'loss': 2.9477, 'grad_norm': 3.656158924102783, 'learning_rate': 8.71358024691358e-05, 'epoch': 1.69}
{'loss': 2.8467, 'grad_norm': 3.99554705619812, 'learning_rate': 8.707407407407408e-05, 'epoch': 1.7}
{'loss': 2.8148, 'grad_norm': 7.956282615661621, 'learning_rate': 8.701234567901235e-05, 'epoch': 1.7}
{'loss': 2.4029, 'grad_norm': 4.477110862731934, 'learning_rate': 8.695061728395062e-05, 'epoch': 1.7}
{'loss': 3.248, 'grad_norm': 8.54481315612793, 'learning_rate': 8.68888888888889e-05, 'epoch': 1.7}
{'loss': 2.4573, 'grad_norm': 4.65305233001709, 'learning_rate': 8.682716049382716e-05, 'epoch': 1.7}
{'loss': 2.6253, 'grad_norm': 5.528728485107422, 'learning_rate': 8.676543209876543e-05, 'epoch': 1.7}
{'loss': 2.557, 'grad_norm': 4.736031532287598, 'learning_rate': 8.67037037037037e-05, 'epoch': 1.7}
{'loss': 2.8995, 'grad_norm': 7.303680896759033, 'learning_rate': 8.664197530864198e-05, 'epoch': 1.7}
{'loss': 2.5815, 'grad_norm': 4.083306312561035, 'learning_rate': 8.658024691358025e-05, 'epoch': 1.7}
{'loss': 2.784, 'grad_norm': 4.841534614562988, 'learning_rate': 8.651851851851851e-05, 'epoch': 1.7}
{'loss': 3.4593, 'grad_norm': 6.679257392883301, 'learning_rate': 8.64567901234568e-05, 'epoch': 1.7}
{'loss': 2.8193, 'grad_norm': 8.052464485168457, 'learning_rate': 8.639506172839506e-05, 'epoch': 1.71}
{'loss': 2.8745, 'grad_norm': 5.651405334472656, 'learning_rate': 8.633333333333334e-05, 'epoch': 1.71}
{'loss': 2.9992, 'grad_norm': 6.59385347366333, 'learning_rate': 8.627160493827161e-05, 'epoch': 1.71}
{'loss': 2.6161, 'grad_norm': 5.756149768829346, 'learning_rate': 8.620987654320988e-05, 'epoch': 1.71}
{'loss': 2.833, 'grad_norm': 4.460391521453857, 'learning_rate': 8.614814814814816e-05, 'epoch': 1.71}
{'loss': 3.5302, 'grad_norm': 9.318596839904785, 'learning_rate': 8.608641975308642e-05, 'epoch': 1.71}
{'loss': 2.623, 'grad_norm': 4.25971794128418, 'learning_rate': 8.60246913580247e-05, 'epoch': 1.71}
{'loss': 2.8709, 'grad_norm': 5.116976737976074, 'learning_rate': 8.596296296296296e-05, 'epoch': 1.71}
{'loss': 3.0763, 'grad_norm': 17.862850189208984, 'learning_rate': 8.590123456790124e-05, 'epoch': 1.71}
{'loss': 2.8847, 'grad_norm': 5.7343950271606445, 'learning_rate': 8.583950617283951e-05, 'epoch': 1.71}
{'loss': 2.8323, 'grad_norm': 4.365030765533447, 'learning_rate': 8.577777777777777e-05, 'epoch': 1.71}
{'loss': 3.2247, 'grad_norm': 7.949731349945068, 'learning_rate': 8.571604938271606e-05, 'epoch': 1.72}
{'loss': 2.3528, 'grad_norm': 8.322185516357422, 'learning_rate': 8.565432098765432e-05, 'epoch': 1.72}
{'loss': 3.1341, 'grad_norm': 5.117659091949463, 'learning_rate': 8.559259259259261e-05, 'epoch': 1.72}
{'loss': 3.1909, 'grad_norm': 5.323453903198242, 'learning_rate': 8.553086419753087e-05, 'epoch': 1.72}
{'loss': 3.0191, 'grad_norm': 6.051165580749512, 'learning_rate': 8.546913580246914e-05, 'epoch': 1.72}
{'loss': 2.9925, 'grad_norm': 4.693211555480957, 'learning_rate': 8.540740740740742e-05, 'epoch': 1.72}
{'loss': 2.905, 'grad_norm': 5.240839004516602, 'learning_rate': 8.534567901234568e-05, 'epoch': 1.72}
{'loss': 2.0046, 'grad_norm': 4.479364395141602, 'learning_rate': 8.528395061728396e-05, 'epoch': 1.72}
{'loss': 2.3919, 'grad_norm': 5.541345596313477, 'learning_rate': 8.522222222222222e-05, 'epoch': 1.72}
{'loss': 2.4778, 'grad_norm': 4.4627156257629395, 'learning_rate': 8.51604938271605e-05, 'epoch': 1.72}
{'loss': 2.6273, 'grad_norm': 34.10129165649414, 'learning_rate': 8.509876543209877e-05, 'epoch': 1.73}
{'loss': 2.2963, 'grad_norm': 5.460414409637451, 'learning_rate': 8.503703703703703e-05, 'epoch': 1.73}
{'loss': 2.8703, 'grad_norm': 5.02569055557251, 'learning_rate': 8.497530864197532e-05, 'epoch': 1.73}
{'loss': 2.5711, 'grad_norm': 7.645537376403809, 'learning_rate': 8.491358024691358e-05, 'epoch': 1.73}
{'loss': 2.2781, 'grad_norm': 3.994044303894043, 'learning_rate': 8.485185185185187e-05, 'epoch': 1.73}
{'loss': 2.6635, 'grad_norm': 4.144870758056641, 'learning_rate': 8.479012345679013e-05, 'epoch': 1.73}
{'loss': 2.4438, 'grad_norm': 4.944822788238525, 'learning_rate': 8.47283950617284e-05, 'epoch': 1.73}
{'loss': 2.8108, 'grad_norm': 5.66574239730835, 'learning_rate': 8.466666666666667e-05, 'epoch': 1.73}
{'loss': 2.3969, 'grad_norm': 6.635031223297119, 'learning_rate': 8.460493827160493e-05, 'epoch': 1.73}
{'loss': 3.2948, 'grad_norm': 11.856736183166504, 'learning_rate': 8.454320987654322e-05, 'epoch': 1.73}
{'loss': 3.5364, 'grad_norm': 5.1684889793396, 'learning_rate': 8.448148148148148e-05, 'epoch': 1.73}
{'loss': 2.0951, 'grad_norm': 4.132408618927002, 'learning_rate': 8.441975308641976e-05, 'epoch': 1.74}
{'loss': 2.7241, 'grad_norm': 10.848344802856445, 'learning_rate': 8.435802469135803e-05, 'epoch': 1.74}
{'loss': 2.855, 'grad_norm': 5.139986515045166, 'learning_rate': 8.429629629629629e-05, 'epoch': 1.74}
{'loss': 4.1076, 'grad_norm': 5.441534042358398, 'learning_rate': 8.423456790123458e-05, 'epoch': 1.74}
{'loss': 2.6646, 'grad_norm': 5.3585944175720215, 'learning_rate': 8.417283950617284e-05, 'epoch': 1.74}
{'loss': 3.504, 'grad_norm': 8.537566184997559, 'learning_rate': 8.411111111111112e-05, 'epoch': 1.74}
{'loss': 2.1021, 'grad_norm': 8.900971412658691, 'learning_rate': 8.404938271604938e-05, 'epoch': 1.74}
{'loss': 2.9376, 'grad_norm': 3.972308874130249, 'learning_rate': 8.398765432098766e-05, 'epoch': 1.74}
{'loss': 3.1967, 'grad_norm': 9.257827758789062, 'learning_rate': 8.392592592592593e-05, 'epoch': 1.74}
{'loss': 2.8312, 'grad_norm': 6.979833602905273, 'learning_rate': 8.386419753086419e-05, 'epoch': 1.74}
{'loss': 3.094, 'grad_norm': 5.389087200164795, 'learning_rate': 8.380246913580248e-05, 'epoch': 1.74}
{'loss': 2.6342, 'grad_norm': 5.578115940093994, 'learning_rate': 8.374074074074074e-05, 'epoch': 1.75}
{'loss': 3.4352, 'grad_norm': 6.435547351837158, 'learning_rate': 8.367901234567901e-05, 'epoch': 1.75}
{'loss': 2.1697, 'grad_norm': 4.689117908477783, 'learning_rate': 8.361728395061729e-05, 'epoch': 1.75}
{'loss': 3.3827, 'grad_norm': 4.470810413360596, 'learning_rate': 8.355555555555556e-05, 'epoch': 1.75}
{'loss': 3.6652, 'grad_norm': 5.984194755554199, 'learning_rate': 8.349382716049383e-05, 'epoch': 1.75}
{'loss': 3.4583, 'grad_norm': 6.73675537109375, 'learning_rate': 8.34320987654321e-05, 'epoch': 1.75}
{'loss': 2.5218, 'grad_norm': 16.624835968017578, 'learning_rate': 8.337037037037038e-05, 'epoch': 1.75}
{'loss': 2.8566, 'grad_norm': 6.073118209838867, 'learning_rate': 8.330864197530864e-05, 'epoch': 1.75}
{'loss': 2.4227, 'grad_norm': 6.551043510437012, 'learning_rate': 8.324691358024692e-05, 'epoch': 1.75}
{'loss': 2.8067, 'grad_norm': 10.898907661437988, 'learning_rate': 8.318518518518519e-05, 'epoch': 1.75}
{'loss': 2.9153, 'grad_norm': 5.036993503570557, 'learning_rate': 8.312345679012346e-05, 'epoch': 1.75}
{'loss': 2.3334, 'grad_norm': 5.564611434936523, 'learning_rate': 8.306172839506174e-05, 'epoch': 1.76}
{'loss': 2.6526, 'grad_norm': 4.095576286315918, 'learning_rate': 8.3e-05, 'epoch': 1.76}
{'loss': 3.2558, 'grad_norm': 6.328534126281738, 'learning_rate': 8.293827160493827e-05, 'epoch': 1.76}
{'loss': 3.0178, 'grad_norm': 7.5608110427856445, 'learning_rate': 8.287654320987655e-05, 'epoch': 1.76}
{'loss': 2.5189, 'grad_norm': 8.493340492248535, 'learning_rate': 8.281481481481482e-05, 'epoch': 1.76}
  warnings.warn(                                  
{'eval_loss': 2.726954460144043, 'eval_runtime': 413.2704, 'eval_samples_per_second': 1.452, 'eval_steps_per_second': 1.452, 'epoch': 1.76}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.7485, 'grad_norm': 3.6688194274902344, 'learning_rate': 8.27530864197531e-05, 'epoch': 1.76}
{'loss': 2.8657, 'grad_norm': 5.719468116760254, 'learning_rate': 8.269135802469135e-05, 'epoch': 1.76}
{'loss': 2.6297, 'grad_norm': 6.116986274719238, 'learning_rate': 8.262962962962964e-05, 'epoch': 1.76}
{'loss': 3.0383, 'grad_norm': 4.252079486846924, 'learning_rate': 8.25679012345679e-05, 'epoch': 1.76}
{'loss': 2.255, 'grad_norm': 4.649080276489258, 'learning_rate': 8.250617283950617e-05, 'epoch': 1.76}
{'loss': 2.6906, 'grad_norm': 4.655247211456299, 'learning_rate': 8.244444444444445e-05, 'epoch': 1.76}
{'loss': 3.1387, 'grad_norm': 4.263626575469971, 'learning_rate': 8.238271604938272e-05, 'epoch': 1.77}
{'loss': 2.627, 'grad_norm': 5.871675968170166, 'learning_rate': 8.2320987654321e-05, 'epoch': 1.77}
{'loss': 3.1806, 'grad_norm': 5.396848201751709, 'learning_rate': 8.225925925925926e-05, 'epoch': 1.77}
{'loss': 2.9503, 'grad_norm': 9.408256530761719, 'learning_rate': 8.219753086419753e-05, 'epoch': 1.77}
{'loss': 3.3988, 'grad_norm': 5.511654376983643, 'learning_rate': 8.21358024691358e-05, 'epoch': 1.77}
{'loss': 3.2358, 'grad_norm': 3.3230950832366943, 'learning_rate': 8.207407407407408e-05, 'epoch': 1.77}
{'loss': 2.9226, 'grad_norm': 9.069598197937012, 'learning_rate': 8.201234567901235e-05, 'epoch': 1.77}
{'loss': 2.5785, 'grad_norm': 4.619141101837158, 'learning_rate': 8.195061728395063e-05, 'epoch': 1.77}
{'loss': 3.5335, 'grad_norm': 6.576812267303467, 'learning_rate': 8.18888888888889e-05, 'epoch': 1.77}
{'loss': 2.8105, 'grad_norm': 5.957680702209473, 'learning_rate': 8.182716049382716e-05, 'epoch': 1.77}
{'loss': 2.5852, 'grad_norm': 5.078332901000977, 'learning_rate': 8.176543209876543e-05, 'epoch': 1.77}
{'loss': 2.4279, 'grad_norm': 5.30387544631958, 'learning_rate': 8.170370370370371e-05, 'epoch': 1.78}
{'loss': 3.1039, 'grad_norm': 35.269954681396484, 'learning_rate': 8.164197530864198e-05, 'epoch': 1.78}
{'loss': 2.787, 'grad_norm': 4.330175399780273, 'learning_rate': 8.158024691358025e-05, 'epoch': 1.78}
{'loss': 2.5536, 'grad_norm': 5.034367084503174, 'learning_rate': 8.151851851851853e-05, 'epoch': 1.78}
{'loss': 2.9671, 'grad_norm': 6.904561996459961, 'learning_rate': 8.145679012345679e-05, 'epoch': 1.78}
{'loss': 2.897, 'grad_norm': 3.8329296112060547, 'learning_rate': 8.139506172839506e-05, 'epoch': 1.78}
{'loss': 3.37, 'grad_norm': 9.271833419799805, 'learning_rate': 8.133333333333334e-05, 'epoch': 1.78}
{'loss': 2.3226, 'grad_norm': 5.06947660446167, 'learning_rate': 8.127160493827161e-05, 'epoch': 1.78}
{'loss': 3.0343, 'grad_norm': 4.524598121643066, 'learning_rate': 8.120987654320988e-05, 'epoch': 1.78}
{'loss': 2.4684, 'grad_norm': 5.943017959594727, 'learning_rate': 8.114814814814816e-05, 'epoch': 1.78}
{'loss': 2.7704, 'grad_norm': 6.194456577301025, 'learning_rate': 8.108641975308643e-05, 'epoch': 1.79}
{'loss': 2.3814, 'grad_norm': 6.4973368644714355, 'learning_rate': 8.102469135802469e-05, 'epoch': 1.79}
{'loss': 2.9042, 'grad_norm': 8.244938850402832, 'learning_rate': 8.096296296296297e-05, 'epoch': 1.79}
{'loss': 2.9085, 'grad_norm': 5.148009777069092, 'learning_rate': 8.090123456790124e-05, 'epoch': 1.79}
{'loss': 2.5579, 'grad_norm': 5.945466041564941, 'learning_rate': 8.083950617283951e-05, 'epoch': 1.79}
{'loss': 2.7198, 'grad_norm': 4.119710922241211, 'learning_rate': 8.077777777777779e-05, 'epoch': 1.79}
{'loss': 3.6911, 'grad_norm': 9.31970500946045, 'learning_rate': 8.071604938271605e-05, 'epoch': 1.79}
{'loss': 2.6792, 'grad_norm': 4.324676513671875, 'learning_rate': 8.065432098765432e-05, 'epoch': 1.79}
{'loss': 2.495, 'grad_norm': 6.407337188720703, 'learning_rate': 8.05925925925926e-05, 'epoch': 1.79}
{'loss': 3.5449, 'grad_norm': 4.2195844650268555, 'learning_rate': 8.053086419753087e-05, 'epoch': 1.79}
{'loss': 2.6784, 'grad_norm': 4.742652893066406, 'learning_rate': 8.046913580246914e-05, 'epoch': 1.79}
{'loss': 3.0616, 'grad_norm': 6.916474342346191, 'learning_rate': 8.040740740740742e-05, 'epoch': 1.8}
{'loss': 2.8864, 'grad_norm': 6.151372909545898, 'learning_rate': 8.034567901234569e-05, 'epoch': 1.8}
{'loss': 2.4147, 'grad_norm': 4.029847145080566, 'learning_rate': 8.028395061728395e-05, 'epoch': 1.8}
{'loss': 2.498, 'grad_norm': 5.234224796295166, 'learning_rate': 8.022222222222222e-05, 'epoch': 1.8}
{'loss': 3.564, 'grad_norm': 6.416835784912109, 'learning_rate': 8.01604938271605e-05, 'epoch': 1.8}
{'loss': 3.185, 'grad_norm': 7.097557544708252, 'learning_rate': 8.009876543209877e-05, 'epoch': 1.8}
{'loss': 3.0798, 'grad_norm': 5.363969326019287, 'learning_rate': 8.003703703703704e-05, 'epoch': 1.8}
{'loss': 2.5764, 'grad_norm': 4.848182201385498, 'learning_rate': 7.99753086419753e-05, 'epoch': 1.8}
{'loss': 2.4394, 'grad_norm': 4.453774929046631, 'learning_rate': 7.991358024691359e-05, 'epoch': 1.8}
{'loss': 2.6904, 'grad_norm': 8.503569602966309, 'learning_rate': 7.985185185185185e-05, 'epoch': 1.8}
{'loss': 2.9851, 'grad_norm': 6.042108058929443, 'learning_rate': 7.979012345679013e-05, 'epoch': 1.8}
{'loss': 3.5229, 'grad_norm': 5.085117340087891, 'learning_rate': 7.97283950617284e-05, 'epoch': 1.81}
{'loss': 2.9601, 'grad_norm': 2.9257187843322754, 'learning_rate': 7.966666666666666e-05, 'epoch': 1.81}
{'loss': 2.5116, 'grad_norm': 4.297189235687256, 'learning_rate': 7.960493827160495e-05, 'epoch': 1.81}
{'loss': 2.6498, 'grad_norm': 5.528359413146973, 'learning_rate': 7.954320987654321e-05, 'epoch': 1.81}
{'loss': 2.4464, 'grad_norm': 3.8581221103668213, 'learning_rate': 7.94814814814815e-05, 'epoch': 1.81}
{'loss': 3.1087, 'grad_norm': 9.512357711791992, 'learning_rate': 7.941975308641976e-05, 'epoch': 1.81}
{'loss': 3.1653, 'grad_norm': 4.8184332847595215, 'learning_rate': 7.935802469135803e-05, 'epoch': 1.81}
{'loss': 3.4992, 'grad_norm': 4.700610160827637, 'learning_rate': 7.92962962962963e-05, 'epoch': 1.81}
{'loss': 3.1668, 'grad_norm': 9.020992279052734, 'learning_rate': 7.923456790123456e-05, 'epoch': 1.81}
{'loss': 3.1612, 'grad_norm': 6.396001815795898, 'learning_rate': 7.917283950617285e-05, 'epoch': 1.81}
{'loss': 2.6883, 'grad_norm': 5.216619968414307, 'learning_rate': 7.911111111111111e-05, 'epoch': 1.81}
{'loss': 2.1618, 'grad_norm': 4.561461925506592, 'learning_rate': 7.904938271604938e-05, 'epoch': 1.82}
{'loss': 2.8775, 'grad_norm': 4.608831882476807, 'learning_rate': 7.898765432098766e-05, 'epoch': 1.82}
{'loss': 2.6259, 'grad_norm': 3.8687431812286377, 'learning_rate': 7.892592592592592e-05, 'epoch': 1.82}
{'loss': 2.8572, 'grad_norm': 7.580416679382324, 'learning_rate': 7.88641975308642e-05, 'epoch': 1.82}
{'loss': 3.0079, 'grad_norm': 4.991982936859131, 'learning_rate': 7.880246913580247e-05, 'epoch': 1.82}
{'loss': 3.0891, 'grad_norm': 4.626596450805664, 'learning_rate': 7.874074074074075e-05, 'epoch': 1.82}
{'loss': 2.9429, 'grad_norm': 6.950397968292236, 'learning_rate': 7.867901234567901e-05, 'epoch': 1.82}
{'loss': 2.9561, 'grad_norm': 6.332466125488281, 'learning_rate': 7.861728395061729e-05, 'epoch': 1.82}
{'loss': 2.918, 'grad_norm': 3.947925567626953, 'learning_rate': 7.855555555555556e-05, 'epoch': 1.82}
{'loss': 2.9037, 'grad_norm': 5.218379020690918, 'learning_rate': 7.849382716049382e-05, 'epoch': 1.82}
{'loss': 3.2238, 'grad_norm': 6.8765153884887695, 'learning_rate': 7.843209876543211e-05, 'epoch': 1.82}
{'loss': 3.0395, 'grad_norm': 4.449704647064209, 'learning_rate': 7.837037037037037e-05, 'epoch': 1.83}
{'loss': 2.5022, 'grad_norm': 5.117282390594482, 'learning_rate': 7.830864197530864e-05, 'epoch': 1.83}
{'loss': 2.8091, 'grad_norm': 11.922511100769043, 'learning_rate': 7.824691358024692e-05, 'epoch': 1.83}
{'loss': 3.0066, 'grad_norm': 4.2935004234313965, 'learning_rate': 7.818518518518518e-05, 'epoch': 1.83}
{'loss': 3.7771, 'grad_norm': 6.202637195587158, 'learning_rate': 7.812345679012346e-05, 'epoch': 1.83}
{'loss': 3.1995, 'grad_norm': 5.42442512512207, 'learning_rate': 7.806172839506172e-05, 'epoch': 1.83}
{'loss': 2.4361, 'grad_norm': 6.639120578765869, 'learning_rate': 7.800000000000001e-05, 'epoch': 1.83}
{'loss': 2.4486, 'grad_norm': 5.777218818664551, 'learning_rate': 7.793827160493827e-05, 'epoch': 1.83}
{'loss': 2.6795, 'grad_norm': 8.4716157913208, 'learning_rate': 7.787654320987655e-05, 'epoch': 1.83}
{'loss': 2.8704, 'grad_norm': 3.9997568130493164, 'learning_rate': 7.781481481481482e-05, 'epoch': 1.83}
{'loss': 2.7192, 'grad_norm': 5.549373626708984, 'learning_rate': 7.775308641975308e-05, 'epoch': 1.84}
{'loss': 2.9515, 'grad_norm': 4.616735935211182, 'learning_rate': 7.769135802469137e-05, 'epoch': 1.84}
{'loss': 2.9944, 'grad_norm': 9.515761375427246, 'learning_rate': 7.762962962962963e-05, 'epoch': 1.84}
{'loss': 3.5049, 'grad_norm': 6.230173110961914, 'learning_rate': 7.75679012345679e-05, 'epoch': 1.84}
{'loss': 3.0178, 'grad_norm': 6.798357009887695, 'learning_rate': 7.750617283950618e-05, 'epoch': 1.84}
{'loss': 2.4324, 'grad_norm': 3.493617057800293, 'learning_rate': 7.744444444444445e-05, 'epoch': 1.84}
{'loss': 2.9823, 'grad_norm': 4.9736151695251465, 'learning_rate': 7.738271604938272e-05, 'epoch': 1.84}
{'loss': 2.9658, 'grad_norm': 5.723906517028809, 'learning_rate': 7.732098765432098e-05, 'epoch': 1.84}
{'loss': 3.1153, 'grad_norm': 6.6835246086120605, 'learning_rate': 7.725925925925927e-05, 'epoch': 1.84}
{'loss': 3.3546, 'grad_norm': 5.278111934661865, 'learning_rate': 7.719753086419753e-05, 'epoch': 1.84}
{'loss': 2.6209, 'grad_norm': 4.8310160636901855, 'learning_rate': 7.71358024691358e-05, 'epoch': 1.84}
{'loss': 2.424, 'grad_norm': 4.032835960388184, 'learning_rate': 7.707407407407408e-05, 'epoch': 1.85}
{'loss': 3.0026, 'grad_norm': 8.077707290649414, 'learning_rate': 7.701234567901235e-05, 'epoch': 1.85}
{'loss': 2.9321, 'grad_norm': 4.604111194610596, 'learning_rate': 7.695061728395063e-05, 'epoch': 1.85}
{'loss': 3.014, 'grad_norm': 5.431849956512451, 'learning_rate': 7.688888888888889e-05, 'epoch': 1.85}
{'loss': 2.8638, 'grad_norm': 10.715290069580078, 'learning_rate': 7.682716049382716e-05, 'epoch': 1.85}
{'loss': 2.9556, 'grad_norm': 3.934943914413452, 'learning_rate': 7.676543209876543e-05, 'epoch': 1.85}
{'loss': 2.2556, 'grad_norm': 5.368222236633301, 'learning_rate': 7.670370370370371e-05, 'epoch': 1.85}
{'loss': 2.2794, 'grad_norm': 5.57512903213501, 'learning_rate': 7.664197530864198e-05, 'epoch': 1.85}
  warnings.warn(                                  
{'eval_loss': 2.7231509685516357, 'eval_runtime': 412.8328, 'eval_samples_per_second': 1.453, 'eval_steps_per_second': 1.453, 'epoch': 1.85}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                  
{'loss': 2.6715, 'grad_norm': 4.702826023101807, 'learning_rate': 7.658024691358025e-05, 'epoch': 1.85}
{'loss': 2.9299, 'grad_norm': 4.246367454528809, 'learning_rate': 7.651851851851853e-05, 'epoch': 1.85}
{'loss': 2.8827, 'grad_norm': 5.831399917602539, 'learning_rate': 7.645679012345679e-05, 'epoch': 1.85}
{'loss': 3.087, 'grad_norm': 5.991057395935059, 'learning_rate': 7.639506172839506e-05, 'epoch': 1.86}
{'loss': 2.7564, 'grad_norm': 5.378758907318115, 'learning_rate': 7.633333333333334e-05, 'epoch': 1.86}
{'loss': 2.8211, 'grad_norm': 4.668968677520752, 'learning_rate': 7.627160493827161e-05, 'epoch': 1.86}
{'loss': 3.0025, 'grad_norm': 7.827273845672607, 'learning_rate': 7.620987654320988e-05, 'epoch': 1.86}
{'loss': 2.6771, 'grad_norm': 8.235081672668457, 'learning_rate': 7.614814814814816e-05, 'epoch': 1.86}
{'loss': 2.9066, 'grad_norm': 4.162021160125732, 'learning_rate': 7.608641975308642e-05, 'epoch': 1.86}
{'loss': 2.3919, 'grad_norm': 4.805264472961426, 'learning_rate': 7.602469135802469e-05, 'epoch': 1.86}
{'loss': 2.2505, 'grad_norm': 4.172347545623779, 'learning_rate': 7.596296296296297e-05, 'epoch': 1.86}
{'loss': 3.3032, 'grad_norm': 6.388679504394531, 'learning_rate': 7.590123456790124e-05, 'epoch': 1.86}
{'loss': 3.3663, 'grad_norm': 5.5201416015625, 'learning_rate': 7.583950617283951e-05, 'epoch': 1.86}
{'loss': 3.0354, 'grad_norm': 7.259823799133301, 'learning_rate': 7.577777777777779e-05, 'epoch': 1.86}
{'loss': 3.1725, 'grad_norm': 6.004700660705566, 'learning_rate': 7.571604938271605e-05, 'epoch': 1.87}
{'loss': 2.9511, 'grad_norm': 7.882262229919434, 'learning_rate': 7.565432098765432e-05, 'epoch': 1.87}
{'loss': 3.032, 'grad_norm': 4.408294200897217, 'learning_rate': 7.55925925925926e-05, 'epoch': 1.87}
{'loss': 3.0087, 'grad_norm': 5.683597087860107, 'learning_rate': 7.553086419753087e-05, 'epoch': 1.87}
{'loss': 3.0864, 'grad_norm': 6.604797840118408, 'learning_rate': 7.546913580246914e-05, 'epoch': 1.87}
{'loss': 2.2261, 'grad_norm': 5.031521797180176, 'learning_rate': 7.540740740740742e-05, 'epoch': 1.87}
{'loss': 2.9449, 'grad_norm': 3.5307717323303223, 'learning_rate': 7.534567901234568e-05, 'epoch': 1.87}
{'loss': 3.2041, 'grad_norm': 5.8295793533325195, 'learning_rate': 7.528395061728395e-05, 'epoch': 1.87}
{'loss': 2.8858, 'grad_norm': 6.045376777648926, 'learning_rate': 7.522222222222222e-05, 'epoch': 1.87}
{'loss': 2.2666, 'grad_norm': 4.24018669128418, 'learning_rate': 7.51604938271605e-05, 'epoch': 1.87}
{'loss': 2.5733, 'grad_norm': 5.542993545532227, 'learning_rate': 7.509876543209877e-05, 'epoch': 1.88}
{'loss': 2.6101, 'grad_norm': 5.610224723815918, 'learning_rate': 7.503703703703705e-05, 'epoch': 1.88}
{'loss': 2.801, 'grad_norm': 5.146008491516113, 'learning_rate': 7.497530864197532e-05, 'epoch': 1.88}
{'loss': 2.8074, 'grad_norm': 4.129288196563721, 'learning_rate': 7.491358024691358e-05, 'epoch': 1.88}
{'loss': 3.0313, 'grad_norm': 5.557767868041992, 'learning_rate': 7.485185185185185e-05, 'epoch': 1.88}
{'loss': 2.8122, 'grad_norm': 3.739452838897705, 'learning_rate': 7.479012345679013e-05, 'epoch': 1.88}
{'loss': 3.1561, 'grad_norm': 5.666379928588867, 'learning_rate': 7.47283950617284e-05, 'epoch': 1.88}
{'loss': 3.0129, 'grad_norm': 4.040482521057129, 'learning_rate': 7.466666666666667e-05, 'epoch': 1.88}
{'loss': 2.9103, 'grad_norm': 6.767704486846924, 'learning_rate': 7.460493827160493e-05, 'epoch': 1.88}
{'loss': 2.2608, 'grad_norm': 4.586772918701172, 'learning_rate': 7.454320987654322e-05, 'epoch': 1.88}
{'loss': 2.483, 'grad_norm': 5.292047500610352, 'learning_rate': 7.448148148148148e-05, 'epoch': 1.88}
{'loss': 2.5132, 'grad_norm': 4.1939544677734375, 'learning_rate': 7.441975308641976e-05, 'epoch': 1.89}
{'loss': 3.0027, 'grad_norm': 5.921123027801514, 'learning_rate': 7.435802469135803e-05, 'epoch': 1.89}
{'loss': 3.3294, 'grad_norm': 10.324808120727539, 'learning_rate': 7.42962962962963e-05, 'epoch': 1.89}
{'loss': 2.2245, 'grad_norm': 6.365973472595215, 'learning_rate': 7.423456790123458e-05, 'epoch': 1.89}
{'loss': 3.1758, 'grad_norm': 4.2490434646606445, 'learning_rate': 7.417283950617284e-05, 'epoch': 1.89}
{'loss': 2.4485, 'grad_norm': 5.156520843505859, 'learning_rate': 7.411111111111113e-05, 'epoch': 1.89}
{'loss': 3.4291, 'grad_norm': 11.150484085083008, 'learning_rate': 7.404938271604939e-05, 'epoch': 1.89}
{'loss': 2.9518, 'grad_norm': 5.743117332458496, 'learning_rate': 7.398765432098766e-05, 'epoch': 1.89}
{'loss': 3.0084, 'grad_norm': 5.73130464553833, 'learning_rate': 7.392592592592593e-05, 'epoch': 1.89}
{'loss': 3.2427, 'grad_norm': 3.237365245819092, 'learning_rate': 7.386419753086419e-05, 'epoch': 1.89}
{'loss': 2.3476, 'grad_norm': 4.814239501953125, 'learning_rate': 7.380246913580248e-05, 'epoch': 1.89}
{'loss': 2.0557, 'grad_norm': 4.250428199768066, 'learning_rate': 7.374074074074074e-05, 'epoch': 1.9}
{'loss': 2.6132, 'grad_norm': 6.426995754241943, 'learning_rate': 7.367901234567901e-05, 'epoch': 1.9}
{'loss': 3.2806, 'grad_norm': 6.401905059814453, 'learning_rate': 7.361728395061729e-05, 'epoch': 1.9}
{'loss': 2.4654, 'grad_norm': 6.97351598739624, 'learning_rate': 7.355555555555556e-05, 'epoch': 1.9}
{'loss': 2.6016, 'grad_norm': 5.766634464263916, 'learning_rate': 7.349382716049384e-05, 'epoch': 1.9}
{'loss': 3.0026, 'grad_norm': 6.220612049102783, 'learning_rate': 7.34320987654321e-05, 'epoch': 1.9}
{'loss': 2.7136, 'grad_norm': 7.271107196807861, 'learning_rate': 7.337037037037038e-05, 'epoch': 1.9}
{'loss': 3.1521, 'grad_norm': 6.8282294273376465, 'learning_rate': 7.330864197530864e-05, 'epoch': 1.9}
{'loss': 2.8255, 'grad_norm': 5.4068732261657715, 'learning_rate': 7.324691358024692e-05, 'epoch': 1.9}
{'loss': 2.7573, 'grad_norm': 4.394793510437012, 'learning_rate': 7.318518518518519e-05, 'epoch': 1.9}
{'loss': 3.6854, 'grad_norm': 8.164036750793457, 'learning_rate': 7.312345679012345e-05, 'epoch': 1.9}
{'loss': 2.0425, 'grad_norm': 4.257905960083008, 'learning_rate': 7.306172839506174e-05, 'epoch': 1.91}
{'loss': 2.5339, 'grad_norm': 4.010073661804199, 'learning_rate': 7.3e-05, 'epoch': 1.91}
{'loss': 2.7412, 'grad_norm': 4.443714141845703, 'learning_rate': 7.293827160493829e-05, 'epoch': 1.91}
{'loss': 2.2696, 'grad_norm': 7.550203323364258, 'learning_rate': 7.287654320987655e-05, 'epoch': 1.91}
{'loss': 2.6735, 'grad_norm': 3.501314163208008, 'learning_rate': 7.281481481481481e-05, 'epoch': 1.91}
{'loss': 2.8946, 'grad_norm': 4.19167947769165, 'learning_rate': 7.27530864197531e-05, 'epoch': 1.91}
{'loss': 3.5046, 'grad_norm': 12.447519302368164, 'learning_rate': 7.269135802469135e-05, 'epoch': 1.91}
{'loss': 2.8801, 'grad_norm': 8.160737037658691, 'learning_rate': 7.262962962962964e-05, 'epoch': 1.91}
{'loss': 2.8435, 'grad_norm': 5.687143325805664, 'learning_rate': 7.25679012345679e-05, 'epoch': 1.91}
{'loss': 3.2233, 'grad_norm': 5.400318622589111, 'learning_rate': 7.250617283950618e-05, 'epoch': 1.91}
{'loss': 2.9811, 'grad_norm': 6.656890869140625, 'learning_rate': 7.244444444444445e-05, 'epoch': 1.91}
{'loss': 3.1987, 'grad_norm': 6.7267937660217285, 'learning_rate': 7.238271604938271e-05, 'epoch': 1.92}
{'loss': 2.7998, 'grad_norm': 4.061373233795166, 'learning_rate': 7.2320987654321e-05, 'epoch': 1.92}
{'loss': 3.4265, 'grad_norm': 9.273030281066895, 'learning_rate': 7.225925925925926e-05, 'epoch': 1.92}
{'loss': 3.469, 'grad_norm': 9.032089233398438, 'learning_rate': 7.219753086419753e-05, 'epoch': 1.92}
{'loss': 3.1011, 'grad_norm': 5.7018513679504395, 'learning_rate': 7.21358024691358e-05, 'epoch': 1.92}
{'loss': 2.964, 'grad_norm': 6.390251159667969, 'learning_rate': 7.207407407407408e-05, 'epoch': 1.92}
{'loss': 2.7946, 'grad_norm': 7.4089250564575195, 'learning_rate': 7.201234567901235e-05, 'epoch': 1.92}
{'loss': 2.7375, 'grad_norm': 5.22885274887085, 'learning_rate': 7.195061728395061e-05, 'epoch': 1.92}
{'loss': 2.8765, 'grad_norm': 4.975030899047852, 'learning_rate': 7.18888888888889e-05, 'epoch': 1.92}
{'loss': 2.9594, 'grad_norm': 4.418105125427246, 'learning_rate': 7.182716049382716e-05, 'epoch': 1.92}
{'loss': 2.6321, 'grad_norm': 5.187465667724609, 'learning_rate': 7.176543209876543e-05, 'epoch': 1.93}
{'loss': 3.072, 'grad_norm': 6.346240043640137, 'learning_rate': 7.170370370370371e-05, 'epoch': 1.93}
{'loss': 3.0695, 'grad_norm': 4.253634452819824, 'learning_rate': 7.164197530864198e-05, 'epoch': 1.93}
{'loss': 2.7617, 'grad_norm': 5.541133403778076, 'learning_rate': 7.158024691358026e-05, 'epoch': 1.93}
{'loss': 2.776, 'grad_norm': 5.334901809692383, 'learning_rate': 7.151851851851852e-05, 'epoch': 1.93}
{'loss': 2.6276, 'grad_norm': 5.965966701507568, 'learning_rate': 7.145679012345679e-05, 'epoch': 1.93}
{'loss': 3.0335, 'grad_norm': 5.590890407562256, 'learning_rate': 7.139506172839506e-05, 'epoch': 1.93}
{'loss': 3.0819, 'grad_norm': 5.09395694732666, 'learning_rate': 7.133333333333334e-05, 'epoch': 1.93}
{'loss': 2.5813, 'grad_norm': 5.198731899261475, 'learning_rate': 7.127160493827161e-05, 'epoch': 1.93}
{'loss': 3.2047, 'grad_norm': 3.730958938598633, 'learning_rate': 7.120987654320987e-05, 'epoch': 1.93}
{'loss': 2.9041, 'grad_norm': 5.43135929107666, 'learning_rate': 7.114814814814816e-05, 'epoch': 1.93}
{'loss': 2.9529, 'grad_norm': 8.930295944213867, 'learning_rate': 7.108641975308642e-05, 'epoch': 1.94}
{'loss': 2.7441, 'grad_norm': 4.206943035125732, 'learning_rate': 7.102469135802469e-05, 'epoch': 1.94}
{'loss': 3.6228, 'grad_norm': 4.431305885314941, 'learning_rate': 7.096296296296297e-05, 'epoch': 1.94}
{'loss': 2.5605, 'grad_norm': 5.152261257171631, 'learning_rate': 7.090123456790124e-05, 'epoch': 1.94}
{'loss': 2.8624, 'grad_norm': 9.169718742370605, 'learning_rate': 7.083950617283951e-05, 'epoch': 1.94}
{'loss': 2.5592, 'grad_norm': 4.992839813232422, 'learning_rate': 7.077777777777777e-05, 'epoch': 1.94}
{'loss': 2.7844, 'grad_norm': 6.843775272369385, 'learning_rate': 7.071604938271605e-05, 'epoch': 1.94}
{'loss': 3.3254, 'grad_norm': 10.454997062683105, 'learning_rate': 7.065432098765432e-05, 'epoch': 1.94}
{'loss': 2.3924, 'grad_norm': 4.289632320404053, 'learning_rate': 7.05925925925926e-05, 'epoch': 1.94}
{'loss': 2.8944, 'grad_norm': 6.3550591468811035, 'learning_rate': 7.053086419753087e-05, 'epoch': 1.94}
{'loss': 2.987, 'grad_norm': 7.931032657623291, 'learning_rate': 7.046913580246914e-05, 'epoch': 1.94}
  warnings.warn(                                                                                                           
{'eval_loss': 2.7069668769836426, 'eval_runtime': 413.5046, 'eval_samples_per_second': 1.451, 'eval_steps_per_second': 1.451, 'epoch': 1.94}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                                           
{'loss': 2.57, 'grad_norm': 9.010884284973145, 'learning_rate': 7.040740740740742e-05, 'epoch': 1.95}
{'loss': 2.5096, 'grad_norm': 5.119513034820557, 'learning_rate': 7.034567901234568e-05, 'epoch': 1.95}
{'loss': 2.119, 'grad_norm': 5.8476152420043945, 'learning_rate': 7.028395061728395e-05, 'epoch': 1.95}
{'loss': 3.264, 'grad_norm': 6.504524230957031, 'learning_rate': 7.022222222222222e-05, 'epoch': 1.95}
{'loss': 3.2399, 'grad_norm': 8.983126640319824, 'learning_rate': 7.01604938271605e-05, 'epoch': 1.95}
{'loss': 2.0448, 'grad_norm': 7.0277276039123535, 'learning_rate': 7.009876543209877e-05, 'epoch': 1.95}
{'loss': 2.54, 'grad_norm': 4.385974884033203, 'learning_rate': 7.003703703703705e-05, 'epoch': 1.95}
{'loss': 2.5026, 'grad_norm': 4.88424825668335, 'learning_rate': 6.99753086419753e-05, 'epoch': 1.95}
{'loss': 3.2178, 'grad_norm': 8.858074188232422, 'learning_rate': 6.991358024691358e-05, 'epoch': 1.95}
{'loss': 2.46, 'grad_norm': 3.1081550121307373, 'learning_rate': 6.985185185185185e-05, 'epoch': 1.95}
{'loss': 3.2332, 'grad_norm': 5.553472995758057, 'learning_rate': 6.979012345679013e-05, 'epoch': 1.95}
{'loss': 2.7265, 'grad_norm': 5.566587448120117, 'learning_rate': 6.97283950617284e-05, 'epoch': 1.96}
{'loss': 2.8515, 'grad_norm': 3.8539421558380127, 'learning_rate': 6.966666666666668e-05, 'epoch': 1.96}
{'loss': 2.8102, 'grad_norm': 4.798822402954102, 'learning_rate': 6.960493827160495e-05, 'epoch': 1.96}
{'loss': 2.7094, 'grad_norm': 8.519399642944336, 'learning_rate': 6.954320987654321e-05, 'epoch': 1.96}
{'loss': 2.4948, 'grad_norm': 4.0947771072387695, 'learning_rate': 6.948148148148148e-05, 'epoch': 1.96}
{'loss': 2.8883, 'grad_norm': 4.485358715057373, 'learning_rate': 6.941975308641976e-05, 'epoch': 1.96}
{'loss': 2.7958, 'grad_norm': 4.017088890075684, 'learning_rate': 6.935802469135803e-05, 'epoch': 1.96}
{'loss': 2.6985, 'grad_norm': 4.451185703277588, 'learning_rate': 6.92962962962963e-05, 'epoch': 1.96}
{'loss': 2.3073, 'grad_norm': 3.630446672439575, 'learning_rate': 6.923456790123456e-05, 'epoch': 1.96}
{'loss': 2.9227, 'grad_norm': 6.610639572143555, 'learning_rate': 6.917283950617284e-05, 'epoch': 1.96}
{'loss': 2.8834, 'grad_norm': 4.508208751678467, 'learning_rate': 6.911111111111111e-05, 'epoch': 1.96}
{'loss': 2.4295, 'grad_norm': 5.123344898223877, 'learning_rate': 6.904938271604939e-05, 'epoch': 1.97}
{'loss': 2.6029, 'grad_norm': 5.221297740936279, 'learning_rate': 6.898765432098766e-05, 'epoch': 1.97}
{'loss': 3.936, 'grad_norm': 5.734447002410889, 'learning_rate': 6.892592592592593e-05, 'epoch': 1.97}
{'loss': 2.6752, 'grad_norm': 5.130468368530273, 'learning_rate': 6.886419753086421e-05, 'epoch': 1.97}
{'loss': 2.6379, 'grad_norm': 4.896350860595703, 'learning_rate': 6.880246913580247e-05, 'epoch': 1.97}
{'loss': 2.9257, 'grad_norm': 4.6951823234558105, 'learning_rate': 6.874074074074074e-05, 'epoch': 1.97}
{'loss': 3.1443, 'grad_norm': 3.7305078506469727, 'learning_rate': 6.867901234567901e-05, 'epoch': 1.97}
{'loss': 2.9378, 'grad_norm': 8.417450904846191, 'learning_rate': 6.861728395061729e-05, 'epoch': 1.97}
{'loss': 2.9856, 'grad_norm': 6.6291046142578125, 'learning_rate': 6.855555555555556e-05, 'epoch': 1.97}
{'loss': 2.6515, 'grad_norm': 4.303177356719971, 'learning_rate': 6.849382716049382e-05, 'epoch': 1.97}
{'loss': 2.956, 'grad_norm': 10.717041969299316, 'learning_rate': 6.843209876543211e-05, 'epoch': 1.98}
{'loss': 2.8808, 'grad_norm': 5.092548847198486, 'learning_rate': 6.837037037037037e-05, 'epoch': 1.98}
{'loss': 2.8357, 'grad_norm': 4.885144233703613, 'learning_rate': 6.830864197530864e-05, 'epoch': 1.98}
{'loss': 3.4635, 'grad_norm': 5.257580757141113, 'learning_rate': 6.824691358024692e-05, 'epoch': 1.98}
{'loss': 2.9087, 'grad_norm': 5.183627128601074, 'learning_rate': 6.818518518518519e-05, 'epoch': 1.98}
{'loss': 2.3646, 'grad_norm': 5.193194389343262, 'learning_rate': 6.812345679012347e-05, 'epoch': 1.98}
{'loss': 2.5933, 'grad_norm': 4.770822525024414, 'learning_rate': 6.806172839506173e-05, 'epoch': 1.98}
{'loss': 3.1211, 'grad_norm': 6.332512378692627, 'learning_rate': 6.800000000000001e-05, 'epoch': 1.98}
{'loss': 2.8905, 'grad_norm': 7.941270351409912, 'learning_rate': 6.793827160493827e-05, 'epoch': 1.98}
{'loss': 2.9188, 'grad_norm': 4.349846839904785, 'learning_rate': 6.787654320987655e-05, 'epoch': 1.98}
{'loss': 2.895, 'grad_norm': 6.328182697296143, 'learning_rate': 6.781481481481482e-05, 'epoch': 1.98}
{'loss': 3.6378, 'grad_norm': 5.1393656730651855, 'learning_rate': 6.775308641975308e-05, 'epoch': 1.99}
{'loss': 2.7764, 'grad_norm': 6.715023040771484, 'learning_rate': 6.769135802469137e-05, 'epoch': 1.99}
{'loss': 2.4503, 'grad_norm': 5.780460834503174, 'learning_rate': 6.762962962962963e-05, 'epoch': 1.99}
{'loss': 2.2415, 'grad_norm': 4.335625171661377, 'learning_rate': 6.756790123456792e-05, 'epoch': 1.99}
{'loss': 3.1476, 'grad_norm': 4.855902671813965, 'learning_rate': 6.750617283950618e-05, 'epoch': 1.99}
{'loss': 2.8972, 'grad_norm': 4.182611465454102, 'learning_rate': 6.744444444444445e-05, 'epoch': 1.99}
{'loss': 3.3316, 'grad_norm': 6.399723529815674, 'learning_rate': 6.738271604938272e-05, 'epoch': 1.99}
{'loss': 2.761, 'grad_norm': 5.868222713470459, 'learning_rate': 6.732098765432098e-05, 'epoch': 1.99}
{'loss': 3.0869, 'grad_norm': 6.76553201675415, 'learning_rate': 6.725925925925927e-05, 'epoch': 1.99}
{'loss': 3.5435, 'grad_norm': 4.875380516052246, 'learning_rate': 6.719753086419753e-05, 'epoch': 1.99}
{'loss': 2.7497, 'grad_norm': 5.494291305541992, 'learning_rate': 6.71358024691358e-05, 'epoch': 1.99}
{'loss': 2.6527, 'grad_norm': 5.576930046081543, 'learning_rate': 6.707407407407408e-05, 'epoch': 2.0}
{'loss': 2.7285, 'grad_norm': 5.960142135620117, 'learning_rate': 6.701234567901234e-05, 'epoch': 2.0}
{'loss': 2.7864, 'grad_norm': 3.7843360900878906, 'learning_rate': 6.695061728395063e-05, 'epoch': 2.0}
{'loss': 2.3487, 'grad_norm': 3.505457639694214, 'learning_rate': 6.688888888888889e-05, 'epoch': 2.0}
{'loss': 3.0986, 'grad_norm': 7.521742343902588, 'learning_rate': 6.682716049382717e-05, 'epoch': 2.0}
{'loss': 3.0607, 'grad_norm': 4.402223587036133, 'learning_rate': 6.676543209876543e-05, 'epoch': 2.0}
{'loss': 2.2315, 'grad_norm': 4.647372722625732, 'learning_rate': 6.670370370370371e-05, 'epoch': 2.0}
{'loss': 2.5521, 'grad_norm': 7.78883171081543, 'learning_rate': 6.664197530864198e-05, 'epoch': 2.0}
{'loss': 4.1967, 'grad_norm': 8.891942024230957, 'learning_rate': 6.658024691358024e-05, 'epoch': 2.0}
{'loss': 2.1109, 'grad_norm': 3.954739809036255, 'learning_rate': 6.651851851851853e-05, 'epoch': 2.0}
{'loss': 2.2641, 'grad_norm': 6.968895435333252, 'learning_rate': 6.645679012345679e-05, 'epoch': 2.0}
{'loss': 2.6049, 'grad_norm': 5.562133312225342, 'learning_rate': 6.639506172839506e-05, 'epoch': 2.01}
{'loss': 2.748, 'grad_norm': 6.0034613609313965, 'learning_rate': 6.633333333333334e-05, 'epoch': 2.01}
{'loss': 2.396, 'grad_norm': 4.312771797180176, 'learning_rate': 6.62716049382716e-05, 'epoch': 2.01}
{'loss': 2.4706, 'grad_norm': 8.432011604309082, 'learning_rate': 6.620987654320989e-05, 'epoch': 2.01}
{'loss': 2.9889, 'grad_norm': 3.6529860496520996, 'learning_rate': 6.614814814814815e-05, 'epoch': 2.01}
{'loss': 2.178, 'grad_norm': 4.255139350891113, 'learning_rate': 6.608641975308643e-05, 'epoch': 2.01}
{'loss': 2.6616, 'grad_norm': 6.058113098144531, 'learning_rate': 6.602469135802469e-05, 'epoch': 2.01}
{'loss': 2.8742, 'grad_norm': 10.87837028503418, 'learning_rate': 6.596296296296297e-05, 'epoch': 2.01}
{'loss': 2.8999, 'grad_norm': 4.162372589111328, 'learning_rate': 6.590123456790124e-05, 'epoch': 2.01}
{'loss': 3.0395, 'grad_norm': 4.180574893951416, 'learning_rate': 6.58395061728395e-05, 'epoch': 2.01}
{'loss': 3.0972, 'grad_norm': 3.6833479404449463, 'learning_rate': 6.577777777777779e-05, 'epoch': 2.01}
{'loss': 2.4004, 'grad_norm': 5.303365707397461, 'learning_rate': 6.571604938271605e-05, 'epoch': 2.02}
{'loss': 2.2562, 'grad_norm': 4.142404079437256, 'learning_rate': 6.565432098765432e-05, 'epoch': 2.02}
{'loss': 2.5575, 'grad_norm': 4.167563438415527, 'learning_rate': 6.55925925925926e-05, 'epoch': 2.02}
{'loss': 2.6929, 'grad_norm': 4.950218677520752, 'learning_rate': 6.553086419753087e-05, 'epoch': 2.02}
{'loss': 2.9777, 'grad_norm': 5.368699550628662, 'learning_rate': 6.546913580246914e-05, 'epoch': 2.02}
{'loss': 2.7056, 'grad_norm': 5.753688335418701, 'learning_rate': 6.54074074074074e-05, 'epoch': 2.02}
{'loss': 2.9459, 'grad_norm': 6.26458740234375, 'learning_rate': 6.534567901234568e-05, 'epoch': 2.02}
{'loss': 2.7265, 'grad_norm': 4.565927028656006, 'learning_rate': 6.528395061728395e-05, 'epoch': 2.02}
{'loss': 2.4404, 'grad_norm': 5.62406587600708, 'learning_rate': 6.522222222222222e-05, 'epoch': 2.02}
{'loss': 3.1132, 'grad_norm': 4.0917229652404785, 'learning_rate': 6.51604938271605e-05, 'epoch': 2.02}
{'loss': 2.81, 'grad_norm': 6.914229393005371, 'learning_rate': 6.509876543209877e-05, 'epoch': 2.02}
{'loss': 2.7121, 'grad_norm': 5.509322643280029, 'learning_rate': 6.503703703703705e-05, 'epoch': 2.03}
{'loss': 2.3943, 'grad_norm': 5.173908710479736, 'learning_rate': 6.49753086419753e-05, 'epoch': 2.03}
{'loss': 3.155, 'grad_norm': 8.639824867248535, 'learning_rate': 6.491358024691358e-05, 'epoch': 2.03}
{'loss': 2.6632, 'grad_norm': 4.258796215057373, 'learning_rate': 6.485185185185185e-05, 'epoch': 2.03}
{'loss': 2.5487, 'grad_norm': 5.6582841873168945, 'learning_rate': 6.479012345679013e-05, 'epoch': 2.03}
{'loss': 2.6403, 'grad_norm': 4.188715934753418, 'learning_rate': 6.47283950617284e-05, 'epoch': 2.03}
{'loss': 2.8434, 'grad_norm': 3.6211557388305664, 'learning_rate': 6.466666666666666e-05, 'epoch': 2.03}
{'loss': 2.6884, 'grad_norm': 8.911165237426758, 'learning_rate': 6.460493827160494e-05, 'epoch': 2.03}
{'loss': 3.2207, 'grad_norm': 5.343064308166504, 'learning_rate': 6.454320987654321e-05, 'epoch': 2.03}
{'loss': 3.0309, 'grad_norm': 6.502140998840332, 'learning_rate': 6.448148148148148e-05, 'epoch': 2.03}
{'loss': 2.8157, 'grad_norm': 6.64519739151001, 'learning_rate': 6.441975308641976e-05, 'epoch': 2.04}
{'loss': 3.0776, 'grad_norm': 9.771990776062012, 'learning_rate': 6.435802469135803e-05, 'epoch': 2.04}
{'loss': 3.0984, 'grad_norm': 5.3430705070495605, 'learning_rate': 6.42962962962963e-05, 'epoch': 2.04}
  warnings.warn(                                                                                      
{'eval_loss': 2.7063469886779785, 'eval_runtime': 413.4906, 'eval_samples_per_second': 1.451, 'eval_steps_per_second': 1.451, 'epoch': 2.04}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.4527, 'grad_norm': 5.257244110107422, 'learning_rate': 6.423456790123456e-05, 'epoch': 2.04}
{'loss': 3.4184, 'grad_norm': 5.159951686859131, 'learning_rate': 6.417283950617284e-05, 'epoch': 2.04}
{'loss': 3.1132, 'grad_norm': 6.07464075088501, 'learning_rate': 6.411111111111111e-05, 'epoch': 2.04}
{'loss': 2.7601, 'grad_norm': 3.798884630203247, 'learning_rate': 6.404938271604939e-05, 'epoch': 2.04}
{'loss': 2.7862, 'grad_norm': 3.3161818981170654, 'learning_rate': 6.398765432098766e-05, 'epoch': 2.04}
{'loss': 2.6992, 'grad_norm': 5.722412109375, 'learning_rate': 6.392592592592593e-05, 'epoch': 2.04}
{'loss': 2.3587, 'grad_norm': 4.799414157867432, 'learning_rate': 6.38641975308642e-05, 'epoch': 2.04}
{'loss': 2.5291, 'grad_norm': 6.479038238525391, 'learning_rate': 6.380246913580247e-05, 'epoch': 2.04}
{'loss': 2.5891, 'grad_norm': 6.3036699295043945, 'learning_rate': 6.374074074074074e-05, 'epoch': 2.05}
{'loss': 2.6358, 'grad_norm': 6.15410852432251, 'learning_rate': 6.367901234567902e-05, 'epoch': 2.05}
{'loss': 3.2765, 'grad_norm': 4.816832542419434, 'learning_rate': 6.361728395061729e-05, 'epoch': 2.05}
{'loss': 2.505, 'grad_norm': 6.670268535614014, 'learning_rate': 6.355555555555556e-05, 'epoch': 2.05}
{'loss': 2.9366, 'grad_norm': 5.587503910064697, 'learning_rate': 6.349382716049384e-05, 'epoch': 2.05}
{'loss': 3.1604, 'grad_norm': 24.32608985900879, 'learning_rate': 6.34320987654321e-05, 'epoch': 2.05}
{'loss': 2.4132, 'grad_norm': 6.182607173919678, 'learning_rate': 6.337037037037037e-05, 'epoch': 2.05}
{'loss': 2.7282, 'grad_norm': 4.445930480957031, 'learning_rate': 6.330864197530864e-05, 'epoch': 2.05}
{'loss': 2.4268, 'grad_norm': 3.780503511428833, 'learning_rate': 6.324691358024692e-05, 'epoch': 2.05}
{'loss': 2.3419, 'grad_norm': 4.8168864250183105, 'learning_rate': 6.318518518518519e-05, 'epoch': 2.05}
{'loss': 2.3255, 'grad_norm': 5.454299449920654, 'learning_rate': 6.312345679012345e-05, 'epoch': 2.05}
{'loss': 3.2398, 'grad_norm': 5.8757219314575195, 'learning_rate': 6.306172839506174e-05, 'epoch': 2.06}
{'loss': 2.7866, 'grad_norm': 6.117912769317627, 'learning_rate': 6.3e-05, 'epoch': 2.06}
{'loss': 2.9546, 'grad_norm': 4.934657096862793, 'learning_rate': 6.293827160493827e-05, 'epoch': 2.06}
{'loss': 2.2343, 'grad_norm': 9.482428550720215, 'learning_rate': 6.287654320987655e-05, 'epoch': 2.06}
{'loss': 2.4925, 'grad_norm': 6.950913429260254, 'learning_rate': 6.281481481481482e-05, 'epoch': 2.06}
{'loss': 2.7673, 'grad_norm': 4.558872699737549, 'learning_rate': 6.27530864197531e-05, 'epoch': 2.06}
{'loss': 3.0223, 'grad_norm': 4.460453987121582, 'learning_rate': 6.269135802469136e-05, 'epoch': 2.06}
{'loss': 3.1275, 'grad_norm': 5.980405807495117, 'learning_rate': 6.262962962962964e-05, 'epoch': 2.06}
{'loss': 2.3297, 'grad_norm': 6.044805526733398, 'learning_rate': 6.25679012345679e-05, 'epoch': 2.06}
{'loss': 2.7801, 'grad_norm': 7.678963661193848, 'learning_rate': 6.250617283950618e-05, 'epoch': 2.06}
{'loss': 3.0457, 'grad_norm': 5.402866363525391, 'learning_rate': 6.244444444444445e-05, 'epoch': 2.06}
{'loss': 1.8014, 'grad_norm': 4.529799461364746, 'learning_rate': 6.238271604938271e-05, 'epoch': 2.07}
{'loss': 3.4844, 'grad_norm': 5.511253833770752, 'learning_rate': 6.2320987654321e-05, 'epoch': 2.07}
{'loss': 2.8603, 'grad_norm': 6.464457035064697, 'learning_rate': 6.225925925925926e-05, 'epoch': 2.07}
{'loss': 2.8821, 'grad_norm': 5.291919231414795, 'learning_rate': 6.219753086419753e-05, 'epoch': 2.07}
{'loss': 2.6276, 'grad_norm': 5.342041969299316, 'learning_rate': 6.21358024691358e-05, 'epoch': 2.07}
{'loss': 2.5255, 'grad_norm': 5.015181064605713, 'learning_rate': 6.207407407407408e-05, 'epoch': 2.07}
{'loss': 2.2986, 'grad_norm': 4.729461669921875, 'learning_rate': 6.201234567901235e-05, 'epoch': 2.07}
{'loss': 2.1207, 'grad_norm': 4.998441696166992, 'learning_rate': 6.195061728395061e-05, 'epoch': 2.07}
{'loss': 3.1448, 'grad_norm': 5.615572452545166, 'learning_rate': 6.18888888888889e-05, 'epoch': 2.07}
{'loss': 2.0474, 'grad_norm': 4.136787414550781, 'learning_rate': 6.182716049382716e-05, 'epoch': 2.07}
{'loss': 3.9238, 'grad_norm': 4.88931941986084, 'learning_rate': 6.176543209876543e-05, 'epoch': 2.08}
{'loss': 2.5782, 'grad_norm': 4.907289981842041, 'learning_rate': 6.170370370370371e-05, 'epoch': 2.08}
{'loss': 1.9946, 'grad_norm': 4.873592853546143, 'learning_rate': 6.164197530864197e-05, 'epoch': 2.08}
{'loss': 2.4704, 'grad_norm': 8.935851097106934, 'learning_rate': 6.158024691358026e-05, 'epoch': 2.08}
{'loss': 3.1521, 'grad_norm': 5.726729869842529, 'learning_rate': 6.151851851851852e-05, 'epoch': 2.08}
{'loss': 2.3532, 'grad_norm': 4.073261260986328, 'learning_rate': 6.14567901234568e-05, 'epoch': 2.08}
{'loss': 2.8813, 'grad_norm': 5.752762317657471, 'learning_rate': 6.139506172839506e-05, 'epoch': 2.08}
{'loss': 2.4675, 'grad_norm': 5.4413299560546875, 'learning_rate': 6.133333333333334e-05, 'epoch': 2.08}
{'loss': 2.3061, 'grad_norm': 5.399168968200684, 'learning_rate': 6.127160493827161e-05, 'epoch': 2.08}
{'loss': 2.7014, 'grad_norm': 6.73828649520874, 'learning_rate': 6.120987654320987e-05, 'epoch': 2.08}
{'loss': 2.7934, 'grad_norm': 6.698798179626465, 'learning_rate': 6.114814814814816e-05, 'epoch': 2.08}
{'loss': 2.3235, 'grad_norm': 4.918240547180176, 'learning_rate': 6.108641975308642e-05, 'epoch': 2.09}
{'loss': 3.0497, 'grad_norm': 6.746330738067627, 'learning_rate': 6.10246913580247e-05, 'epoch': 2.09}
{'loss': 2.3406, 'grad_norm': 4.972590923309326, 'learning_rate': 6.096296296296297e-05, 'epoch': 2.09}
{'loss': 2.6827, 'grad_norm': 6.784214496612549, 'learning_rate': 6.0901234567901234e-05, 'epoch': 2.09}
{'loss': 2.7324, 'grad_norm': 6.3499908447265625, 'learning_rate': 6.083950617283951e-05, 'epoch': 2.09}
{'loss': 2.5397, 'grad_norm': 3.864241600036621, 'learning_rate': 6.0777777777777775e-05, 'epoch': 2.09}
{'loss': 2.0401, 'grad_norm': 4.344887733459473, 'learning_rate': 6.0716049382716055e-05, 'epoch': 2.09}
{'loss': 2.3509, 'grad_norm': 5.087818622589111, 'learning_rate': 6.065432098765432e-05, 'epoch': 2.09}
{'loss': 2.533, 'grad_norm': 6.597871780395508, 'learning_rate': 6.05925925925926e-05, 'epoch': 2.09}
{'loss': 2.7686, 'grad_norm': 6.28061056137085, 'learning_rate': 6.053086419753087e-05, 'epoch': 2.09}
{'loss': 3.1792, 'grad_norm': 10.773768424987793, 'learning_rate': 6.046913580246914e-05, 'epoch': 2.09}
{'loss': 3.2951, 'grad_norm': 6.863739013671875, 'learning_rate': 6.040740740740741e-05, 'epoch': 2.1}
{'loss': 2.4038, 'grad_norm': 3.921283721923828, 'learning_rate': 6.034567901234568e-05, 'epoch': 2.1}
{'loss': 2.2889, 'grad_norm': 8.249430656433105, 'learning_rate': 6.028395061728396e-05, 'epoch': 2.1}
{'loss': 2.6884, 'grad_norm': 7.314764976501465, 'learning_rate': 6.0222222222222225e-05, 'epoch': 2.1}
{'loss': 2.7779, 'grad_norm': 7.416573524475098, 'learning_rate': 6.016049382716049e-05, 'epoch': 2.1}
{'loss': 2.3877, 'grad_norm': 5.460142135620117, 'learning_rate': 6.0098765432098766e-05, 'epoch': 2.1}
{'loss': 3.1257, 'grad_norm': 8.519573211669922, 'learning_rate': 6.003703703703703e-05, 'epoch': 2.1}
{'loss': 2.7908, 'grad_norm': 7.57172155380249, 'learning_rate': 5.9975308641975314e-05, 'epoch': 2.1}
{'loss': 2.5426, 'grad_norm': 8.451539993286133, 'learning_rate': 5.991358024691358e-05, 'epoch': 2.1}
{'loss': 2.8067, 'grad_norm': 5.048714637756348, 'learning_rate': 5.985185185185186e-05, 'epoch': 2.1}
{'loss': 2.4665, 'grad_norm': 4.181381702423096, 'learning_rate': 5.979012345679013e-05, 'epoch': 2.1}
{'loss': 3.0569, 'grad_norm': 5.81977653503418, 'learning_rate': 5.9728395061728395e-05, 'epoch': 2.11}
{'loss': 2.2494, 'grad_norm': 6.371474742889404, 'learning_rate': 5.966666666666667e-05, 'epoch': 2.11}
{'loss': 3.7218, 'grad_norm': 5.358131408691406, 'learning_rate': 5.9604938271604936e-05, 'epoch': 2.11}
{'loss': 2.375, 'grad_norm': 7.8860673904418945, 'learning_rate': 5.954320987654322e-05, 'epoch': 2.11}
{'loss': 2.5811, 'grad_norm': 5.52492618560791, 'learning_rate': 5.9481481481481484e-05, 'epoch': 2.11}
{'loss': 2.7583, 'grad_norm': 5.090794563293457, 'learning_rate': 5.941975308641976e-05, 'epoch': 2.11}
{'loss': 3.2488, 'grad_norm': 5.1939697265625, 'learning_rate': 5.9358024691358024e-05, 'epoch': 2.11}
{'loss': 2.3015, 'grad_norm': 4.505670070648193, 'learning_rate': 5.929629629629629e-05, 'epoch': 2.11}
{'loss': 3.2041, 'grad_norm': 7.924904823303223, 'learning_rate': 5.923456790123457e-05, 'epoch': 2.11}
{'loss': 2.754, 'grad_norm': 5.070210933685303, 'learning_rate': 5.917283950617284e-05, 'epoch': 2.11}
{'loss': 2.8834, 'grad_norm': 7.167827606201172, 'learning_rate': 5.911111111111112e-05, 'epoch': 2.11}
{'loss': 2.426, 'grad_norm': 5.403106212615967, 'learning_rate': 5.904938271604939e-05, 'epoch': 2.12}
{'loss': 3.1611, 'grad_norm': 7.662105083465576, 'learning_rate': 5.898765432098766e-05, 'epoch': 2.12}
{'loss': 2.9516, 'grad_norm': 10.90983772277832, 'learning_rate': 5.892592592592593e-05, 'epoch': 2.12}
{'loss': 2.5737, 'grad_norm': 6.253375053405762, 'learning_rate': 5.8864197530864194e-05, 'epoch': 2.12}
{'loss': 2.8461, 'grad_norm': 6.084298133850098, 'learning_rate': 5.8802469135802475e-05, 'epoch': 2.12}
{'loss': 2.4914, 'grad_norm': 6.114695072174072, 'learning_rate': 5.874074074074074e-05, 'epoch': 2.12}
{'loss': 2.7385, 'grad_norm': 6.25112247467041, 'learning_rate': 5.8679012345679016e-05, 'epoch': 2.12}
{'loss': 2.4203, 'grad_norm': 5.54033088684082, 'learning_rate': 5.861728395061728e-05, 'epoch': 2.12}
{'loss': 2.7101, 'grad_norm': 4.231837272644043, 'learning_rate': 5.855555555555556e-05, 'epoch': 2.12}
{'loss': 2.3464, 'grad_norm': 3.9872984886169434, 'learning_rate': 5.849382716049383e-05, 'epoch': 2.12}
{'loss': 2.4598, 'grad_norm': 8.485745429992676, 'learning_rate': 5.84320987654321e-05, 'epoch': 2.12}
{'loss': 2.1793, 'grad_norm': 4.973787784576416, 'learning_rate': 5.837037037037038e-05, 'epoch': 2.13}
{'loss': 2.5627, 'grad_norm': 4.705873489379883, 'learning_rate': 5.8308641975308645e-05, 'epoch': 2.13}
{'loss': 2.7558, 'grad_norm': 5.2261528968811035, 'learning_rate': 5.824691358024692e-05, 'epoch': 2.13}
{'loss': 2.6147, 'grad_norm': 5.188419818878174, 'learning_rate': 5.8185185185185186e-05, 'epoch': 2.13}
{'loss': 3.319, 'grad_norm': 5.404627799987793, 'learning_rate': 5.8123456790123466e-05, 'epoch': 2.13}
  warnings.warn(                                                                                      
{'eval_loss': 2.7055492401123047, 'eval_runtime': 412.053, 'eval_samples_per_second': 1.456, 'eval_steps_per_second': 1.456, 'epoch': 2.13}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.4932, 'grad_norm': 4.948166847229004, 'learning_rate': 5.806172839506173e-05, 'epoch': 2.13}
{'loss': 2.6067, 'grad_norm': 5.109818935394287, 'learning_rate': 5.8e-05, 'epoch': 2.13}
{'loss': 2.545, 'grad_norm': 6.090786933898926, 'learning_rate': 5.7938271604938274e-05, 'epoch': 2.13}
{'loss': 2.527, 'grad_norm': 5.953309059143066, 'learning_rate': 5.787654320987654e-05, 'epoch': 2.13}
{'loss': 2.7213, 'grad_norm': 5.895705699920654, 'learning_rate': 5.781481481481482e-05, 'epoch': 2.13}
{'loss': 3.5183, 'grad_norm': 9.308809280395508, 'learning_rate': 5.775308641975309e-05, 'epoch': 2.14}
{'loss': 2.7903, 'grad_norm': 7.0184102058410645, 'learning_rate': 5.7691358024691356e-05, 'epoch': 2.14}
{'loss': 2.7684, 'grad_norm': 5.178775310516357, 'learning_rate': 5.7629629629629636e-05, 'epoch': 2.14}
{'loss': 2.5721, 'grad_norm': 7.281202793121338, 'learning_rate': 5.75679012345679e-05, 'epoch': 2.14}
{'loss': 3.0251, 'grad_norm': 5.14561128616333, 'learning_rate': 5.750617283950618e-05, 'epoch': 2.14}
{'loss': 2.7178, 'grad_norm': 7.692835330963135, 'learning_rate': 5.7444444444444444e-05, 'epoch': 2.14}
{'loss': 2.8379, 'grad_norm': 6.316153049468994, 'learning_rate': 5.7382716049382725e-05, 'epoch': 2.14}
{'loss': 2.8906, 'grad_norm': 4.847637176513672, 'learning_rate': 5.732098765432099e-05, 'epoch': 2.14}
{'loss': 2.7198, 'grad_norm': 4.619415760040283, 'learning_rate': 5.725925925925926e-05, 'epoch': 2.14}
{'loss': 1.8494, 'grad_norm': 6.981899738311768, 'learning_rate': 5.719753086419753e-05, 'epoch': 2.14}
{'loss': 2.6864, 'grad_norm': 5.8989644050598145, 'learning_rate': 5.71358024691358e-05, 'epoch': 2.14}
{'loss': 2.6531, 'grad_norm': 4.73486328125, 'learning_rate': 5.707407407407408e-05, 'epoch': 2.15}
{'loss': 2.3212, 'grad_norm': 6.4173173904418945, 'learning_rate': 5.701234567901235e-05, 'epoch': 2.15}
{'loss': 2.5063, 'grad_norm': 3.8150179386138916, 'learning_rate': 5.695061728395063e-05, 'epoch': 2.15}
{'loss': 2.428, 'grad_norm': 6.083053112030029, 'learning_rate': 5.6888888888888895e-05, 'epoch': 2.15}
{'loss': 2.6637, 'grad_norm': 4.136608600616455, 'learning_rate': 5.682716049382716e-05, 'epoch': 2.15}
{'loss': 2.2343, 'grad_norm': 4.075809001922607, 'learning_rate': 5.6765432098765435e-05, 'epoch': 2.15}
{'loss': 2.5738, 'grad_norm': 9.311080932617188, 'learning_rate': 5.67037037037037e-05, 'epoch': 2.15}
{'loss': 2.8447, 'grad_norm': 4.421613693237305, 'learning_rate': 5.664197530864198e-05, 'epoch': 2.15}
{'loss': 2.7908, 'grad_norm': 6.554210662841797, 'learning_rate': 5.658024691358025e-05, 'epoch': 2.15}
{'loss': 2.8532, 'grad_norm': 6.869311809539795, 'learning_rate': 5.6518518518518524e-05, 'epoch': 2.15}
{'loss': 2.637, 'grad_norm': 3.260913133621216, 'learning_rate': 5.645679012345679e-05, 'epoch': 2.15}
{'loss': 2.544, 'grad_norm': 4.62531042098999, 'learning_rate': 5.639506172839506e-05, 'epoch': 2.16}
{'loss': 2.8519, 'grad_norm': 5.812037467956543, 'learning_rate': 5.633333333333334e-05, 'epoch': 2.16}
{'loss': 3.3527, 'grad_norm': 7.758358001708984, 'learning_rate': 5.6271604938271605e-05, 'epoch': 2.16}
{'loss': 1.8735, 'grad_norm': 4.122522354125977, 'learning_rate': 5.6209876543209886e-05, 'epoch': 2.16}
{'loss': 2.7273, 'grad_norm': 5.20061731338501, 'learning_rate': 5.614814814814815e-05, 'epoch': 2.16}
{'loss': 2.0743, 'grad_norm': 4.9596734046936035, 'learning_rate': 5.608641975308643e-05, 'epoch': 2.16}
{'loss': 2.3173, 'grad_norm': 3.3670310974121094, 'learning_rate': 5.6024691358024694e-05, 'epoch': 2.16}
{'loss': 2.9265, 'grad_norm': 4.5414347648620605, 'learning_rate': 5.596296296296296e-05, 'epoch': 2.16}
{'loss': 2.5928, 'grad_norm': 8.318451881408691, 'learning_rate': 5.590123456790124e-05, 'epoch': 2.16}
{'loss': 3.0472, 'grad_norm': 5.862398147583008, 'learning_rate': 5.583950617283951e-05, 'epoch': 2.16}
{'loss': 2.5549, 'grad_norm': 7.045017242431641, 'learning_rate': 5.577777777777778e-05, 'epoch': 2.16}
{'loss': 2.5199, 'grad_norm': 6.283923149108887, 'learning_rate': 5.571604938271605e-05, 'epoch': 2.17}
{'loss': 2.9427, 'grad_norm': 8.256413459777832, 'learning_rate': 5.5654320987654316e-05, 'epoch': 2.17}
{'loss': 2.8268, 'grad_norm': 6.1454758644104, 'learning_rate': 5.55925925925926e-05, 'epoch': 2.17}
{'loss': 2.7396, 'grad_norm': 10.130622863769531, 'learning_rate': 5.5530864197530864e-05, 'epoch': 2.17}
{'loss': 2.3797, 'grad_norm': 3.772820234298706, 'learning_rate': 5.548148148148148e-05, 'epoch': 2.17}
{'loss': 2.7821, 'grad_norm': 12.74034595489502, 'learning_rate': 5.5419753086419755e-05, 'epoch': 2.17}
{'loss': 2.5062, 'grad_norm': 4.123752117156982, 'learning_rate': 5.535802469135802e-05, 'epoch': 2.17}
{'loss': 2.7289, 'grad_norm': 4.522000312805176, 'learning_rate': 5.52962962962963e-05, 'epoch': 2.17}
{'loss': 3.0939, 'grad_norm': 5.97663688659668, 'learning_rate': 5.523456790123457e-05, 'epoch': 2.17}
{'loss': 3.2658, 'grad_norm': 4.739026069641113, 'learning_rate': 5.5172839506172843e-05, 'epoch': 2.17}
{'loss': 2.7441, 'grad_norm': 5.98363733291626, 'learning_rate': 5.511111111111111e-05, 'epoch': 2.17}
{'loss': 2.4628, 'grad_norm': 3.608470916748047, 'learning_rate': 5.504938271604938e-05, 'epoch': 2.18}
{'loss': 2.8404, 'grad_norm': 7.8047661781311035, 'learning_rate': 5.498765432098766e-05, 'epoch': 2.18}
{'loss': 2.6687, 'grad_norm': 4.090211391448975, 'learning_rate': 5.4925925925925925e-05, 'epoch': 2.18}
{'loss': 3.1346, 'grad_norm': 9.093720436096191, 'learning_rate': 5.4864197530864206e-05, 'epoch': 2.18}
{'loss': 2.6288, 'grad_norm': 6.427942752838135, 'learning_rate': 5.480246913580247e-05, 'epoch': 2.18}
{'loss': 3.1976, 'grad_norm': 5.4186625480651855, 'learning_rate': 5.4740740740740746e-05, 'epoch': 2.18}
{'loss': 2.5743, 'grad_norm': 4.363254547119141, 'learning_rate': 5.4679012345679013e-05, 'epoch': 2.18}
{'loss': 2.5949, 'grad_norm': 9.430184364318848, 'learning_rate': 5.461728395061728e-05, 'epoch': 2.18}
{'loss': 3.1979, 'grad_norm': 4.817688941955566, 'learning_rate': 5.455555555555556e-05, 'epoch': 2.18}
{'loss': 2.8147, 'grad_norm': 5.2935028076171875, 'learning_rate': 5.449382716049383e-05, 'epoch': 2.18}
{'loss': 3.2145, 'grad_norm': 6.90031623840332, 'learning_rate': 5.44320987654321e-05, 'epoch': 2.19}
{'loss': 2.8915, 'grad_norm': 4.146831035614014, 'learning_rate': 5.437037037037037e-05, 'epoch': 2.19}
{'loss': 2.5638, 'grad_norm': 9.451601028442383, 'learning_rate': 5.430864197530865e-05, 'epoch': 2.19}
{'loss': 2.7119, 'grad_norm': 5.526691913604736, 'learning_rate': 5.4246913580246916e-05, 'epoch': 2.19}
{'loss': 2.763, 'grad_norm': 6.100063323974609, 'learning_rate': 5.4185185185185183e-05, 'epoch': 2.19}
{'loss': 2.863, 'grad_norm': 5.657125949859619, 'learning_rate': 5.4123456790123464e-05, 'epoch': 2.19}
{'loss': 2.9217, 'grad_norm': 4.933946132659912, 'learning_rate': 5.406172839506173e-05, 'epoch': 2.19}
{'loss': 2.8468, 'grad_norm': 8.125434875488281, 'learning_rate': 5.4000000000000005e-05, 'epoch': 2.19}
{'loss': 3.0266, 'grad_norm': 9.982282638549805, 'learning_rate': 5.393827160493827e-05, 'epoch': 2.19}
{'loss': 2.1379, 'grad_norm': 5.414762020111084, 'learning_rate': 5.387654320987655e-05, 'epoch': 2.19}
{'loss': 3.2613, 'grad_norm': 6.917311191558838, 'learning_rate': 5.381481481481482e-05, 'epoch': 2.19}
{'loss': 2.843, 'grad_norm': 8.087315559387207, 'learning_rate': 5.3753086419753086e-05, 'epoch': 2.2}
{'loss': 3.0627, 'grad_norm': 5.45245885848999, 'learning_rate': 5.369135802469136e-05, 'epoch': 2.2}
{'loss': 2.5698, 'grad_norm': 5.724452495574951, 'learning_rate': 5.362962962962963e-05, 'epoch': 2.2}
{'loss': 2.4394, 'grad_norm': 7.130449295043945, 'learning_rate': 5.356790123456791e-05, 'epoch': 2.2}
{'loss': 3.1641, 'grad_norm': 4.686941623687744, 'learning_rate': 5.3506172839506175e-05, 'epoch': 2.2}
{'loss': 3.4686, 'grad_norm': 3.4799883365631104, 'learning_rate': 5.3444444444444455e-05, 'epoch': 2.2}
{'loss': 2.6876, 'grad_norm': 4.195199966430664, 'learning_rate': 5.338271604938272e-05, 'epoch': 2.2}
{'loss': 2.5378, 'grad_norm': 4.021090984344482, 'learning_rate': 5.332098765432099e-05, 'epoch': 2.2}
{'loss': 2.8174, 'grad_norm': 6.721956729888916, 'learning_rate': 5.325925925925926e-05, 'epoch': 2.2}
{'loss': 3.1617, 'grad_norm': 7.038358688354492, 'learning_rate': 5.319753086419753e-05, 'epoch': 2.2}
{'loss': 2.816, 'grad_norm': 4.673220634460449, 'learning_rate': 5.313580246913581e-05, 'epoch': 2.2}
{'loss': 2.7194, 'grad_norm': 8.683624267578125, 'learning_rate': 5.307407407407408e-05, 'epoch': 2.21}
{'loss': 2.6317, 'grad_norm': 5.9924798011779785, 'learning_rate': 5.3012345679012345e-05, 'epoch': 2.21}
{'loss': 2.3209, 'grad_norm': 4.4802165031433105, 'learning_rate': 5.295061728395062e-05, 'epoch': 2.21}
{'loss': 2.803, 'grad_norm': 5.786491870880127, 'learning_rate': 5.2888888888888885e-05, 'epoch': 2.21}
{'loss': 2.9568, 'grad_norm': 6.4874749183654785, 'learning_rate': 5.2827160493827166e-05, 'epoch': 2.21}
{'loss': 3.584, 'grad_norm': 3.3995561599731445, 'learning_rate': 5.276543209876543e-05, 'epoch': 2.21}
{'loss': 2.321, 'grad_norm': 5.090339660644531, 'learning_rate': 5.2703703703703714e-05, 'epoch': 2.21}
{'loss': 3.0593, 'grad_norm': 16.31871223449707, 'learning_rate': 5.264197530864198e-05, 'epoch': 2.21}
{'loss': 2.6938, 'grad_norm': 7.516047477722168, 'learning_rate': 5.258024691358025e-05, 'epoch': 2.21}
{'loss': 2.3516, 'grad_norm': 4.035016059875488, 'learning_rate': 5.251851851851852e-05, 'epoch': 2.21}
{'loss': 3.1588, 'grad_norm': 13.00253677368164, 'learning_rate': 5.245679012345679e-05, 'epoch': 2.21}
{'loss': 3.0015, 'grad_norm': 4.736677646636963, 'learning_rate': 5.239506172839507e-05, 'epoch': 2.22}
{'loss': 2.9057, 'grad_norm': 7.919316291809082, 'learning_rate': 5.2333333333333336e-05, 'epoch': 2.22}
{'loss': 2.4619, 'grad_norm': 11.597914695739746, 'learning_rate': 5.227160493827161e-05, 'epoch': 2.22}
{'loss': 3.0242, 'grad_norm': 11.69769287109375, 'learning_rate': 5.220987654320988e-05, 'epoch': 2.22}
{'loss': 3.0971, 'grad_norm': 5.885128498077393, 'learning_rate': 5.2148148148148144e-05, 'epoch': 2.22}
{'loss': 3.4651, 'grad_norm': 6.566462993621826, 'learning_rate': 5.2086419753086424e-05, 'epoch': 2.22}
{'loss': 2.7749, 'grad_norm': 9.544536590576172, 'learning_rate': 5.202469135802469e-05, 'epoch': 2.22}
{'loss': 2.9021, 'grad_norm': 6.549741744995117, 'learning_rate': 5.196296296296297e-05, 'epoch': 2.22}
  warnings.warn(                                                                                      
{'eval_loss': 2.695924997329712, 'eval_runtime': 412.5777, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 1.454, 'epoch': 2.22}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.6385, 'grad_norm': 4.684302806854248, 'learning_rate': 5.190123456790124e-05, 'epoch': 2.22}
{'loss': 2.5142, 'grad_norm': 4.641409397125244, 'learning_rate': 5.183950617283951e-05, 'epoch': 2.22}
{'loss': 3.0593, 'grad_norm': 4.316633224487305, 'learning_rate': 5.177777777777778e-05, 'epoch': 2.23}
{'loss': 2.8394, 'grad_norm': 8.4218168258667, 'learning_rate': 5.171604938271605e-05, 'epoch': 2.23}
{'loss': 2.9159, 'grad_norm': 9.666667938232422, 'learning_rate': 5.165432098765433e-05, 'epoch': 2.23}
{'loss': 3.0149, 'grad_norm': 6.698500156402588, 'learning_rate': 5.1592592592592594e-05, 'epoch': 2.23}
{'loss': 3.2272, 'grad_norm': 4.229693412780762, 'learning_rate': 5.153086419753087e-05, 'epoch': 2.23}
{'loss': 2.6937, 'grad_norm': 22.618568420410156, 'learning_rate': 5.1469135802469135e-05, 'epoch': 2.23}
{'loss': 2.3599, 'grad_norm': 4.050175666809082, 'learning_rate': 5.1407407407407416e-05, 'epoch': 2.23}
{'loss': 2.2556, 'grad_norm': 5.071486949920654, 'learning_rate': 5.134567901234568e-05, 'epoch': 2.23}
{'loss': 2.2423, 'grad_norm': 6.74315881729126, 'learning_rate': 5.128395061728395e-05, 'epoch': 2.23}
{'loss': 3.1014, 'grad_norm': 4.005022048950195, 'learning_rate': 5.122222222222223e-05, 'epoch': 2.23}
{'loss': 2.9392, 'grad_norm': 4.347785472869873, 'learning_rate': 5.11604938271605e-05, 'epoch': 2.23}
{'loss': 3.0813, 'grad_norm': 8.340980529785156, 'learning_rate': 5.109876543209877e-05, 'epoch': 2.24}
{'loss': 3.0621, 'grad_norm': 5.7449259757995605, 'learning_rate': 5.103703703703704e-05, 'epoch': 2.24}
{'loss': 2.5806, 'grad_norm': 4.810673713684082, 'learning_rate': 5.0975308641975305e-05, 'epoch': 2.24}
{'loss': 2.1227, 'grad_norm': 4.994187355041504, 'learning_rate': 5.0913580246913586e-05, 'epoch': 2.24}
{'loss': 3.1295, 'grad_norm': 5.668885231018066, 'learning_rate': 5.085185185185185e-05, 'epoch': 2.24}
{'loss': 3.3797, 'grad_norm': 15.678468704223633, 'learning_rate': 5.0790123456790126e-05, 'epoch': 2.24}
{'loss': 2.4114, 'grad_norm': 4.634156703948975, 'learning_rate': 5.0728395061728393e-05, 'epoch': 2.24}
{'loss': 3.0239, 'grad_norm': 4.59739351272583, 'learning_rate': 5.0666666666666674e-05, 'epoch': 2.24}
{'loss': 2.0146, 'grad_norm': 4.69181489944458, 'learning_rate': 5.060493827160494e-05, 'epoch': 2.24}
{'loss': 2.964, 'grad_norm': 3.5035560131073, 'learning_rate': 5.054320987654321e-05, 'epoch': 2.24}
{'loss': 2.6866, 'grad_norm': 3.6445014476776123, 'learning_rate': 5.048148148148148e-05, 'epoch': 2.24}
{'loss': 3.3233, 'grad_norm': 5.08696985244751, 'learning_rate': 5.041975308641975e-05, 'epoch': 2.25}
{'loss': 2.4017, 'grad_norm': 4.884110927581787, 'learning_rate': 5.035802469135803e-05, 'epoch': 2.25}
{'loss': 3.0565, 'grad_norm': 3.785884141921997, 'learning_rate': 5.0296296296296296e-05, 'epoch': 2.25}
{'loss': 3.1258, 'grad_norm': 5.9968109130859375, 'learning_rate': 5.023456790123458e-05, 'epoch': 2.25}
{'loss': 2.8085, 'grad_norm': 5.271413803100586, 'learning_rate': 5.0172839506172844e-05, 'epoch': 2.25}
{'loss': 2.43, 'grad_norm': 5.809089660644531, 'learning_rate': 5.011111111111111e-05, 'epoch': 2.25}
{'loss': 3.1253, 'grad_norm': 7.878067970275879, 'learning_rate': 5.0049382716049385e-05, 'epoch': 2.25}
{'loss': 2.7636, 'grad_norm': 5.85589075088501, 'learning_rate': 4.998765432098766e-05, 'epoch': 2.25}
{'loss': 3.4726, 'grad_norm': 8.69910717010498, 'learning_rate': 4.9925925925925926e-05, 'epoch': 2.25}
{'loss': 2.8897, 'grad_norm': 8.128931999206543, 'learning_rate': 4.98641975308642e-05, 'epoch': 2.25}
{'loss': 2.8257, 'grad_norm': 4.604623794555664, 'learning_rate': 4.980246913580247e-05, 'epoch': 2.25}
{'loss': 2.6094, 'grad_norm': 8.786492347717285, 'learning_rate': 4.974074074074074e-05, 'epoch': 2.26}
{'loss': 3.1625, 'grad_norm': 8.081217765808105, 'learning_rate': 4.9679012345679014e-05, 'epoch': 2.26}
{'loss': 3.4168, 'grad_norm': 4.695072650909424, 'learning_rate': 4.961728395061729e-05, 'epoch': 2.26}
{'loss': 2.9777, 'grad_norm': 11.51052188873291, 'learning_rate': 4.955555555555556e-05, 'epoch': 2.26}
{'loss': 3.0993, 'grad_norm': 5.461165904998779, 'learning_rate': 4.949382716049383e-05, 'epoch': 2.26}
{'loss': 2.6794, 'grad_norm': 5.7435102462768555, 'learning_rate': 4.94320987654321e-05, 'epoch': 2.26}
{'loss': 3.0998, 'grad_norm': 5.756800174713135, 'learning_rate': 4.937037037037037e-05, 'epoch': 2.26}
{'loss': 2.5826, 'grad_norm': 11.58034896850586, 'learning_rate': 4.930864197530864e-05, 'epoch': 2.26}
{'loss': 3.0682, 'grad_norm': 4.045400142669678, 'learning_rate': 4.924691358024692e-05, 'epoch': 2.26}
{'loss': 2.9888, 'grad_norm': 3.961854934692383, 'learning_rate': 4.918518518518519e-05, 'epoch': 2.26}
{'loss': 2.5322, 'grad_norm': 4.00358247756958, 'learning_rate': 4.912345679012346e-05, 'epoch': 2.26}
{'loss': 2.8699, 'grad_norm': 7.171113967895508, 'learning_rate': 4.906172839506173e-05, 'epoch': 2.27}
{'loss': 2.7157, 'grad_norm': 6.539085865020752, 'learning_rate': 4.9e-05, 'epoch': 2.27}
{'loss': 2.3858, 'grad_norm': 5.804222583770752, 'learning_rate': 4.893827160493827e-05, 'epoch': 2.27}
{'loss': 3.6861, 'grad_norm': 5.091920852661133, 'learning_rate': 4.8876543209876546e-05, 'epoch': 2.27}
{'loss': 2.6367, 'grad_norm': 5.0628275871276855, 'learning_rate': 4.881481481481482e-05, 'epoch': 2.27}
{'loss': 3.5053, 'grad_norm': 8.266775131225586, 'learning_rate': 4.8753086419753094e-05, 'epoch': 2.27}
{'loss': 2.4957, 'grad_norm': 7.679712772369385, 'learning_rate': 4.869135802469136e-05, 'epoch': 2.27}
{'loss': 2.6871, 'grad_norm': 6.535408020019531, 'learning_rate': 4.862962962962963e-05, 'epoch': 2.27}
{'loss': 2.5792, 'grad_norm': 5.562045097351074, 'learning_rate': 4.85679012345679e-05, 'epoch': 2.27}
{'loss': 2.4125, 'grad_norm': 4.264439105987549, 'learning_rate': 4.8506172839506175e-05, 'epoch': 2.27}
{'loss': 3.1133, 'grad_norm': 9.357213020324707, 'learning_rate': 4.844444444444445e-05, 'epoch': 2.27}
{'loss': 3.4315, 'grad_norm': 6.7997636795043945, 'learning_rate': 4.838271604938272e-05, 'epoch': 2.28}
{'loss': 2.4705, 'grad_norm': 7.525460720062256, 'learning_rate': 4.832098765432099e-05, 'epoch': 2.28}
{'loss': 3.1523, 'grad_norm': 5.126056671142578, 'learning_rate': 4.825925925925926e-05, 'epoch': 2.28}
{'loss': 3.0994, 'grad_norm': 7.391665458679199, 'learning_rate': 4.819753086419753e-05, 'epoch': 2.28}
{'loss': 3.0103, 'grad_norm': 5.256948947906494, 'learning_rate': 4.8135802469135804e-05, 'epoch': 2.28}
{'loss': 2.827, 'grad_norm': 3.936760425567627, 'learning_rate': 4.807407407407408e-05, 'epoch': 2.28}
{'loss': 2.4974, 'grad_norm': 7.7176337242126465, 'learning_rate': 4.801234567901235e-05, 'epoch': 2.28}
{'loss': 3.2593, 'grad_norm': 6.059706211090088, 'learning_rate': 4.795061728395062e-05, 'epoch': 2.28}
{'loss': 3.1305, 'grad_norm': 6.0238356590271, 'learning_rate': 4.7888888888888886e-05, 'epoch': 2.28}
{'loss': 2.6208, 'grad_norm': 6.112025737762451, 'learning_rate': 4.782716049382716e-05, 'epoch': 2.28}
{'loss': 2.8629, 'grad_norm': 4.700009822845459, 'learning_rate': 4.7765432098765433e-05, 'epoch': 2.29}
{'loss': 2.4024, 'grad_norm': 8.132534980773926, 'learning_rate': 4.770370370370371e-05, 'epoch': 2.29}
{'loss': 2.3139, 'grad_norm': 5.298452854156494, 'learning_rate': 4.764197530864198e-05, 'epoch': 2.29}
{'loss': 2.5416, 'grad_norm': 3.8866944313049316, 'learning_rate': 4.758024691358025e-05, 'epoch': 2.29}
{'loss': 2.3508, 'grad_norm': 4.381477355957031, 'learning_rate': 4.751851851851852e-05, 'epoch': 2.29}
{'loss': 2.588, 'grad_norm': 5.512454032897949, 'learning_rate': 4.745679012345679e-05, 'epoch': 2.29}
{'loss': 2.8123, 'grad_norm': 5.036257743835449, 'learning_rate': 4.739506172839506e-05, 'epoch': 2.29}
{'loss': 2.95, 'grad_norm': 5.742496013641357, 'learning_rate': 4.7333333333333336e-05, 'epoch': 2.29}
{'loss': 2.6762, 'grad_norm': 11.273347854614258, 'learning_rate': 4.727160493827161e-05, 'epoch': 2.29}
{'loss': 2.7988, 'grad_norm': 5.298511981964111, 'learning_rate': 4.720987654320988e-05, 'epoch': 2.29}
{'loss': 2.3955, 'grad_norm': 11.663622856140137, 'learning_rate': 4.714814814814815e-05, 'epoch': 2.29}
{'loss': 2.765, 'grad_norm': 5.494909286499023, 'learning_rate': 4.708641975308642e-05, 'epoch': 2.3}
{'loss': 2.8579, 'grad_norm': 3.279987335205078, 'learning_rate': 4.702469135802469e-05, 'epoch': 2.3}
{'loss': 2.5665, 'grad_norm': 15.98495101928711, 'learning_rate': 4.6962962962962966e-05, 'epoch': 2.3}
{'loss': 2.505, 'grad_norm': 4.45973014831543, 'learning_rate': 4.690123456790124e-05, 'epoch': 2.3}
{'loss': 2.7519, 'grad_norm': 6.126152515411377, 'learning_rate': 4.6839506172839506e-05, 'epoch': 2.3}
{'loss': 2.4532, 'grad_norm': 3.513498067855835, 'learning_rate': 4.677777777777778e-05, 'epoch': 2.3}
{'loss': 2.5463, 'grad_norm': 5.721611499786377, 'learning_rate': 4.6716049382716054e-05, 'epoch': 2.3}
{'loss': 2.7714, 'grad_norm': 4.228446960449219, 'learning_rate': 4.665432098765432e-05, 'epoch': 2.3}
{'loss': 3.2752, 'grad_norm': 8.909096717834473, 'learning_rate': 4.6592592592592595e-05, 'epoch': 2.3}
{'loss': 3.3726, 'grad_norm': 8.353696823120117, 'learning_rate': 4.653086419753087e-05, 'epoch': 2.3}
{'loss': 2.7395, 'grad_norm': 8.560708999633789, 'learning_rate': 4.6469135802469136e-05, 'epoch': 2.3}
{'loss': 2.4978, 'grad_norm': 4.479576587677002, 'learning_rate': 4.640740740740741e-05, 'epoch': 2.31}
{'loss': 2.8586, 'grad_norm': 10.419736862182617, 'learning_rate': 4.634567901234568e-05, 'epoch': 2.31}
{'loss': 2.6655, 'grad_norm': 4.75848913192749, 'learning_rate': 4.628395061728396e-05, 'epoch': 2.31}
{'loss': 2.8364, 'grad_norm': 6.832077980041504, 'learning_rate': 4.6222222222222224e-05, 'epoch': 2.31}
{'loss': 2.2892, 'grad_norm': 7.760917663574219, 'learning_rate': 4.61604938271605e-05, 'epoch': 2.31}
{'loss': 3.2628, 'grad_norm': 5.341071128845215, 'learning_rate': 4.6098765432098765e-05, 'epoch': 2.31}
{'loss': 2.6949, 'grad_norm': 4.918601036071777, 'learning_rate': 4.603703703703704e-05, 'epoch': 2.31}
{'loss': 2.3755, 'grad_norm': 10.33640193939209, 'learning_rate': 4.597530864197531e-05, 'epoch': 2.31}
{'loss': 3.0669, 'grad_norm': 4.18122673034668, 'learning_rate': 4.5913580246913586e-05, 'epoch': 2.31}
{'loss': 2.8085, 'grad_norm': 4.464283466339111, 'learning_rate': 4.585185185185185e-05, 'epoch': 2.31}
{'loss': 3.0071, 'grad_norm': 4.480714321136475, 'learning_rate': 4.579012345679013e-05, 'epoch': 2.31}
  warnings.warn(                                                                                      
{'eval_loss': 2.6941754817962646, 'eval_runtime': 413.1545, 'eval_samples_per_second': 1.452, 'eval_steps_per_second': 1.452, 'epoch': 2.31}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.63, 'grad_norm': 6.073531150817871, 'learning_rate': 4.5728395061728394e-05, 'epoch': 2.32}
{'loss': 2.2655, 'grad_norm': 6.647103786468506, 'learning_rate': 4.566666666666667e-05, 'epoch': 2.32}
{'loss': 2.3924, 'grad_norm': 4.879457950592041, 'learning_rate': 4.560493827160494e-05, 'epoch': 2.32}
{'loss': 3.5284, 'grad_norm': 6.0418829917907715, 'learning_rate': 4.5543209876543215e-05, 'epoch': 2.32}
{'loss': 2.3229, 'grad_norm': 5.7810750007629395, 'learning_rate': 4.548148148148149e-05, 'epoch': 2.32}
{'loss': 3.3757, 'grad_norm': 4.817550182342529, 'learning_rate': 4.5419753086419756e-05, 'epoch': 2.32}
{'loss': 2.4692, 'grad_norm': 10.169139862060547, 'learning_rate': 4.535802469135802e-05, 'epoch': 2.32}
{'loss': 2.1718, 'grad_norm': 5.176560401916504, 'learning_rate': 4.52962962962963e-05, 'epoch': 2.32}
{'loss': 2.5169, 'grad_norm': 6.492581844329834, 'learning_rate': 4.523456790123457e-05, 'epoch': 2.32}
{'loss': 2.9257, 'grad_norm': 7.745481967926025, 'learning_rate': 4.5172839506172844e-05, 'epoch': 2.32}
{'loss': 2.8175, 'grad_norm': 5.406307697296143, 'learning_rate': 4.511111111111112e-05, 'epoch': 2.33}
{'loss': 2.9231, 'grad_norm': 6.079051971435547, 'learning_rate': 4.5049382716049385e-05, 'epoch': 2.33}
{'loss': 2.6005, 'grad_norm': 5.104325294494629, 'learning_rate': 4.498765432098765e-05, 'epoch': 2.33}
{'loss': 2.8911, 'grad_norm': 5.034546852111816, 'learning_rate': 4.4925925925925926e-05, 'epoch': 2.33}
{'loss': 2.6729, 'grad_norm': 5.481729984283447, 'learning_rate': 4.48641975308642e-05, 'epoch': 2.33}
{'loss': 2.3146, 'grad_norm': 8.342754364013672, 'learning_rate': 4.4802469135802474e-05, 'epoch': 2.33}
{'loss': 2.4889, 'grad_norm': 7.516028881072998, 'learning_rate': 4.474074074074075e-05, 'epoch': 2.33}
{'loss': 2.5252, 'grad_norm': 4.579501152038574, 'learning_rate': 4.4679012345679014e-05, 'epoch': 2.33}
{'loss': 2.9319, 'grad_norm': 8.336627960205078, 'learning_rate': 4.461728395061728e-05, 'epoch': 2.33}
{'loss': 2.4742, 'grad_norm': 6.378483772277832, 'learning_rate': 4.4555555555555555e-05, 'epoch': 2.33}
{'loss': 2.5256, 'grad_norm': 8.582880020141602, 'learning_rate': 4.449382716049383e-05, 'epoch': 2.33}
{'loss': 2.833, 'grad_norm': 4.22092866897583, 'learning_rate': 4.44320987654321e-05, 'epoch': 2.34}
{'loss': 2.8957, 'grad_norm': 5.78746223449707, 'learning_rate': 4.4370370370370376e-05, 'epoch': 2.34}
{'loss': 3.2424, 'grad_norm': 5.874680995941162, 'learning_rate': 4.4308641975308643e-05, 'epoch': 2.34}
{'loss': 2.347, 'grad_norm': 4.547984600067139, 'learning_rate': 4.424691358024692e-05, 'epoch': 2.34}
{'loss': 2.0504, 'grad_norm': 5.167910575866699, 'learning_rate': 4.4185185185185184e-05, 'epoch': 2.34}
{'loss': 2.7016, 'grad_norm': 4.838475227355957, 'learning_rate': 4.412345679012346e-05, 'epoch': 2.34}
{'loss': 2.6318, 'grad_norm': 5.862342357635498, 'learning_rate': 4.406172839506173e-05, 'epoch': 2.34}
{'loss': 3.1426, 'grad_norm': 7.216671943664551, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.34}
{'loss': 2.4243, 'grad_norm': 5.106966018676758, 'learning_rate': 4.393827160493827e-05, 'epoch': 2.34}
{'loss': 2.1739, 'grad_norm': 3.532180070877075, 'learning_rate': 4.3876543209876546e-05, 'epoch': 2.34}
{'loss': 2.2552, 'grad_norm': 5.44830322265625, 'learning_rate': 4.381481481481482e-05, 'epoch': 2.34}
{'loss': 2.4893, 'grad_norm': 4.894577980041504, 'learning_rate': 4.375308641975309e-05, 'epoch': 2.35}
{'loss': 3.0426, 'grad_norm': 5.036734104156494, 'learning_rate': 4.369135802469136e-05, 'epoch': 2.35}
{'loss': 2.8743, 'grad_norm': 5.213240623474121, 'learning_rate': 4.3629629629629635e-05, 'epoch': 2.35}
{'loss': 2.5452, 'grad_norm': 3.964447259902954, 'learning_rate': 4.35679012345679e-05, 'epoch': 2.35}
{'loss': 2.4814, 'grad_norm': 4.341341972351074, 'learning_rate': 4.3506172839506176e-05, 'epoch': 2.35}
{'loss': 2.7509, 'grad_norm': 3.8858351707458496, 'learning_rate': 4.344444444444445e-05, 'epoch': 2.35}
{'loss': 2.8123, 'grad_norm': 12.35608959197998, 'learning_rate': 4.3382716049382716e-05, 'epoch': 2.35}
{'loss': 2.2298, 'grad_norm': 4.512179374694824, 'learning_rate': 4.332098765432099e-05, 'epoch': 2.35}
{'loss': 3.0644, 'grad_norm': 7.5640482902526855, 'learning_rate': 4.325925925925926e-05, 'epoch': 2.35}
{'loss': 3.222, 'grad_norm': 9.27399730682373, 'learning_rate': 4.319753086419753e-05, 'epoch': 2.35}
{'loss': 2.7229, 'grad_norm': 4.045044898986816, 'learning_rate': 4.3135802469135805e-05, 'epoch': 2.35}
{'loss': 3.1171, 'grad_norm': 6.332042694091797, 'learning_rate': 4.307407407407408e-05, 'epoch': 2.36}
{'loss': 3.6414, 'grad_norm': 8.192663192749023, 'learning_rate': 4.301234567901235e-05, 'epoch': 2.36}
{'loss': 2.0382, 'grad_norm': 5.151056289672852, 'learning_rate': 4.295061728395062e-05, 'epoch': 2.36}
{'loss': 2.1714, 'grad_norm': 4.237442493438721, 'learning_rate': 4.2888888888888886e-05, 'epoch': 2.36}
{'loss': 2.1129, 'grad_norm': 5.761255741119385, 'learning_rate': 4.282716049382716e-05, 'epoch': 2.36}
{'loss': 2.4182, 'grad_norm': 5.091663837432861, 'learning_rate': 4.2765432098765434e-05, 'epoch': 2.36}
{'loss': 2.8467, 'grad_norm': 4.803737163543701, 'learning_rate': 4.270370370370371e-05, 'epoch': 2.36}
{'loss': 2.3669, 'grad_norm': 6.155822277069092, 'learning_rate': 4.264197530864198e-05, 'epoch': 2.36}
{'loss': 2.6039, 'grad_norm': 5.632507801055908, 'learning_rate': 4.258024691358025e-05, 'epoch': 2.36}
{'loss': 2.7915, 'grad_norm': 8.066665649414062, 'learning_rate': 4.2518518518518515e-05, 'epoch': 2.36}
{'loss': 2.2958, 'grad_norm': 5.908178806304932, 'learning_rate': 4.245679012345679e-05, 'epoch': 2.36}
{'loss': 2.7685, 'grad_norm': 7.911006450653076, 'learning_rate': 4.239506172839506e-05, 'epoch': 2.37}
{'loss': 2.4589, 'grad_norm': 4.311945915222168, 'learning_rate': 4.233333333333334e-05, 'epoch': 2.37}
{'loss': 2.9559, 'grad_norm': 7.42835807800293, 'learning_rate': 4.227160493827161e-05, 'epoch': 2.37}
{'loss': 2.1091, 'grad_norm': 5.455525875091553, 'learning_rate': 4.220987654320988e-05, 'epoch': 2.37}
{'loss': 2.5426, 'grad_norm': 6.07620906829834, 'learning_rate': 4.2148148148148145e-05, 'epoch': 2.37}
{'loss': 2.962, 'grad_norm': 5.427661418914795, 'learning_rate': 4.208641975308642e-05, 'epoch': 2.37}
{'loss': 2.855, 'grad_norm': 6.220632076263428, 'learning_rate': 4.202469135802469e-05, 'epoch': 2.37}
{'loss': 2.6789, 'grad_norm': 4.932268142700195, 'learning_rate': 4.1962962962962966e-05, 'epoch': 2.37}
{'loss': 3.3543, 'grad_norm': 9.359521865844727, 'learning_rate': 4.190123456790124e-05, 'epoch': 2.37}
{'loss': 2.2864, 'grad_norm': 7.352468013763428, 'learning_rate': 4.183950617283951e-05, 'epoch': 2.37}
{'loss': 2.4919, 'grad_norm': 4.855428695678711, 'learning_rate': 4.177777777777778e-05, 'epoch': 2.38}
{'loss': 2.444, 'grad_norm': 6.006441593170166, 'learning_rate': 4.171604938271605e-05, 'epoch': 2.38}
{'loss': 3.1512, 'grad_norm': 4.232474327087402, 'learning_rate': 4.165432098765432e-05, 'epoch': 2.38}
{'loss': 3.3195, 'grad_norm': 9.829546928405762, 'learning_rate': 4.1592592592592595e-05, 'epoch': 2.38}
{'loss': 3.0201, 'grad_norm': 7.531822681427002, 'learning_rate': 4.153086419753087e-05, 'epoch': 2.38}
{'loss': 2.6375, 'grad_norm': 6.2444281578063965, 'learning_rate': 4.1469135802469136e-05, 'epoch': 2.38}
{'loss': 2.3682, 'grad_norm': 10.601737022399902, 'learning_rate': 4.140740740740741e-05, 'epoch': 2.38}
{'loss': 3.1376, 'grad_norm': 64.77815246582031, 'learning_rate': 4.134567901234568e-05, 'epoch': 2.38}
{'loss': 3.2433, 'grad_norm': 8.00708293914795, 'learning_rate': 4.128395061728395e-05, 'epoch': 2.38}
{'loss': 2.7244, 'grad_norm': 6.685473918914795, 'learning_rate': 4.1222222222222224e-05, 'epoch': 2.38}
{'loss': 2.7681, 'grad_norm': 5.359119415283203, 'learning_rate': 4.11604938271605e-05, 'epoch': 2.38}
{'loss': 2.3194, 'grad_norm': 4.524722099304199, 'learning_rate': 4.1098765432098765e-05, 'epoch': 2.39}
{'loss': 2.6705, 'grad_norm': 6.261246681213379, 'learning_rate': 4.103703703703704e-05, 'epoch': 2.39}
{'loss': 2.4108, 'grad_norm': 5.17302131652832, 'learning_rate': 4.097530864197531e-05, 'epoch': 2.39}
{'loss': 2.3721, 'grad_norm': 4.559926509857178, 'learning_rate': 4.091358024691358e-05, 'epoch': 2.39}
{'loss': 2.4943, 'grad_norm': 4.329777240753174, 'learning_rate': 4.0851851851851853e-05, 'epoch': 2.39}
{'loss': 2.1686, 'grad_norm': 6.200409412384033, 'learning_rate': 4.079012345679013e-05, 'epoch': 2.39}
{'loss': 2.9962, 'grad_norm': 4.228999614715576, 'learning_rate': 4.0728395061728394e-05, 'epoch': 2.39}
{'loss': 2.4369, 'grad_norm': 5.466275691986084, 'learning_rate': 4.066666666666667e-05, 'epoch': 2.39}
{'loss': 2.6139, 'grad_norm': 7.2718048095703125, 'learning_rate': 4.060493827160494e-05, 'epoch': 2.39}
{'loss': 2.7266, 'grad_norm': 7.477381229400635, 'learning_rate': 4.0543209876543216e-05, 'epoch': 2.39}
{'loss': 3.0276, 'grad_norm': 3.798130989074707, 'learning_rate': 4.048148148148148e-05, 'epoch': 2.39}
{'loss': 2.485, 'grad_norm': 3.312847852706909, 'learning_rate': 4.0419753086419756e-05, 'epoch': 2.4}
{'loss': 2.3343, 'grad_norm': 4.82284688949585, 'learning_rate': 4.0358024691358023e-05, 'epoch': 2.4}
{'loss': 3.2773, 'grad_norm': 7.784284591674805, 'learning_rate': 4.02962962962963e-05, 'epoch': 2.4}
{'loss': 2.5459, 'grad_norm': 6.019309043884277, 'learning_rate': 4.023456790123457e-05, 'epoch': 2.4}
{'loss': 2.7195, 'grad_norm': 6.541524410247803, 'learning_rate': 4.0172839506172845e-05, 'epoch': 2.4}
{'loss': 2.5746, 'grad_norm': 4.611537933349609, 'learning_rate': 4.011111111111111e-05, 'epoch': 2.4}
{'loss': 2.2299, 'grad_norm': 5.283024787902832, 'learning_rate': 4.0049382716049386e-05, 'epoch': 2.4}
{'loss': 3.2677, 'grad_norm': 6.562917232513428, 'learning_rate': 3.998765432098765e-05, 'epoch': 2.4}
{'loss': 2.7737, 'grad_norm': 5.101728916168213, 'learning_rate': 3.9925925925925926e-05, 'epoch': 2.4}
{'loss': 2.5687, 'grad_norm': 4.554786205291748, 'learning_rate': 3.98641975308642e-05, 'epoch': 2.4}
{'loss': 3.5067, 'grad_norm': 8.715919494628906, 'learning_rate': 3.9802469135802474e-05, 'epoch': 2.4}
{'loss': 2.6098, 'grad_norm': 3.7608132362365723, 'learning_rate': 3.974074074074075e-05, 'epoch': 2.41}
{'loss': 2.6568, 'grad_norm': 4.706800937652588, 'learning_rate': 3.9679012345679015e-05, 'epoch': 2.41}
{'loss': 2.5787, 'grad_norm': 5.339761734008789, 'learning_rate': 3.961728395061728e-05, 'epoch': 2.41}
  warnings.warn(                                                                                      
{'eval_loss': 2.702946662902832, 'eval_runtime': 412.9312, 'eval_samples_per_second': 1.453, 'eval_steps_per_second': 1.453, 'epoch': 2.41}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 3.0702, 'grad_norm': 4.995443820953369, 'learning_rate': 3.9555555555555556e-05, 'epoch': 2.41}
{'loss': 2.7428, 'grad_norm': 5.234138488769531, 'learning_rate': 3.949382716049383e-05, 'epoch': 2.41}
{'loss': 3.0857, 'grad_norm': 6.411479473114014, 'learning_rate': 3.94320987654321e-05, 'epoch': 2.41}
{'loss': 3.0518, 'grad_norm': 7.451663017272949, 'learning_rate': 3.937037037037038e-05, 'epoch': 2.41}
{'loss': 3.5391, 'grad_norm': 14.048367500305176, 'learning_rate': 3.9308641975308644e-05, 'epoch': 2.41}
{'loss': 2.0972, 'grad_norm': 5.8986663818359375, 'learning_rate': 3.924691358024691e-05, 'epoch': 2.41}
{'loss': 2.6806, 'grad_norm': 7.489979267120361, 'learning_rate': 3.9185185185185185e-05, 'epoch': 2.41}
{'loss': 3.1668, 'grad_norm': 5.920039653778076, 'learning_rate': 3.912345679012346e-05, 'epoch': 2.41}
{'loss': 3.1532, 'grad_norm': 15.997166633605957, 'learning_rate': 3.906172839506173e-05, 'epoch': 2.42}
{'loss': 2.9308, 'grad_norm': 5.89834451675415, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.42}
{'loss': 2.8652, 'grad_norm': 4.785416603088379, 'learning_rate': 3.893827160493827e-05, 'epoch': 2.42}
{'loss': 2.6435, 'grad_norm': 6.411830425262451, 'learning_rate': 3.887654320987654e-05, 'epoch': 2.42}
{'loss': 2.6897, 'grad_norm': 5.9256696701049805, 'learning_rate': 3.8814814814814814e-05, 'epoch': 2.42}
{'loss': 2.971, 'grad_norm': 4.836452484130859, 'learning_rate': 3.875308641975309e-05, 'epoch': 2.42}
{'loss': 2.7466, 'grad_norm': 10.447210311889648, 'learning_rate': 3.869135802469136e-05, 'epoch': 2.42}
{'loss': 2.6407, 'grad_norm': 5.883202075958252, 'learning_rate': 3.8629629629629635e-05, 'epoch': 2.42}
{'loss': 2.9546, 'grad_norm': 4.736559867858887, 'learning_rate': 3.85679012345679e-05, 'epoch': 2.42}
{'loss': 2.4939, 'grad_norm': 4.998514652252197, 'learning_rate': 3.8506172839506176e-05, 'epoch': 2.42}
{'loss': 2.7888, 'grad_norm': 4.768002033233643, 'learning_rate': 3.844444444444444e-05, 'epoch': 2.42}
{'loss': 2.9982, 'grad_norm': 4.335008144378662, 'learning_rate': 3.838271604938272e-05, 'epoch': 2.43}
{'loss': 2.6491, 'grad_norm': 6.413266181945801, 'learning_rate': 3.832098765432099e-05, 'epoch': 2.43}
{'loss': 2.8183, 'grad_norm': 5.09312105178833, 'learning_rate': 3.8259259259259264e-05, 'epoch': 2.43}
{'loss': 2.7067, 'grad_norm': 6.913061618804932, 'learning_rate': 3.819753086419753e-05, 'epoch': 2.43}
{'loss': 2.8649, 'grad_norm': 4.265130043029785, 'learning_rate': 3.8135802469135805e-05, 'epoch': 2.43}
{'loss': 2.7584, 'grad_norm': 7.8733391761779785, 'learning_rate': 3.807407407407408e-05, 'epoch': 2.43}
{'loss': 2.9674, 'grad_norm': 4.532275676727295, 'learning_rate': 3.8012345679012346e-05, 'epoch': 2.43}
{'loss': 2.942, 'grad_norm': 7.913593292236328, 'learning_rate': 3.795061728395062e-05, 'epoch': 2.43}
{'loss': 2.9767, 'grad_norm': 6.194107532501221, 'learning_rate': 3.7888888888888894e-05, 'epoch': 2.43}
{'loss': 2.6027, 'grad_norm': 6.524878025054932, 'learning_rate': 3.782716049382716e-05, 'epoch': 2.43}
{'loss': 2.6654, 'grad_norm': 8.576889991760254, 'learning_rate': 3.7765432098765434e-05, 'epoch': 2.44}
{'loss': 2.2872, 'grad_norm': 4.919229507446289, 'learning_rate': 3.770370370370371e-05, 'epoch': 2.44}
{'loss': 2.7711, 'grad_norm': 5.78886604309082, 'learning_rate': 3.7641975308641975e-05, 'epoch': 2.44}
{'loss': 3.1829, 'grad_norm': 3.9367568492889404, 'learning_rate': 3.758024691358025e-05, 'epoch': 2.44}
{'loss': 2.9526, 'grad_norm': 8.8059720993042, 'learning_rate': 3.751851851851852e-05, 'epoch': 2.44}
{'loss': 3.3881, 'grad_norm': 4.812808513641357, 'learning_rate': 3.745679012345679e-05, 'epoch': 2.44}
{'loss': 2.5631, 'grad_norm': 5.500405788421631, 'learning_rate': 3.7395061728395064e-05, 'epoch': 2.44}
{'loss': 2.7022, 'grad_norm': 6.421433448791504, 'learning_rate': 3.733333333333334e-05, 'epoch': 2.44}
{'loss': 2.0954, 'grad_norm': 7.1149115562438965, 'learning_rate': 3.727160493827161e-05, 'epoch': 2.44}
{'loss': 2.5892, 'grad_norm': 7.235933303833008, 'learning_rate': 3.720987654320988e-05, 'epoch': 2.44}
{'loss': 2.888, 'grad_norm': 6.5911383628845215, 'learning_rate': 3.714814814814815e-05, 'epoch': 2.44}
{'loss': 2.4522, 'grad_norm': 4.791273593902588, 'learning_rate': 3.708641975308642e-05, 'epoch': 2.45}
{'loss': 2.9801, 'grad_norm': 6.3537917137146, 'learning_rate': 3.702469135802469e-05, 'epoch': 2.45}
{'loss': 2.7932, 'grad_norm': 6.06898307800293, 'learning_rate': 3.6962962962962966e-05, 'epoch': 2.45}
{'loss': 2.6529, 'grad_norm': 11.587882995605469, 'learning_rate': 3.690123456790124e-05, 'epoch': 2.45}
{'loss': 3.4782, 'grad_norm': 5.714900493621826, 'learning_rate': 3.683950617283951e-05, 'epoch': 2.45}
{'loss': 2.4268, 'grad_norm': 4.063136577606201, 'learning_rate': 3.677777777777778e-05, 'epoch': 2.45}
{'loss': 2.5756, 'grad_norm': 5.249583721160889, 'learning_rate': 3.671604938271605e-05, 'epoch': 2.45}
{'loss': 2.9924, 'grad_norm': 5.2772135734558105, 'learning_rate': 3.665432098765432e-05, 'epoch': 2.45}
{'loss': 2.9446, 'grad_norm': 9.065515518188477, 'learning_rate': 3.6592592592592596e-05, 'epoch': 2.45}
{'loss': 2.8936, 'grad_norm': 6.996504306793213, 'learning_rate': 3.653086419753087e-05, 'epoch': 2.45}
{'loss': 3.2853, 'grad_norm': 5.755354881286621, 'learning_rate': 3.646913580246914e-05, 'epoch': 2.45}
{'loss': 3.3542, 'grad_norm': 8.03757381439209, 'learning_rate': 3.6407407407407403e-05, 'epoch': 2.46}
{'loss': 2.2878, 'grad_norm': 4.408076286315918, 'learning_rate': 3.634567901234568e-05, 'epoch': 2.46}
{'loss': 3.1205, 'grad_norm': 5.6577653884887695, 'learning_rate': 3.628395061728395e-05, 'epoch': 2.46}
{'loss': 2.7623, 'grad_norm': 4.706141948699951, 'learning_rate': 3.6222222222222225e-05, 'epoch': 2.46}
{'loss': 2.8379, 'grad_norm': 7.015376567840576, 'learning_rate': 3.61604938271605e-05, 'epoch': 2.46}
{'loss': 2.1898, 'grad_norm': 5.150890350341797, 'learning_rate': 3.6098765432098766e-05, 'epoch': 2.46}
{'loss': 2.9393, 'grad_norm': 5.371715545654297, 'learning_rate': 3.603703703703704e-05, 'epoch': 2.46}
{'loss': 2.8879, 'grad_norm': 6.031945705413818, 'learning_rate': 3.5975308641975306e-05, 'epoch': 2.46}
{'loss': 2.5841, 'grad_norm': 5.728742599487305, 'learning_rate': 3.591358024691358e-05, 'epoch': 2.46}
{'loss': 2.881, 'grad_norm': 5.165940761566162, 'learning_rate': 3.5851851851851854e-05, 'epoch': 2.46}
{'loss': 2.9758, 'grad_norm': 6.51519775390625, 'learning_rate': 3.579012345679013e-05, 'epoch': 2.46}
{'loss': 2.7605, 'grad_norm': 10.31019115447998, 'learning_rate': 3.5728395061728395e-05, 'epoch': 2.47}
{'loss': 3.1371, 'grad_norm': 11.216300010681152, 'learning_rate': 3.566666666666667e-05, 'epoch': 2.47}
{'loss': 2.9148, 'grad_norm': 8.499187469482422, 'learning_rate': 3.5604938271604936e-05, 'epoch': 2.47}
{'loss': 2.659, 'grad_norm': 5.329583644866943, 'learning_rate': 3.554320987654321e-05, 'epoch': 2.47}
{'loss': 3.7026, 'grad_norm': 6.87369966506958, 'learning_rate': 3.548148148148148e-05, 'epoch': 2.47}
{'loss': 3.6167, 'grad_norm': 8.428289413452148, 'learning_rate': 3.541975308641976e-05, 'epoch': 2.47}
{'loss': 2.819, 'grad_norm': 7.001640319824219, 'learning_rate': 3.5358024691358024e-05, 'epoch': 2.47}
{'loss': 2.5231, 'grad_norm': 4.170083999633789, 'learning_rate': 3.52962962962963e-05, 'epoch': 2.47}
{'loss': 2.7425, 'grad_norm': 6.976241588592529, 'learning_rate': 3.523456790123457e-05, 'epoch': 2.47}
{'loss': 2.9435, 'grad_norm': 6.890292167663574, 'learning_rate': 3.517283950617284e-05, 'epoch': 2.47}
{'loss': 2.7984, 'grad_norm': 7.2801690101623535, 'learning_rate': 3.511111111111111e-05, 'epoch': 2.48}
{'loss': 2.7063, 'grad_norm': 5.951628684997559, 'learning_rate': 3.5049382716049386e-05, 'epoch': 2.48}
{'loss': 2.4416, 'grad_norm': 4.042957782745361, 'learning_rate': 3.498765432098765e-05, 'epoch': 2.48}
{'loss': 2.6979, 'grad_norm': 7.106197834014893, 'learning_rate': 3.492592592592593e-05, 'epoch': 2.48}
{'loss': 3.1172, 'grad_norm': 4.919058322906494, 'learning_rate': 3.48641975308642e-05, 'epoch': 2.48}
{'loss': 2.6097, 'grad_norm': 6.24229621887207, 'learning_rate': 3.4802469135802474e-05, 'epoch': 2.48}
{'loss': 3.1295, 'grad_norm': 7.62973165512085, 'learning_rate': 3.474074074074074e-05, 'epoch': 2.48}
{'loss': 3.2444, 'grad_norm': 18.474138259887695, 'learning_rate': 3.4679012345679015e-05, 'epoch': 2.48}
{'loss': 3.4329, 'grad_norm': 5.767014503479004, 'learning_rate': 3.461728395061728e-05, 'epoch': 2.48}
{'loss': 2.7357, 'grad_norm': 6.501822471618652, 'learning_rate': 3.4555555555555556e-05, 'epoch': 2.48}
{'loss': 3.0367, 'grad_norm': 10.253414154052734, 'learning_rate': 3.449382716049383e-05, 'epoch': 2.48}
{'loss': 2.6192, 'grad_norm': 6.17183256149292, 'learning_rate': 3.4432098765432104e-05, 'epoch': 2.49}
{'loss': 2.839, 'grad_norm': 5.522701263427734, 'learning_rate': 3.437037037037037e-05, 'epoch': 2.49}
{'loss': 3.0831, 'grad_norm': 5.196125507354736, 'learning_rate': 3.4308641975308644e-05, 'epoch': 2.49}
{'loss': 2.5633, 'grad_norm': 6.095915794372559, 'learning_rate': 3.424691358024691e-05, 'epoch': 2.49}
{'loss': 2.1497, 'grad_norm': 4.643712520599365, 'learning_rate': 3.4185185185185185e-05, 'epoch': 2.49}
{'loss': 2.6776, 'grad_norm': 7.950479984283447, 'learning_rate': 3.412345679012346e-05, 'epoch': 2.49}
{'loss': 2.738, 'grad_norm': 5.958986759185791, 'learning_rate': 3.406172839506173e-05, 'epoch': 2.49}
{'loss': 2.9111, 'grad_norm': 6.169843673706055, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.49}
{'loss': 2.8831, 'grad_norm': 5.671764373779297, 'learning_rate': 3.3938271604938274e-05, 'epoch': 2.49}
{'loss': 2.9632, 'grad_norm': 4.616458415985107, 'learning_rate': 3.387654320987654e-05, 'epoch': 2.49}
{'loss': 3.1762, 'grad_norm': 5.293817043304443, 'learning_rate': 3.3814814814814814e-05, 'epoch': 2.49}
{'loss': 2.7535, 'grad_norm': 6.010622024536133, 'learning_rate': 3.375308641975309e-05, 'epoch': 2.5}
{'loss': 3.138, 'grad_norm': 5.126764297485352, 'learning_rate': 3.369135802469136e-05, 'epoch': 2.5}
{'loss': 2.9493, 'grad_norm': 7.639876365661621, 'learning_rate': 3.3629629629629636e-05, 'epoch': 2.5}
{'loss': 2.5196, 'grad_norm': 4.8970513343811035, 'learning_rate': 3.35679012345679e-05, 'epoch': 2.5}
{'loss': 2.9239, 'grad_norm': 4.975774765014648, 'learning_rate': 3.350617283950617e-05, 'epoch': 2.5}
{'loss': 2.231, 'grad_norm': 4.738290309906006, 'learning_rate': 3.3444444444444443e-05, 'epoch': 2.5}
  warnings.warn(                                                                                      
{'eval_loss': 2.6943836212158203, 'eval_runtime': 413.4345, 'eval_samples_per_second': 1.451, 'eval_steps_per_second': 1.451, 'epoch': 2.5}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.6335, 'grad_norm': 6.3187761306762695, 'learning_rate': 3.338271604938272e-05, 'epoch': 2.5}
{'loss': 2.6418, 'grad_norm': 7.630374908447266, 'learning_rate': 3.332098765432099e-05, 'epoch': 2.5}
{'loss': 2.6328, 'grad_norm': 5.053310394287109, 'learning_rate': 3.3259259259259265e-05, 'epoch': 2.5}
{'loss': 1.9447, 'grad_norm': 4.631132125854492, 'learning_rate': 3.319753086419753e-05, 'epoch': 2.5}
{'loss': 2.5547, 'grad_norm': 6.900111675262451, 'learning_rate': 3.31358024691358e-05, 'epoch': 2.5}
{'loss': 2.3033, 'grad_norm': 5.6914873123168945, 'learning_rate': 3.307407407407407e-05, 'epoch': 2.51}
{'loss': 2.5131, 'grad_norm': 4.7502827644348145, 'learning_rate': 3.3012345679012346e-05, 'epoch': 2.51}
{'loss': 2.7062, 'grad_norm': 5.734246253967285, 'learning_rate': 3.295061728395062e-05, 'epoch': 2.51}
{'loss': 2.709, 'grad_norm': 5.939759731292725, 'learning_rate': 3.2888888888888894e-05, 'epoch': 2.51}
{'loss': 2.8116, 'grad_norm': 8.408397674560547, 'learning_rate': 3.282716049382716e-05, 'epoch': 2.51}
{'loss': 2.8132, 'grad_norm': 5.814764022827148, 'learning_rate': 3.2765432098765435e-05, 'epoch': 2.51}
{'loss': 2.6185, 'grad_norm': 6.076150417327881, 'learning_rate': 3.27037037037037e-05, 'epoch': 2.51}
{'loss': 3.0107, 'grad_norm': 8.308460235595703, 'learning_rate': 3.2641975308641976e-05, 'epoch': 2.51}
{'loss': 2.5445, 'grad_norm': 4.797641754150391, 'learning_rate': 3.258024691358025e-05, 'epoch': 2.51}
{'loss': 3.2235, 'grad_norm': 6.16339635848999, 'learning_rate': 3.251851851851852e-05, 'epoch': 2.51}
{'loss': 2.7431, 'grad_norm': 8.157962799072266, 'learning_rate': 3.245679012345679e-05, 'epoch': 2.51}
{'loss': 2.6495, 'grad_norm': 6.812528133392334, 'learning_rate': 3.2395061728395064e-05, 'epoch': 2.52}
{'loss': 3.0845, 'grad_norm': 6.216084957122803, 'learning_rate': 3.233333333333333e-05, 'epoch': 2.52}
{'loss': 3.4805, 'grad_norm': 11.047379493713379, 'learning_rate': 3.2271604938271605e-05, 'epoch': 2.52}
{'loss': 2.5107, 'grad_norm': 4.394883155822754, 'learning_rate': 3.220987654320988e-05, 'epoch': 2.52}
{'loss': 2.7358, 'grad_norm': 5.868507385253906, 'learning_rate': 3.214814814814815e-05, 'epoch': 2.52}
{'loss': 2.3976, 'grad_norm': 4.56708288192749, 'learning_rate': 3.208641975308642e-05, 'epoch': 2.52}
{'loss': 3.0123, 'grad_norm': 4.3870697021484375, 'learning_rate': 3.202469135802469e-05, 'epoch': 2.52}
{'loss': 2.2219, 'grad_norm': 5.333641529083252, 'learning_rate': 3.196296296296297e-05, 'epoch': 2.52}
{'loss': 3.0075, 'grad_norm': 6.070115566253662, 'learning_rate': 3.1901234567901234e-05, 'epoch': 2.52}
{'loss': 2.8196, 'grad_norm': 5.812347888946533, 'learning_rate': 3.183950617283951e-05, 'epoch': 2.52}
{'loss': 2.7342, 'grad_norm': 4.437644004821777, 'learning_rate': 3.177777777777778e-05, 'epoch': 2.52}
{'loss': 3.2708, 'grad_norm': 5.238282680511475, 'learning_rate': 3.171604938271605e-05, 'epoch': 2.53}
{'loss': 2.6989, 'grad_norm': 10.831485748291016, 'learning_rate': 3.165432098765432e-05, 'epoch': 2.53}
{'loss': 3.1421, 'grad_norm': 7.447076320648193, 'learning_rate': 3.1592592592592596e-05, 'epoch': 2.53}
{'loss': 2.7043, 'grad_norm': 5.385818004608154, 'learning_rate': 3.153086419753087e-05, 'epoch': 2.53}
{'loss': 2.4098, 'grad_norm': 4.612493991851807, 'learning_rate': 3.146913580246914e-05, 'epoch': 2.53}
{'loss': 2.9891, 'grad_norm': 5.499094009399414, 'learning_rate': 3.140740740740741e-05, 'epoch': 2.53}
{'loss': 2.8184, 'grad_norm': 3.927696943283081, 'learning_rate': 3.134567901234568e-05, 'epoch': 2.53}
{'loss': 2.0669, 'grad_norm': 5.192868232727051, 'learning_rate': 3.128395061728395e-05, 'epoch': 2.53}
{'loss': 2.3217, 'grad_norm': 7.123658657073975, 'learning_rate': 3.1222222222222225e-05, 'epoch': 2.53}
{'loss': 2.9458, 'grad_norm': 9.413938522338867, 'learning_rate': 3.11604938271605e-05, 'epoch': 2.53}
{'loss': 2.3235, 'grad_norm': 4.791723728179932, 'learning_rate': 3.1098765432098766e-05, 'epoch': 2.54}
{'loss': 3.0182, 'grad_norm': 6.891837120056152, 'learning_rate': 3.103703703703704e-05, 'epoch': 2.54}
{'loss': 2.8837, 'grad_norm': 5.326327800750732, 'learning_rate': 3.097530864197531e-05, 'epoch': 2.54}
{'loss': 3.2253, 'grad_norm': 5.45125675201416, 'learning_rate': 3.091358024691358e-05, 'epoch': 2.54}
{'loss': 3.1821, 'grad_norm': 9.154474258422852, 'learning_rate': 3.0851851851851854e-05, 'epoch': 2.54}
{'loss': 2.2364, 'grad_norm': 8.82715129852295, 'learning_rate': 3.079012345679013e-05, 'epoch': 2.54}
{'loss': 2.926, 'grad_norm': 7.876628398895264, 'learning_rate': 3.07283950617284e-05, 'epoch': 2.54}
{'loss': 3.1046, 'grad_norm': 6.134747505187988, 'learning_rate': 3.066666666666667e-05, 'epoch': 2.54}
{'loss': 2.3923, 'grad_norm': 5.634725570678711, 'learning_rate': 3.0604938271604936e-05, 'epoch': 2.54}
{'loss': 2.586, 'grad_norm': 7.124932765960693, 'learning_rate': 3.054320987654321e-05, 'epoch': 2.54}
{'loss': 3.1099, 'grad_norm': 10.667588233947754, 'learning_rate': 3.0481481481481484e-05, 'epoch': 2.54}
{'loss': 3.5603, 'grad_norm': 4.781472206115723, 'learning_rate': 3.0419753086419754e-05, 'epoch': 2.55}
{'loss': 3.1571, 'grad_norm': 8.306282043457031, 'learning_rate': 3.0358024691358028e-05, 'epoch': 2.55}
{'loss': 2.9426, 'grad_norm': 4.817840576171875, 'learning_rate': 3.02962962962963e-05, 'epoch': 2.55}
{'loss': 2.5347, 'grad_norm': 4.504376411437988, 'learning_rate': 3.023456790123457e-05, 'epoch': 2.55}
{'loss': 2.5369, 'grad_norm': 4.331977844238281, 'learning_rate': 3.017283950617284e-05, 'epoch': 2.55}
{'loss': 2.8852, 'grad_norm': 8.489648818969727, 'learning_rate': 3.0111111111111113e-05, 'epoch': 2.55}
{'loss': 3.0463, 'grad_norm': 9.668073654174805, 'learning_rate': 3.0049382716049383e-05, 'epoch': 2.55}
{'loss': 2.8048, 'grad_norm': 4.368093013763428, 'learning_rate': 2.9987654320987657e-05, 'epoch': 2.55}
{'loss': 2.8388, 'grad_norm': 4.16474723815918, 'learning_rate': 2.992592592592593e-05, 'epoch': 2.55}
{'loss': 2.4596, 'grad_norm': 5.436882972717285, 'learning_rate': 2.9864197530864198e-05, 'epoch': 2.55}
{'loss': 2.4926, 'grad_norm': 4.510033130645752, 'learning_rate': 2.9802469135802468e-05, 'epoch': 2.55}
{'loss': 2.7827, 'grad_norm': 5.5686235427856445, 'learning_rate': 2.9740740740740742e-05, 'epoch': 2.56}
{'loss': 2.3505, 'grad_norm': 6.850635528564453, 'learning_rate': 2.9679012345679012e-05, 'epoch': 2.56}
{'loss': 2.4736, 'grad_norm': 3.589963674545288, 'learning_rate': 2.9617283950617286e-05, 'epoch': 2.56}
{'loss': 2.8725, 'grad_norm': 5.002680778503418, 'learning_rate': 2.955555555555556e-05, 'epoch': 2.56}
{'loss': 2.356, 'grad_norm': 6.755716323852539, 'learning_rate': 2.949382716049383e-05, 'epoch': 2.56}
{'loss': 2.7705, 'grad_norm': 6.594424724578857, 'learning_rate': 2.9432098765432097e-05, 'epoch': 2.56}
{'loss': 3.4618, 'grad_norm': 10.113792419433594, 'learning_rate': 2.937037037037037e-05, 'epoch': 2.56}
{'loss': 2.1437, 'grad_norm': 5.5143513679504395, 'learning_rate': 2.930864197530864e-05, 'epoch': 2.56}
{'loss': 2.5953, 'grad_norm': 4.952777862548828, 'learning_rate': 2.9246913580246915e-05, 'epoch': 2.56}
{'loss': 2.4578, 'grad_norm': 5.904198169708252, 'learning_rate': 2.918518518518519e-05, 'epoch': 2.56}
{'loss': 2.9745, 'grad_norm': 5.363093852996826, 'learning_rate': 2.912345679012346e-05, 'epoch': 2.56}
{'loss': 2.6274, 'grad_norm': 7.655372619628906, 'learning_rate': 2.9061728395061733e-05, 'epoch': 2.57}
{'loss': 2.6791, 'grad_norm': 6.581823348999023, 'learning_rate': 2.9e-05, 'epoch': 2.57}
{'loss': 2.9329, 'grad_norm': 6.974964141845703, 'learning_rate': 2.893827160493827e-05, 'epoch': 2.57}
{'loss': 2.3582, 'grad_norm': 6.843812942504883, 'learning_rate': 2.8876543209876544e-05, 'epoch': 2.57}
{'loss': 3.0295, 'grad_norm': 6.370820045471191, 'learning_rate': 2.8814814814814818e-05, 'epoch': 2.57}
{'loss': 2.7918, 'grad_norm': 4.6842427253723145, 'learning_rate': 2.875308641975309e-05, 'epoch': 2.57}
{'loss': 2.4926, 'grad_norm': 6.811275959014893, 'learning_rate': 2.8691358024691362e-05, 'epoch': 2.57}
{'loss': 2.9802, 'grad_norm': 7.2238969802856445, 'learning_rate': 2.862962962962963e-05, 'epoch': 2.57}
{'loss': 2.5483, 'grad_norm': 6.378690242767334, 'learning_rate': 2.85679012345679e-05, 'epoch': 2.57}
{'loss': 2.8541, 'grad_norm': 6.914481163024902, 'learning_rate': 2.8506172839506174e-05, 'epoch': 2.57}
{'loss': 2.5584, 'grad_norm': 5.115135192871094, 'learning_rate': 2.8444444444444447e-05, 'epoch': 2.58}
{'loss': 2.4138, 'grad_norm': 4.372788906097412, 'learning_rate': 2.8382716049382718e-05, 'epoch': 2.58}
{'loss': 2.6114, 'grad_norm': 4.890627384185791, 'learning_rate': 2.832098765432099e-05, 'epoch': 2.58}
{'loss': 3.3369, 'grad_norm': 4.5655694007873535, 'learning_rate': 2.8259259259259262e-05, 'epoch': 2.58}
{'loss': 2.737, 'grad_norm': 9.913761138916016, 'learning_rate': 2.819753086419753e-05, 'epoch': 2.58}
{'loss': 3.1541, 'grad_norm': 8.060053825378418, 'learning_rate': 2.8135802469135803e-05, 'epoch': 2.58}
{'loss': 2.7513, 'grad_norm': 3.421475410461426, 'learning_rate': 2.8074074074074076e-05, 'epoch': 2.58}
{'loss': 2.7446, 'grad_norm': 6.551069259643555, 'learning_rate': 2.8012345679012347e-05, 'epoch': 2.58}
{'loss': 2.4904, 'grad_norm': 3.581683874130249, 'learning_rate': 2.795061728395062e-05, 'epoch': 2.58}
{'loss': 2.7858, 'grad_norm': 5.408212184906006, 'learning_rate': 2.788888888888889e-05, 'epoch': 2.58}
{'loss': 2.8269, 'grad_norm': 3.713156223297119, 'learning_rate': 2.7827160493827158e-05, 'epoch': 2.58}
{'loss': 2.7327, 'grad_norm': 4.257705211639404, 'learning_rate': 2.7765432098765432e-05, 'epoch': 2.59}
{'loss': 3.3369, 'grad_norm': 4.8830976486206055, 'learning_rate': 2.7703703703703706e-05, 'epoch': 2.59}
{'loss': 2.4003, 'grad_norm': 4.633909702301025, 'learning_rate': 2.7641975308641976e-05, 'epoch': 2.59}
{'loss': 2.7033, 'grad_norm': 4.080289363861084, 'learning_rate': 2.758024691358025e-05, 'epoch': 2.59}
{'loss': 3.1829, 'grad_norm': 5.75717306137085, 'learning_rate': 2.751851851851852e-05, 'epoch': 2.59}
{'loss': 2.9007, 'grad_norm': 8.096942901611328, 'learning_rate': 2.7456790123456794e-05, 'epoch': 2.59}
{'loss': 2.2799, 'grad_norm': 6.2797088623046875, 'learning_rate': 2.739506172839506e-05, 'epoch': 2.59}
{'loss': 3.1125, 'grad_norm': 9.582197189331055, 'learning_rate': 2.733333333333333e-05, 'epoch': 2.59}
{'loss': 3.1046, 'grad_norm': 7.034270286560059, 'learning_rate': 2.7271604938271605e-05, 'epoch': 2.59}
  warnings.warn(                                                                                      
{'eval_loss': 2.6914002895355225, 'eval_runtime': 412.8438, 'eval_samples_per_second': 1.453, 'eval_steps_per_second': 1.453, 'epoch': 2.59}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.528, 'grad_norm': 6.332603931427002, 'learning_rate': 2.720987654320988e-05, 'epoch': 2.59}
{'loss': 3.1762, 'grad_norm': 6.050452709197998, 'learning_rate': 2.714814814814815e-05, 'epoch': 2.59}
{'loss': 2.9089, 'grad_norm': 4.113963603973389, 'learning_rate': 2.7086419753086423e-05, 'epoch': 2.6}
{'loss': 2.437, 'grad_norm': 4.569130897521973, 'learning_rate': 2.7024691358024694e-05, 'epoch': 2.6}
{'loss': 3.3272, 'grad_norm': 5.38490629196167, 'learning_rate': 2.696296296296296e-05, 'epoch': 2.6}
{'loss': 2.4618, 'grad_norm': 4.382595539093018, 'learning_rate': 2.6901234567901234e-05, 'epoch': 2.6}
{'loss': 2.6134, 'grad_norm': 4.2652082443237305, 'learning_rate': 2.6839506172839508e-05, 'epoch': 2.6}
{'loss': 2.7975, 'grad_norm': 7.5480499267578125, 'learning_rate': 2.677777777777778e-05, 'epoch': 2.6}
{'loss': 2.1561, 'grad_norm': 6.371143817901611, 'learning_rate': 2.6716049382716052e-05, 'epoch': 2.6}
{'loss': 2.7854, 'grad_norm': 5.392104148864746, 'learning_rate': 2.6654320987654323e-05, 'epoch': 2.6}
{'loss': 2.6185, 'grad_norm': 7.074281215667725, 'learning_rate': 2.659259259259259e-05, 'epoch': 2.6}
{'loss': 2.2682, 'grad_norm': 5.132711887359619, 'learning_rate': 2.6530864197530863e-05, 'epoch': 2.6}
{'loss': 2.2011, 'grad_norm': 5.966280460357666, 'learning_rate': 2.6469135802469137e-05, 'epoch': 2.6}
{'loss': 2.2856, 'grad_norm': 3.3633322715759277, 'learning_rate': 2.6407407407407408e-05, 'epoch': 2.61}
{'loss': 2.8686, 'grad_norm': 6.332118511199951, 'learning_rate': 2.634567901234568e-05, 'epoch': 2.61}
{'loss': 2.5671, 'grad_norm': 5.164677619934082, 'learning_rate': 2.6283950617283952e-05, 'epoch': 2.61}
{'loss': 2.576, 'grad_norm': 3.5572009086608887, 'learning_rate': 2.6222222222222226e-05, 'epoch': 2.61}
{'loss': 2.382, 'grad_norm': 4.152491092681885, 'learning_rate': 2.6160493827160493e-05, 'epoch': 2.61}
{'loss': 2.2849, 'grad_norm': 3.8267409801483154, 'learning_rate': 2.6098765432098766e-05, 'epoch': 2.61}
{'loss': 3.3281, 'grad_norm': 5.882747173309326, 'learning_rate': 2.6037037037037037e-05, 'epoch': 2.61}
{'loss': 2.3933, 'grad_norm': 6.20463752746582, 'learning_rate': 2.597530864197531e-05, 'epoch': 2.61}
{'loss': 2.5019, 'grad_norm': 3.7672884464263916, 'learning_rate': 2.591358024691358e-05, 'epoch': 2.61}
{'loss': 2.5452, 'grad_norm': 4.654371738433838, 'learning_rate': 2.5851851851851855e-05, 'epoch': 2.61}
{'loss': 3.1379, 'grad_norm': 4.576411724090576, 'learning_rate': 2.579012345679013e-05, 'epoch': 2.61}
{'loss': 3.419, 'grad_norm': 8.947042465209961, 'learning_rate': 2.5728395061728396e-05, 'epoch': 2.62}
{'loss': 2.4357, 'grad_norm': 4.749218940734863, 'learning_rate': 2.5666666666666666e-05, 'epoch': 2.62}
{'loss': 2.5586, 'grad_norm': 4.8512959480285645, 'learning_rate': 2.560493827160494e-05, 'epoch': 2.62}
{'loss': 2.295, 'grad_norm': 4.556119441986084, 'learning_rate': 2.554320987654321e-05, 'epoch': 2.62}
{'loss': 2.6792, 'grad_norm': 4.426201343536377, 'learning_rate': 2.5481481481481484e-05, 'epoch': 2.62}
{'loss': 2.8406, 'grad_norm': 8.751204490661621, 'learning_rate': 2.5419753086419758e-05, 'epoch': 2.62}
{'loss': 2.3982, 'grad_norm': 4.552708625793457, 'learning_rate': 2.5358024691358025e-05, 'epoch': 2.62}
{'loss': 2.5404, 'grad_norm': 8.331014633178711, 'learning_rate': 2.5296296296296295e-05, 'epoch': 2.62}
{'loss': 2.6585, 'grad_norm': 5.01390266418457, 'learning_rate': 2.523456790123457e-05, 'epoch': 2.62}
{'loss': 2.7238, 'grad_norm': 5.021160125732422, 'learning_rate': 2.517283950617284e-05, 'epoch': 2.62}
{'loss': 3.0316, 'grad_norm': 5.924464225769043, 'learning_rate': 2.5111111111111113e-05, 'epoch': 2.62}
{'loss': 3.0458, 'grad_norm': 5.780165195465088, 'learning_rate': 2.5049382716049387e-05, 'epoch': 2.63}
{'loss': 2.1072, 'grad_norm': 4.42405891418457, 'learning_rate': 2.4987654320987654e-05, 'epoch': 2.63}
{'loss': 2.7097, 'grad_norm': 7.251749515533447, 'learning_rate': 2.4925925925925928e-05, 'epoch': 2.63}
{'loss': 2.9275, 'grad_norm': 3.7746872901916504, 'learning_rate': 2.48641975308642e-05, 'epoch': 2.63}
{'loss': 3.1955, 'grad_norm': 6.610920429229736, 'learning_rate': 2.480246913580247e-05, 'epoch': 2.63}
{'loss': 2.5981, 'grad_norm': 5.467019081115723, 'learning_rate': 2.4740740740740742e-05, 'epoch': 2.63}
{'loss': 2.9729, 'grad_norm': 6.074129581451416, 'learning_rate': 2.4679012345679016e-05, 'epoch': 2.63}
{'loss': 2.7743, 'grad_norm': 9.529804229736328, 'learning_rate': 2.4617283950617283e-05, 'epoch': 2.63}
{'loss': 2.5499, 'grad_norm': 4.726578712463379, 'learning_rate': 2.4555555555555557e-05, 'epoch': 2.63}
{'loss': 2.9487, 'grad_norm': 10.266683578491211, 'learning_rate': 2.449382716049383e-05, 'epoch': 2.63}
{'loss': 2.5404, 'grad_norm': 5.438378810882568, 'learning_rate': 2.4432098765432098e-05, 'epoch': 2.64}
{'loss': 2.2352, 'grad_norm': 5.046322822570801, 'learning_rate': 2.437037037037037e-05, 'epoch': 2.64}
{'loss': 2.9534, 'grad_norm': 6.968434810638428, 'learning_rate': 2.4308641975308645e-05, 'epoch': 2.64}
{'loss': 3.156, 'grad_norm': 5.156221389770508, 'learning_rate': 2.4246913580246916e-05, 'epoch': 2.64}
{'loss': 3.1284, 'grad_norm': 5.119662761688232, 'learning_rate': 2.4185185185185186e-05, 'epoch': 2.64}
{'loss': 3.1367, 'grad_norm': 7.479367256164551, 'learning_rate': 2.412345679012346e-05, 'epoch': 2.64}
{'loss': 2.5064, 'grad_norm': 5.29256534576416, 'learning_rate': 2.406172839506173e-05, 'epoch': 2.64}
{'loss': 2.9736, 'grad_norm': 6.760035991668701, 'learning_rate': 2.4e-05, 'epoch': 2.64}
{'loss': 2.9213, 'grad_norm': 5.796374320983887, 'learning_rate': 2.3938271604938274e-05, 'epoch': 2.64}
{'loss': 2.8668, 'grad_norm': 7.281489849090576, 'learning_rate': 2.3876543209876545e-05, 'epoch': 2.64}
{'loss': 2.7327, 'grad_norm': 5.688409328460693, 'learning_rate': 2.3814814814814815e-05, 'epoch': 2.64}
{'loss': 2.6553, 'grad_norm': 6.769012451171875, 'learning_rate': 2.3753086419753086e-05, 'epoch': 2.65}
{'loss': 3.138, 'grad_norm': 5.286710739135742, 'learning_rate': 2.369135802469136e-05, 'epoch': 2.65}
{'loss': 3.0792, 'grad_norm': 5.134649753570557, 'learning_rate': 2.3629629629629633e-05, 'epoch': 2.65}
{'loss': 2.0109, 'grad_norm': 5.797685146331787, 'learning_rate': 2.35679012345679e-05, 'epoch': 2.65}
{'loss': 2.7763, 'grad_norm': 7.027207374572754, 'learning_rate': 2.3506172839506174e-05, 'epoch': 2.65}
{'loss': 2.4877, 'grad_norm': 6.700344562530518, 'learning_rate': 2.3444444444444448e-05, 'epoch': 2.65}
{'loss': 2.9178, 'grad_norm': 4.519261837005615, 'learning_rate': 2.3382716049382715e-05, 'epoch': 2.65}
{'loss': 2.8803, 'grad_norm': 7.234945297241211, 'learning_rate': 2.332098765432099e-05, 'epoch': 2.65}
{'loss': 2.5099, 'grad_norm': 10.6450834274292, 'learning_rate': 2.3259259259259262e-05, 'epoch': 2.65}
{'loss': 2.3263, 'grad_norm': 5.557943820953369, 'learning_rate': 2.319753086419753e-05, 'epoch': 2.65}
{'loss': 2.9371, 'grad_norm': 6.12044095993042, 'learning_rate': 2.3135802469135803e-05, 'epoch': 2.65}
{'loss': 2.511, 'grad_norm': 6.115457534790039, 'learning_rate': 2.3074074074074077e-05, 'epoch': 2.66}
{'loss': 2.8085, 'grad_norm': 6.104496002197266, 'learning_rate': 2.3012345679012347e-05, 'epoch': 2.66}
{'loss': 2.81, 'grad_norm': 5.176562309265137, 'learning_rate': 2.2950617283950618e-05, 'epoch': 2.66}
{'loss': 2.9407, 'grad_norm': 14.143400192260742, 'learning_rate': 2.288888888888889e-05, 'epoch': 2.66}
{'loss': 3.0202, 'grad_norm': 4.476571559906006, 'learning_rate': 2.2827160493827162e-05, 'epoch': 2.66}
{'loss': 2.376, 'grad_norm': 4.93251895904541, 'learning_rate': 2.2765432098765432e-05, 'epoch': 2.66}
{'loss': 2.445, 'grad_norm': 4.385576248168945, 'learning_rate': 2.2703703703703706e-05, 'epoch': 2.66}
{'loss': 3.1699, 'grad_norm': 6.787602424621582, 'learning_rate': 2.2641975308641976e-05, 'epoch': 2.66}
{'loss': 2.8431, 'grad_norm': 6.95758581161499, 'learning_rate': 2.2580246913580247e-05, 'epoch': 2.66}
{'loss': 2.5227, 'grad_norm': 8.706892013549805, 'learning_rate': 2.251851851851852e-05, 'epoch': 2.66}
{'loss': 2.5775, 'grad_norm': 5.347005367279053, 'learning_rate': 2.245679012345679e-05, 'epoch': 2.66}
{'loss': 2.2938, 'grad_norm': 6.048563480377197, 'learning_rate': 2.239506172839506e-05, 'epoch': 2.67}
{'loss': 2.4155, 'grad_norm': 5.467212200164795, 'learning_rate': 2.2333333333333335e-05, 'epoch': 2.67}
{'loss': 2.5456, 'grad_norm': 5.808394432067871, 'learning_rate': 2.2271604938271606e-05, 'epoch': 2.67}
{'loss': 2.0242, 'grad_norm': 4.416356086730957, 'learning_rate': 2.220987654320988e-05, 'epoch': 2.67}
{'loss': 2.9944, 'grad_norm': 4.842811107635498, 'learning_rate': 2.214814814814815e-05, 'epoch': 2.67}
{'loss': 2.8109, 'grad_norm': 4.0691046714782715, 'learning_rate': 2.208641975308642e-05, 'epoch': 2.67}
{'loss': 3.2535, 'grad_norm': 6.570542812347412, 'learning_rate': 2.2024691358024694e-05, 'epoch': 2.67}
{'loss': 2.701, 'grad_norm': 9.235389709472656, 'learning_rate': 2.1962962962962964e-05, 'epoch': 2.67}
{'loss': 2.8922, 'grad_norm': 5.898329257965088, 'learning_rate': 2.1901234567901235e-05, 'epoch': 2.67}
{'loss': 2.2357, 'grad_norm': 5.15794563293457, 'learning_rate': 2.183950617283951e-05, 'epoch': 2.67}
{'loss': 2.8421, 'grad_norm': 7.2214274406433105, 'learning_rate': 2.177777777777778e-05, 'epoch': 2.67}
{'loss': 2.7033, 'grad_norm': 4.8700947761535645, 'learning_rate': 2.171604938271605e-05, 'epoch': 2.68}
{'loss': 2.677, 'grad_norm': 5.987757205963135, 'learning_rate': 2.1654320987654323e-05, 'epoch': 2.68}
{'loss': 2.2391, 'grad_norm': 4.791581153869629, 'learning_rate': 2.1592592592592594e-05, 'epoch': 2.68}
{'loss': 2.9575, 'grad_norm': 4.783868789672852, 'learning_rate': 2.1530864197530864e-05, 'epoch': 2.68}
{'loss': 2.9631, 'grad_norm': 5.087779521942139, 'learning_rate': 2.1469135802469138e-05, 'epoch': 2.68}
{'loss': 2.6773, 'grad_norm': 4.952793598175049, 'learning_rate': 2.1407407407407408e-05, 'epoch': 2.68}
{'loss': 2.2924, 'grad_norm': 5.135746002197266, 'learning_rate': 2.134567901234568e-05, 'epoch': 2.68}
{'loss': 2.5591, 'grad_norm': 7.738138675689697, 'learning_rate': 2.1283950617283952e-05, 'epoch': 2.68}
{'loss': 2.303, 'grad_norm': 5.459608554840088, 'learning_rate': 2.1222222222222223e-05, 'epoch': 2.68}
{'loss': 2.2908, 'grad_norm': 6.743983268737793, 'learning_rate': 2.1160493827160493e-05, 'epoch': 2.68}
{'loss': 2.7833, 'grad_norm': 6.062742233276367, 'learning_rate': 2.1098765432098767e-05, 'epoch': 2.69}
  warnings.warn(                                                                                      
{'eval_loss': 2.6917459964752197, 'eval_runtime': 414.1168, 'eval_samples_per_second': 1.449, 'eval_steps_per_second': 1.449, 'epoch': 2.69}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
                                                                                                      
{'loss': 2.8417, 'grad_norm': 6.7260332107543945, 'learning_rate': 2.1037037037037037e-05, 'epoch': 2.69}
{'loss': 2.6669, 'grad_norm': 10.0172758102417, 'learning_rate': 2.097530864197531e-05, 'epoch': 2.69}
{'loss': 2.8869, 'grad_norm': 10.64818286895752, 'learning_rate': 2.091358024691358e-05, 'epoch': 2.69}
{'loss': 3.9376, 'grad_norm': 7.908198833465576, 'learning_rate': 2.0851851851851852e-05, 'epoch': 2.69}
{'loss': 2.3957, 'grad_norm': 6.1116557121276855, 'learning_rate': 2.0790123456790126e-05, 'epoch': 2.69}
{'loss': 2.6641, 'grad_norm': 4.575135231018066, 'learning_rate': 2.0728395061728396e-05, 'epoch': 2.69}
{'loss': 2.6354, 'grad_norm': 4.0176262855529785, 'learning_rate': 2.0666666666666666e-05, 'epoch': 2.69}
{'loss': 2.6078, 'grad_norm': 6.233989715576172, 'learning_rate': 2.060493827160494e-05, 'epoch': 2.69}
{'loss': 3.266, 'grad_norm': 4.8740315437316895, 'learning_rate': 2.054320987654321e-05, 'epoch': 2.69}
{'loss': 2.7858, 'grad_norm': 4.048544406890869, 'learning_rate': 2.048148148148148e-05, 'epoch': 2.69}
{'loss': 2.3793, 'grad_norm': 6.977015018463135, 'learning_rate': 2.0419753086419755e-05, 'epoch': 2.7}
{'loss': 2.2845, 'grad_norm': 5.471200942993164, 'learning_rate': 2.035802469135803e-05, 'epoch': 2.7}
{'loss': 2.6888, 'grad_norm': 7.793852806091309, 'learning_rate': 2.0296296296296296e-05, 'epoch': 2.7}
{'loss': 2.5273, 'grad_norm': 5.840902328491211, 'learning_rate': 2.023456790123457e-05, 'epoch': 2.7}
{'loss': 2.5283, 'grad_norm': 5.143110275268555, 'learning_rate': 2.017283950617284e-05, 'epoch': 2.7}
{'loss': 3.431, 'grad_norm': 14.181384086608887, 'learning_rate': 2.011111111111111e-05, 'epoch': 2.7}
{'loss': 2.6212, 'grad_norm': 4.474231719970703, 'learning_rate': 2.0049382716049384e-05, 'epoch': 2.7}
{'loss': 2.2324, 'grad_norm': 5.861807346343994, 'learning_rate': 1.9987654320987654e-05, 'epoch': 2.7}
{'loss': 2.9064, 'grad_norm': 5.965569019317627, 'learning_rate': 1.9925925925925925e-05, 'epoch': 2.7}
{'loss': 2.2082, 'grad_norm': 5.496546268463135, 'learning_rate': 1.98641975308642e-05, 'epoch': 2.7}
{'loss': 2.4003, 'grad_norm': 5.440264701843262, 'learning_rate': 1.980246913580247e-05, 'epoch': 2.7}
{'loss': 2.1265, 'grad_norm': 4.164881706237793, 'learning_rate': 1.9740740740740743e-05, 'epoch': 2.71}
{'loss': 2.9921, 'grad_norm': 5.098626613616943, 'learning_rate': 1.9679012345679013e-05, 'epoch': 2.71}
{'loss': 2.8108, 'grad_norm': 7.684657573699951, 'learning_rate': 1.9617283950617284e-05, 'epoch': 2.71}
{'loss': 2.618, 'grad_norm': 5.157948017120361, 'learning_rate': 1.9555555555555557e-05, 'epoch': 2.71}
{'loss': 3.4962, 'grad_norm': 6.054504871368408, 'learning_rate': 1.9493827160493828e-05, 'epoch': 2.71}
{'loss': 3.1365, 'grad_norm': 8.59528923034668, 'learning_rate': 1.9432098765432098e-05, 'epoch': 2.71}
{'loss': 2.3718, 'grad_norm': 4.818060398101807, 'learning_rate': 1.9370370370370372e-05, 'epoch': 2.71}
{'loss': 2.1495, 'grad_norm': 6.485076427459717, 'learning_rate': 1.9308641975308642e-05, 'epoch': 2.71}
{'loss': 2.8978, 'grad_norm': 4.013567924499512, 'learning_rate': 1.9246913580246913e-05, 'epoch': 2.71}
{'loss': 2.3334, 'grad_norm': 7.983016490936279, 'learning_rate': 1.9185185185185186e-05, 'epoch': 2.71}
{'loss': 2.9441, 'grad_norm': 5.925459384918213, 'learning_rate': 1.912345679012346e-05, 'epoch': 2.71}
{'loss': 2.801, 'grad_norm': 8.338981628417969, 'learning_rate': 1.9061728395061727e-05, 'epoch': 2.72}
{'loss': 2.7791, 'grad_norm': 7.220567226409912, 'learning_rate': 1.9e-05, 'epoch': 2.72}
{'loss': 3.4587, 'grad_norm': 5.005709648132324, 'learning_rate': 1.8938271604938275e-05, 'epoch': 2.72}
{'loss': 3.005, 'grad_norm': 4.874871253967285, 'learning_rate': 1.8876543209876542e-05, 'epoch': 2.72}
{'loss': 2.588, 'grad_norm': 4.113354206085205, 'learning_rate': 1.8814814814814816e-05, 'epoch': 2.72}
{'loss': 2.404, 'grad_norm': 4.479912757873535, 'learning_rate': 1.875308641975309e-05, 'epoch': 2.72}
{'loss': 2.8091, 'grad_norm': 5.901137351989746, 'learning_rate': 1.8691358024691356e-05, 'epoch': 2.72}
{'loss': 2.679, 'grad_norm': 4.8851470947265625, 'learning_rate': 1.862962962962963e-05, 'epoch': 2.72}
{'loss': 2.8238, 'grad_norm': 5.014654636383057, 'learning_rate': 1.8567901234567904e-05, 'epoch': 2.72}
{'loss': 2.8384, 'grad_norm': 5.426986217498779, 'learning_rate': 1.8506172839506174e-05, 'epoch': 2.72}
{'loss': 2.4914, 'grad_norm': 6.044581890106201, 'learning_rate': 1.8444444444444445e-05, 'epoch': 2.73}
{'loss': 3.2574, 'grad_norm': 7.010993003845215, 'learning_rate': 1.838271604938272e-05, 'epoch': 2.73}
{'loss': 3.0635, 'grad_norm': 5.7568840980529785, 'learning_rate': 1.832098765432099e-05, 'epoch': 2.73}
{'loss': 2.8997, 'grad_norm': 5.502562522888184, 'learning_rate': 1.825925925925926e-05, 'epoch': 2.73}
{'loss': 2.7856, 'grad_norm': 5.5004563331604, 'learning_rate': 1.8197530864197533e-05, 'epoch': 2.73}
{'loss': 3.1013, 'grad_norm': 8.2459077835083, 'learning_rate': 1.8135802469135804e-05, 'epoch': 2.73}
{'loss': 3.1404, 'grad_norm': 4.8199591636657715, 'learning_rate': 1.8074074074074074e-05, 'epoch': 2.73}
{'loss': 3.0661, 'grad_norm': 4.656582832336426, 'learning_rate': 1.8012345679012348e-05, 'epoch': 2.73}
{'loss': 2.9, 'grad_norm': 6.873682498931885, 'learning_rate': 1.7950617283950618e-05, 'epoch': 2.73}
{'loss': 2.8296, 'grad_norm': 8.682225227355957, 'learning_rate': 1.788888888888889e-05, 'epoch': 2.73}
{'loss': 2.5524, 'grad_norm': 4.585686683654785, 'learning_rate': 1.7827160493827162e-05, 'epoch': 2.73}
{'loss': 2.4035, 'grad_norm': 4.988230228424072, 'learning_rate': 1.7765432098765433e-05, 'epoch': 2.74}
{'loss': 3.0128, 'grad_norm': 8.51967716217041, 'learning_rate': 1.7703703703703706e-05, 'epoch': 2.74}
{'loss': 2.9454, 'grad_norm': 5.146594524383545, 'learning_rate': 1.7641975308641977e-05, 'epoch': 2.74}
{'loss': 2.6305, 'grad_norm': 3.7238733768463135, 'learning_rate': 1.7580246913580247e-05, 'epoch': 2.74}
{'loss': 3.1976, 'grad_norm': 15.793943405151367, 'learning_rate': 1.751851851851852e-05, 'epoch': 2.74}
{'loss': 2.7023, 'grad_norm': 9.601899147033691, 'learning_rate': 1.745679012345679e-05, 'epoch': 2.74}
{'loss': 2.6061, 'grad_norm': 4.943004131317139, 'learning_rate': 1.7395061728395062e-05, 'epoch': 2.74}
{'loss': 2.6719, 'grad_norm': 5.548871994018555, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.74}
{'loss': 2.9175, 'grad_norm': 5.718203544616699, 'learning_rate': 1.7271604938271606e-05, 'epoch': 2.74}
{'loss': 2.5741, 'grad_norm': 4.835862159729004, 'learning_rate': 1.7209876543209876e-05, 'epoch': 2.74}
{'loss': 3.1435, 'grad_norm': 6.832781791687012, 'learning_rate': 1.714814814814815e-05, 'epoch': 2.74}
{'loss': 2.3474, 'grad_norm': 3.6859874725341797, 'learning_rate': 1.708641975308642e-05, 'epoch': 2.75}
{'loss': 2.5993, 'grad_norm': 4.672987461090088, 'learning_rate': 1.702469135802469e-05, 'epoch': 2.75}
{'loss': 3.2522, 'grad_norm': 11.204912185668945, 'learning_rate': 1.6962962962962965e-05, 'epoch': 2.75}
{'loss': 2.6283, 'grad_norm': 7.398959636688232, 'learning_rate': 1.6901234567901235e-05, 'epoch': 2.75}
{'loss': 2.8408, 'grad_norm': 8.054279327392578, 'learning_rate': 1.6839506172839506e-05, 'epoch': 2.75}
{'loss': 2.6638, 'grad_norm': 5.445352554321289, 'learning_rate': 1.677777777777778e-05, 'epoch': 2.75}
{'loss': 2.5389, 'grad_norm': 3.972259759902954, 'learning_rate': 1.671604938271605e-05, 'epoch': 2.75}
{'loss': 2.9605, 'grad_norm': 5.595129489898682, 'learning_rate': 1.665432098765432e-05, 'epoch': 2.75}
{'loss': 2.4142, 'grad_norm': 6.26569128036499, 'learning_rate': 1.6592592592592594e-05, 'epoch': 2.75}
{'loss': 2.4238, 'grad_norm': 7.158269882202148, 'learning_rate': 1.6530864197530864e-05, 'epoch': 2.75}
{'loss': 2.9106, 'grad_norm': 5.590030670166016, 'learning_rate': 1.6469135802469138e-05, 'epoch': 2.75}
{'loss': 2.41, 'grad_norm': 4.847620487213135, 'learning_rate': 1.640740740740741e-05, 'epoch': 2.76}
{'loss': 2.8838, 'grad_norm': 6.939249038696289, 'learning_rate': 1.634567901234568e-05, 'epoch': 2.76}
{'loss': 2.7365, 'grad_norm': 4.9940290451049805, 'learning_rate': 1.6283950617283953e-05, 'epoch': 2.76}
{'loss': 2.3976, 'grad_norm': 5.384340763092041, 'learning_rate': 1.6222222222222223e-05, 'epoch': 2.76}
{'loss': 2.6939, 'grad_norm': 5.868856906890869, 'learning_rate': 1.6160493827160494e-05, 'epoch': 2.76}
{'loss': 3.0416, 'grad_norm': 5.654566287994385, 'learning_rate': 1.6098765432098767e-05, 'epoch': 2.76}
{'loss': 2.7862, 'grad_norm': 8.660116195678711, 'learning_rate': 1.6037037037037038e-05, 'epoch': 2.76}
{'loss': 2.3634, 'grad_norm': 4.734980583190918, 'learning_rate': 1.5975308641975308e-05, 'epoch': 2.76}
{'loss': 2.2808, 'grad_norm': 4.931224346160889, 'learning_rate': 1.5913580246913582e-05, 'epoch': 2.76}
{'loss': 2.8691, 'grad_norm': 5.33713960647583, 'learning_rate': 1.5851851851851852e-05, 'epoch': 2.76}
{'loss': 2.7722, 'grad_norm': 7.220944404602051, 'learning_rate': 1.5790123456790123e-05, 'epoch': 2.76}
{'loss': 2.3865, 'grad_norm': 4.904357433319092, 'learning_rate': 1.5728395061728396e-05, 'epoch': 2.77}
{'loss': 2.4556, 'grad_norm': 8.59610366821289, 'learning_rate': 1.5666666666666667e-05, 'epoch': 2.77}
{'loss': 2.6438, 'grad_norm': 4.572022914886475, 'learning_rate': 1.5604938271604937e-05, 'epoch': 2.77}
{'loss': 2.4927, 'grad_norm': 5.916775703430176, 'learning_rate': 1.554320987654321e-05, 'epoch': 2.77}
{'loss': 2.6859, 'grad_norm': 8.111700057983398, 'learning_rate': 1.548148148148148e-05, 'epoch': 2.77}
{'loss': 3.1851, 'grad_norm': 5.940194606781006, 'learning_rate': 1.5419753086419752e-05, 'epoch': 2.77}
{'loss': 2.7856, 'grad_norm': 9.17194938659668, 'learning_rate': 1.5358024691358026e-05, 'epoch': 2.77}
{'loss': 2.5861, 'grad_norm': 6.119302272796631, 'learning_rate': 1.5296296296296296e-05, 'epoch': 2.77}
{'loss': 2.2098, 'grad_norm': 4.9882493019104, 'learning_rate': 1.523456790123457e-05, 'epoch': 2.77}
{'loss': 2.2631, 'grad_norm': 6.969175338745117, 'learning_rate': 1.517283950617284e-05, 'epoch': 2.77}
{'loss': 3.1803, 'grad_norm': 7.6031813621521, 'learning_rate': 1.5111111111111112e-05, 'epoch': 2.77}
{'loss': 2.4932, 'grad_norm': 6.995889186859131, 'learning_rate': 1.5049382716049384e-05, 'epoch': 2.78}
{'loss': 2.1464, 'grad_norm': 4.262861251831055, 'learning_rate': 1.4987654320987655e-05, 'epoch': 2.78}
{'loss': 3.2137, 'grad_norm': 7.426929473876953, 'learning_rate': 1.4925925925925927e-05, 'epoch': 2.78}
  warnings.warn(                                                                                      
{'eval_loss': 2.6891794204711914, 'eval_runtime': 413.9231, 'eval_samples_per_second': 1.45, 'eval_steps_per_second': 1.45, 'epoch': 2.78}
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 93%|████████████████████████████████████████████████████▎   | 15144/16200 [13:19:20<41:21,  2.35s/it]
{'loss': 2.7835, 'grad_norm': 4.84375, 'learning_rate': 1.4864197530864199e-05, 'epoch': 2.78}
{'loss': 2.4997, 'grad_norm': 8.306723594665527, 'learning_rate': 1.480246913580247e-05, 'epoch': 2.78}
{'loss': 2.5358, 'grad_norm': 5.076754093170166, 'learning_rate': 1.4740740740740741e-05, 'epoch': 2.78}
{'loss': 2.4682, 'grad_norm': 6.8408098220825195, 'learning_rate': 1.4679012345679014e-05, 'epoch': 2.78}
{'loss': 2.431, 'grad_norm': 4.4959187507629395, 'learning_rate': 1.4617283950617286e-05, 'epoch': 2.78}
{'loss': 2.8804, 'grad_norm': 4.446401119232178, 'learning_rate': 1.4555555555555556e-05, 'epoch': 2.78}
{'loss': 2.8632, 'grad_norm': 5.863044738769531, 'learning_rate': 1.4493827160493828e-05, 'epoch': 2.78}
{'loss': 2.4307, 'grad_norm': 4.324478626251221, 'learning_rate': 1.44320987654321e-05, 'epoch': 2.79}
{'loss': 2.9597, 'grad_norm': 4.826352119445801, 'learning_rate': 1.437037037037037e-05, 'epoch': 2.79}
{'loss': 2.6398, 'grad_norm': 4.998079776763916, 'learning_rate': 1.4308641975308643e-05, 'epoch': 2.79}
{'loss': 2.846, 'grad_norm': 5.291580677032471, 'learning_rate': 1.4246913580246915e-05, 'epoch': 2.79}
{'loss': 2.7582, 'grad_norm': 5.849451065063477, 'learning_rate': 1.4185185185185185e-05, 'epoch': 2.79}
{'loss': 2.5225, 'grad_norm': 5.080352783203125, 'learning_rate': 1.4123456790123457e-05, 'epoch': 2.79}
{'loss': 3.0361, 'grad_norm': 8.803717613220215, 'learning_rate': 1.406172839506173e-05, 'epoch': 2.79}
{'loss': 3.2037, 'grad_norm': 8.62452507019043, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.79}
{'loss': 3.6524, 'grad_norm': 9.52479362487793, 'learning_rate': 1.3938271604938272e-05, 'epoch': 2.79}
{'loss': 2.3557, 'grad_norm': 7.696145057678223, 'learning_rate': 1.3876543209876544e-05, 'epoch': 2.79}
{'loss': 2.6036, 'grad_norm': 5.656032085418701, 'learning_rate': 1.3814814814814816e-05, 'epoch': 2.79}
{'loss': 2.6942, 'grad_norm': 5.474637985229492, 'learning_rate': 1.3753086419753086e-05, 'epoch': 2.8}
{'loss': 2.6729, 'grad_norm': 5.39240837097168, 'learning_rate': 1.3691358024691359e-05, 'epoch': 2.8}
{'loss': 3.1155, 'grad_norm': 4.854184627532959, 'learning_rate': 1.362962962962963e-05, 'epoch': 2.8}
{'loss': 3.1388, 'grad_norm': 4.244297504425049, 'learning_rate': 1.3567901234567901e-05, 'epoch': 2.8}
{'loss': 3.3646, 'grad_norm': 5.377147674560547, 'learning_rate': 1.3506172839506173e-05, 'epoch': 2.8}
{'loss': 3.2339, 'grad_norm': 4.4544172286987305, 'learning_rate': 1.3444444444444445e-05, 'epoch': 2.8}
{'loss': 3.1481, 'grad_norm': 5.084944248199463, 'learning_rate': 1.3382716049382717e-05, 'epoch': 2.8}
{'loss': 2.678, 'grad_norm': 6.100331783294678, 'learning_rate': 1.3320987654320988e-05, 'epoch': 2.8}
{'loss': 2.1027, 'grad_norm': 5.088811874389648, 'learning_rate': 1.325925925925926e-05, 'epoch': 2.8}
{'loss': 2.6521, 'grad_norm': 4.605925559997559, 'learning_rate': 1.3197530864197532e-05, 'epoch': 2.8}
