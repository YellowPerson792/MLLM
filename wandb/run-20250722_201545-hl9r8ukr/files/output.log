  0%|                                                                        | 0/1125 [00:00<?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  3%|█▌                                                           | 29/1125 [04:00<2:28:34,  8.13s/it]Traceback (most recent call last):
{'loss': 3.3514, 'grad_norm': 35.85979080200195, 'learning_rate': 9.964444444444445e-06, 'epoch': 0.01}
{'loss': 3.0414, 'grad_norm': 30.28618812561035, 'learning_rate': 9.920000000000002e-06, 'epoch': 0.03}
{'loss': 2.8846, 'grad_norm': 26.746063232421875, 'learning_rate': 9.875555555555557e-06, 'epoch': 0.04}
{'loss': 2.722, 'grad_norm': 18.394798278808594, 'learning_rate': 9.831111111111112e-06, 'epoch': 0.05}
{'loss': 2.7088, 'grad_norm': 21.80021095275879, 'learning_rate': 9.786666666666667e-06, 'epoch': 0.07}
  File "/root/autodl-tmp/MLLM/jpeg-lm/classification_encoder_train.py", line 328, in <module>
    trainer.train()
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 3745, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/jpeg-lm/classification_encoder_train.py", line 272, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/accelerate/utils/operations.py", line 818, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/accelerate/utils/operations.py", line 806, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/jpeg-lm/models/jpeglm_encoder.py", line 122, in forward
    pooled_output = self._pool_hidden_states(sequence_output, attention_mask)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/jpeg-lm/models/jpeglm_encoder.py", line 78, in _pool_hidden_states
    sum_embeddings = torch.sum(sequence_output * mask_expanded, 1)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
