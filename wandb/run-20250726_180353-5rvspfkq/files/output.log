  0%|                                                                              | 0/4500 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
                                                                                                            
{'loss': 1.5358, 'grad_norm': 2.7340102195739746, 'learning_rate': 4.978888888888889e-05, 'epoch': 0.01}
{'loss': 0.5155, 'grad_norm': 3.0143463611602783, 'learning_rate': 4.956666666666667e-05, 'epoch': 0.03}
{'loss': 0.5071, 'grad_norm': 2.728285789489746, 'learning_rate': 4.934444444444445e-05, 'epoch': 0.04}
{'loss': 0.5219, 'grad_norm': 3.7496893405914307, 'learning_rate': 4.912222222222223e-05, 'epoch': 0.05}
{'loss': 0.4541, 'grad_norm': 3.3482277393341064, 'learning_rate': 4.89e-05, 'epoch': 0.07}
{'loss': 0.48, 'grad_norm': 2.2097413539886475, 'learning_rate': 4.867777777777778e-05, 'epoch': 0.08}
{'loss': 0.4241, 'grad_norm': 2.5179383754730225, 'learning_rate': 4.845555555555556e-05, 'epoch': 0.09}
{'loss': 0.5096, 'grad_norm': 2.4029057025909424, 'learning_rate': 4.823333333333334e-05, 'epoch': 0.11}
{'loss': 0.4566, 'grad_norm': 2.0494561195373535, 'learning_rate': 4.8011111111111114e-05, 'epoch': 0.12}
{'loss': 0.4689, 'grad_norm': 1.7031632661819458, 'learning_rate': 4.778888888888889e-05, 'epoch': 0.13}
{'loss': 0.41, 'grad_norm': 1.6418014764785767, 'learning_rate': 4.756666666666667e-05, 'epoch': 0.15}
{'loss': 0.4665, 'grad_norm': 1.6135140657424927, 'learning_rate': 4.734444444444445e-05, 'epoch': 0.16}
{'loss': 0.4432, 'grad_norm': 1.3476983308792114, 'learning_rate': 4.7122222222222225e-05, 'epoch': 0.17}
{'loss': 0.4618, 'grad_norm': 1.9340453147888184, 'learning_rate': 4.69e-05, 'epoch': 0.19}
{'loss': 0.4423, 'grad_norm': 1.461277723312378, 'learning_rate': 4.6677777777777785e-05, 'epoch': 0.2}
{'loss': 0.4621, 'grad_norm': 1.5425952672958374, 'learning_rate': 4.645555555555556e-05, 'epoch': 0.21}
{'loss': 0.4307, 'grad_norm': 1.1617958545684814, 'learning_rate': 4.623333333333334e-05, 'epoch': 0.23}
{'loss': 0.4142, 'grad_norm': 1.4282033443450928, 'learning_rate': 4.601111111111111e-05, 'epoch': 0.24}
{'loss': 0.4241, 'grad_norm': 1.2818856239318848, 'learning_rate': 4.578888888888889e-05, 'epoch': 0.25}
{'loss': 0.4302, 'grad_norm': 1.2805389165878296, 'learning_rate': 4.556666666666667e-05, 'epoch': 0.27}
{'loss': 0.4232, 'grad_norm': 1.0188543796539307, 'learning_rate': 4.534444444444445e-05, 'epoch': 0.28}
{'loss': 0.4264, 'grad_norm': 0.8657287359237671, 'learning_rate': 4.5122222222222224e-05, 'epoch': 0.29}
{'loss': 0.4272, 'grad_norm': 1.0518324375152588, 'learning_rate': 4.49e-05, 'epoch': 0.31}
{'loss': 0.4405, 'grad_norm': 1.18131685256958, 'learning_rate': 4.4677777777777777e-05, 'epoch': 0.32}
{'loss': 0.4457, 'grad_norm': 1.0554051399230957, 'learning_rate': 4.445555555555555e-05, 'epoch': 0.33}
{'loss': 0.4553, 'grad_norm': 1.1181466579437256, 'learning_rate': 4.4233333333333336e-05, 'epoch': 0.35}
{'loss': 0.4053, 'grad_norm': 1.022123098373413, 'learning_rate': 4.401111111111111e-05, 'epoch': 0.36}
{'loss': 0.4724, 'grad_norm': 0.7356342673301697, 'learning_rate': 4.378888888888889e-05, 'epoch': 0.37}
{'loss': 0.4323, 'grad_norm': 0.8598480224609375, 'learning_rate': 4.3566666666666664e-05, 'epoch': 0.39}
{'loss': 0.4434, 'grad_norm': 1.023345708847046, 'learning_rate': 4.334444444444445e-05, 'epoch': 0.4}
{'loss': 0.4231, 'grad_norm': 0.7304592728614807, 'learning_rate': 4.312222222222222e-05, 'epoch': 0.41}
{'loss': 0.3938, 'grad_norm': 1.0081360340118408, 'learning_rate': 4.29e-05, 'epoch': 0.43}
{'loss': 0.412, 'grad_norm': 0.8095768094062805, 'learning_rate': 4.2677777777777775e-05, 'epoch': 0.44}
{'loss': 0.3885, 'grad_norm': 0.7250504493713379, 'learning_rate': 4.245555555555556e-05, 'epoch': 0.45}
{'loss': 0.402, 'grad_norm': 0.7562611103057861, 'learning_rate': 4.2233333333333334e-05, 'epoch': 0.47}
{'loss': 0.4048, 'grad_norm': 0.697786808013916, 'learning_rate': 4.201111111111111e-05, 'epoch': 0.48}
{'loss': 0.4166, 'grad_norm': 0.6863263249397278, 'learning_rate': 4.178888888888889e-05, 'epoch': 0.49}
{'loss': 0.4257, 'grad_norm': 0.9043505787849426, 'learning_rate': 4.156666666666667e-05, 'epoch': 0.51}
{'loss': 0.4164, 'grad_norm': 0.68500816822052, 'learning_rate': 4.1344444444444446e-05, 'epoch': 0.52}
{'loss': 0.4119, 'grad_norm': 0.7510678172111511, 'learning_rate': 4.112222222222222e-05, 'epoch': 0.53}
{'loss': 0.418, 'grad_norm': 0.778070330619812, 'learning_rate': 4.09e-05, 'epoch': 0.55}
{'loss': 0.3701, 'grad_norm': 0.7185508012771606, 'learning_rate': 4.067777777777778e-05, 'epoch': 0.56}
{'loss': 0.4057, 'grad_norm': 0.8714028596878052, 'learning_rate': 4.045555555555556e-05, 'epoch': 0.57}
{'loss': 0.4076, 'grad_norm': 0.7711836099624634, 'learning_rate': 4.023333333333333e-05, 'epoch': 0.59}
{'loss': 0.3873, 'grad_norm': 0.6365957260131836, 'learning_rate': 4.001111111111111e-05, 'epoch': 0.6}
{'loss': 0.4154, 'grad_norm': 0.7180523872375488, 'learning_rate': 3.978888888888889e-05, 'epoch': 0.61}
{'loss': 0.4183, 'grad_norm': 0.6411172151565552, 'learning_rate': 3.956666666666667e-05, 'epoch': 0.63}
{'loss': 0.4189, 'grad_norm': 0.7657110095024109, 'learning_rate': 3.9344444444444445e-05, 'epoch': 0.64}
{'loss': 0.399, 'grad_norm': 0.7424474358558655, 'learning_rate': 3.912222222222223e-05, 'epoch': 0.65}
{'loss': 0.391, 'grad_norm': 0.7792336344718933, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.67}
{'loss': 0.4299, 'grad_norm': 0.6809403300285339, 'learning_rate': 3.867777777777778e-05, 'epoch': 0.68}
{'loss': 0.4269, 'grad_norm': 0.5840219855308533, 'learning_rate': 3.8455555555555556e-05, 'epoch': 0.69}
{'loss': 0.4285, 'grad_norm': 0.7348492741584778, 'learning_rate': 3.823333333333334e-05, 'epoch': 0.71}
{'loss': 0.4194, 'grad_norm': 0.6883987188339233, 'learning_rate': 3.8011111111111115e-05, 'epoch': 0.72}
{'loss': 0.4161, 'grad_norm': 0.717815637588501, 'learning_rate': 3.778888888888889e-05, 'epoch': 0.73}
{'loss': 0.4084, 'grad_norm': 0.6006084680557251, 'learning_rate': 3.756666666666667e-05, 'epoch': 0.75}
{'loss': 0.3773, 'grad_norm': 0.7276299595832825, 'learning_rate': 3.734444444444445e-05, 'epoch': 0.76}
{'loss': 0.414, 'grad_norm': 0.7792511582374573, 'learning_rate': 3.7122222222222226e-05, 'epoch': 0.77}
{'loss': 0.4355, 'grad_norm': 0.6471123099327087, 'learning_rate': 3.69e-05, 'epoch': 0.79}
{'loss': 0.4053, 'grad_norm': 0.6928383111953735, 'learning_rate': 3.667777777777778e-05, 'epoch': 0.8}
{'loss': 0.3906, 'grad_norm': 0.601861834526062, 'learning_rate': 3.645555555555556e-05, 'epoch': 0.81}
{'loss': 0.4031, 'grad_norm': 0.5361030697822571, 'learning_rate': 3.623333333333334e-05, 'epoch': 0.83}
{'loss': 0.3542, 'grad_norm': 0.6551283001899719, 'learning_rate': 3.6011111111111114e-05, 'epoch': 0.84}
{'loss': 0.4025, 'grad_norm': 0.7205638289451599, 'learning_rate': 3.578888888888889e-05, 'epoch': 0.85}
{'loss': 0.3629, 'grad_norm': 0.53360915184021, 'learning_rate': 3.556666666666667e-05, 'epoch': 0.87}
{'loss': 0.3777, 'grad_norm': 0.582395613193512, 'learning_rate': 3.534444444444445e-05, 'epoch': 0.88}
{'loss': 0.4127, 'grad_norm': 0.6222747564315796, 'learning_rate': 3.5122222222222225e-05, 'epoch': 0.89}
{'loss': 0.4133, 'grad_norm': 0.7244089841842651, 'learning_rate': 3.49e-05, 'epoch': 0.91}
{'loss': 0.3619, 'grad_norm': 0.6498568654060364, 'learning_rate': 3.4677777777777784e-05, 'epoch': 0.92}
{'loss': 0.3902, 'grad_norm': 0.5777777433395386, 'learning_rate': 3.445555555555556e-05, 'epoch': 0.93}
{'loss': 0.4129, 'grad_norm': 0.6597431302070618, 'learning_rate': 3.4233333333333336e-05, 'epoch': 0.95}
{'loss': 0.3774, 'grad_norm': 0.6595314741134644, 'learning_rate': 3.401111111111111e-05, 'epoch': 0.96}
{'loss': 0.3806, 'grad_norm': 0.6104356050491333, 'learning_rate': 3.378888888888889e-05, 'epoch': 0.97}
{'loss': 0.3878, 'grad_norm': 0.6360877752304077, 'learning_rate': 3.356666666666667e-05, 'epoch': 0.99}
{'loss': 0.4024, 'grad_norm': 0.6625853776931763, 'learning_rate': 3.334444444444445e-05, 'epoch': 1.0}
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.             | 0/250 [00:00<?, ?it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 2/250 [00:00<01:47,  2.30it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 3/250 [00:01<02:30,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 4/250 [00:02<02:52,  1.42it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 5/250 [00:03<02:48,  1.45it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 6/250 [00:03<02:30,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 7/250 [00:04<02:26,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 8/250 [00:04<02:17,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 9/250 [00:05<02:13,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 10/250 [00:05<02:18,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 11/250 [00:06<02:18,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 12/250 [00:07<02:22,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 13/250 [00:07<02:15,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 14/250 [00:08<02:17,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 15/250 [00:08<02:17,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 16/250 [00:09<02:10,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 17/250 [00:09<02:07,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 18/250 [00:10<02:10,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 19/250 [00:11<02:14,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 20/250 [00:11<02:14,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 21/250 [00:12<02:09,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 22/250 [00:12<02:06,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 23/250 [00:13<02:01,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 24/250 [00:13<02:03,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 25/250 [00:14<02:03,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 26/250 [00:14<02:05,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 27/250 [00:15<02:04,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 28/250 [00:16<02:06,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 29/250 [00:16<01:59,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 30/250 [00:17<01:57,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 31/250 [00:17<01:53,  1.93it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 32/250 [00:18<01:51,  1.96it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 33/250 [00:18<01:49,  1.98it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 34/250 [00:19<01:48,  1.99it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 35/250 [00:19<01:52,  1.90it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 36/250 [00:20<01:55,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 37/250 [00:20<01:58,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 38/250 [00:21<01:59,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 39/250 [00:21<02:01,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 40/250 [00:22<02:02,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 41/250 [00:23<01:57,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 42/250 [00:23<01:59,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 43/250 [00:24<02:00,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 44/250 [00:24<01:54,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 45/250 [00:25<01:56,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 46/250 [00:26<01:59,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 47/250 [00:26<01:53,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 48/250 [00:27<01:49,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 49/250 [00:27<01:50,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 50/250 [00:28<01:53,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 51/250 [00:28<01:47,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 52/250 [00:29<01:49,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 53/250 [00:29<01:44,  1.89it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 54/250 [00:30<01:41,  1.93it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 55/250 [00:30<01:47,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 56/250 [00:31<01:43,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 57/250 [00:31<01:45,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 58/250 [00:32<01:41,  1.90it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 59/250 [00:32<01:43,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 60/250 [00:33<01:41,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 61/250 [00:34<01:43,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 62/250 [00:34<01:45,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 63/250 [00:35<01:48,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 64/250 [00:35<01:50,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 65/250 [00:36<01:44,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 66/250 [00:36<01:40,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 67/250 [00:37<01:41,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 68/250 [00:38<01:42,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 69/250 [00:38<01:45,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 70/250 [00:39<01:46,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 71/250 [00:39<01:41,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 72/250 [00:40<01:41,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 73/250 [00:41<01:44,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 74/250 [00:41<01:38,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 75/250 [00:42<01:34,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 76/250 [00:42<01:36,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 77/250 [00:43<01:32,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 78/250 [00:43<01:35,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 79/250 [00:44<01:36,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 80/250 [00:44<01:33,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 81/250 [00:45<01:30,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 82/250 [00:45<01:31,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 83/250 [00:46<01:32,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 84/250 [00:47<01:34,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 85/250 [00:47<01:30,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 86/250 [00:48<01:27,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 87/250 [00:48<01:29,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 88/250 [00:49<01:30,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 89/250 [00:49<01:26,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 90/250 [00:50<01:23,  1.92it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 91/250 [00:50<01:23,  1.90it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 92/250 [00:51<01:24,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 93/250 [00:51<01:30,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 94/250 [00:52<01:29,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 95/250 [00:53<01:29,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 96/250 [00:53<01:26,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 97/250 [00:54<01:27,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 98/250 [00:54<01:24,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 99/250 [00:55<01:26,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 100/250 [00:56<01:29,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 101/250 [00:56<01:29,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 102/250 [00:57<01:28,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 103/250 [00:57<01:24,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 104/250 [00:58<01:25,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 105/250 [00:58<01:24,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 106/250 [00:59<01:24,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 107/250 [01:00<01:25,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 108/250 [01:00<01:25,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 109/250 [01:01<01:20,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 110/250 [01:01<01:21,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 111/250 [01:02<01:19,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 112/250 [01:02<01:19,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 113/250 [01:03<01:15,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 114/250 [01:04<01:16,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 115/250 [01:04<01:13,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 116/250 [01:05<01:11,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 117/250 [01:05<01:13,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 118/250 [01:06<01:10,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 119/250 [01:06<01:08,  1.92it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 120/250 [01:07<01:07,  1.94it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 121/250 [01:07<01:09,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 122/250 [01:08<01:10,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 123/250 [01:08<01:08,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 124/250 [01:09<01:08,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 125/250 [01:10<01:10,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 126/250 [01:10<01:12,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 127/250 [01:11<01:12,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 128/250 [01:11<01:11,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 129/250 [01:12<01:08,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 130/250 [01:12<01:08,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 131/250 [01:13<01:04,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 132/250 [01:13<01:02,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 133/250 [01:14<01:00,  1.94it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 134/250 [01:14<00:59,  1.95it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 135/250 [01:15<00:58,  1.96it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 136/250 [01:16<01:01,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 137/250 [01:16<00:59,  1.91it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 138/250 [01:16<00:57,  1.95it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 139/250 [01:17<00:59,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 140/250 [01:18<01:01,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 141/250 [01:18<00:59,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 142/250 [01:19<01:00,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 143/250 [01:19<00:58,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 144/250 [01:20<00:57,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 145/250 [01:20<00:56,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 146/250 [01:21<00:57,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 147/250 [01:22<00:57,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 148/250 [01:22<00:55,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 149/250 [01:23<00:53,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 150/250 [01:23<00:52,  1.92it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 151/250 [01:24<00:50,  1.97it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 152/250 [01:24<00:48,  2.00it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 153/250 [01:24<00:47,  2.03it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 154/250 [01:25<00:51,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 155/250 [01:26<00:52,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 156/250 [01:26<00:50,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 157/250 [01:27<00:49,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 158/250 [01:27<00:49,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 159/250 [01:28<00:46,  1.94it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 160/250 [01:28<00:45,  2.00it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 161/250 [01:29<00:46,  1.91it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 162/250 [01:29<00:48,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 163/250 [01:30<00:46,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 164/250 [01:30<00:46,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 165/250 [01:31<00:47,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 166/250 [01:32<00:46,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 167/250 [01:32<00:44,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 168/250 [01:33<00:42,  1.92it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 169/250 [01:33<00:42,  1.91it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 170/250 [01:34<00:42,  1.90it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 171/250 [01:34<00:43,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 172/250 [01:35<00:42,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 173/250 [01:35<00:42,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 174/250 [01:36<00:40,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 175/250 [01:36<00:40,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 176/250 [01:37<00:41,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 177/250 [01:37<00:40,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 178/250 [01:38<00:38,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 179/250 [01:39<00:39,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 180/250 [01:39<00:40,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 181/250 [01:40<00:38,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 182/250 [01:40<00:39,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 183/250 [01:41<00:39,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 184/250 [01:42<00:38,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 185/250 [01:42<00:37,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 186/250 [01:43<00:35,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 187/250 [01:43<00:34,  1.85it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 188/250 [01:44<00:32,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 189/250 [01:44<00:32,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 190/250 [01:45<00:31,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 191/250 [01:45<00:32,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 192/250 [01:46<00:31,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 193/250 [01:46<00:30,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 194/250 [01:47<00:29,  1.89it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 195/250 [01:47<00:29,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 196/250 [01:48<00:30,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 197/250 [01:49<00:30,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 198/250 [01:49<00:28,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 199/250 [01:50<00:27,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 200/250 [01:50<00:31,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 201/250 [01:51<00:32,  1.51it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 202/250 [01:52<00:30,  1.55it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 203/250 [01:52<00:29,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 204/250 [01:53<00:27,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 205/250 [01:53<00:25,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 206/250 [01:54<00:23,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 207/250 [01:54<00:24,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 208/250 [01:55<00:22,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 209/250 [01:56<00:23,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 210/250 [01:56<00:22,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 211/250 [01:57<00:21,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 212/250 [01:57<00:21,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 213/250 [01:58<00:21,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 214/250 [01:59<00:21,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 215/250 [01:59<00:20,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 216/250 [02:00<00:18,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 217/250 [02:00<00:17,  1.86it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 218/250 [02:01<00:18,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 219/250 [02:01<00:17,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 220/250 [02:02<00:17,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 221/250 [02:02<00:16,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 222/250 [02:03<00:15,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 223/250 [02:03<00:14,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 224/250 [02:04<00:14,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 225/250 [02:05<00:13,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 226/250 [02:05<00:12,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 227/250 [02:06<00:12,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 228/250 [02:06<00:12,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 229/250 [02:07<00:11,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 230/250 [02:07<00:10,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 231/250 [02:08<00:10,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 232/250 [02:08<00:09,  1.92it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 233/250 [02:09<00:08,  1.97it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 234/250 [02:09<00:08,  1.97it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 235/250 [02:10<00:07,  1.98it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 236/250 [02:10<00:07,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 237/250 [02:11<00:07,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 238/250 [02:12<00:06,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 239/250 [02:12<00:06,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.▏  | 240/250 [02:13<00:05,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.▌  | 241/250 [02:13<00:04,  1.88it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.▊  | 242/250 [02:14<00:04,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█  | 243/250 [02:14<00:03,  1.84it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█▎ | 244/250 [02:15<00:03,  1.90it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█▌ | 245/250 [02:15<00:02,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█▉ | 246/250 [02:16<00:02,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.██▏| 247/250 [02:17<00:01,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.██▍| 248/250 [02:17<00:01,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.██▋| 249/250 [02:18<00:00,  1.83it/s]
                                                                                                            Traceback (most recent call last):
  File "/root/autodl-tmp/MLLM/ImageCaption/train_vit-gpt2.py", line 144, in <module>[02:18<00:00,  2.19it/s]
    trainer.train()
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 2656, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 3095, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 3044, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 4173, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 4463, in evaluation_loop
    metrics = self.compute_metrics(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/ImageCaption/train_vit-gpt2.py", line 48, in compute_metrics
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3810, in batch_decode
    self.decode(
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3849, in decode
    return self._decode(
           ^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py", line 670, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OverflowError: out of range integral type conversion attempted
