  0%|                                                                              | 0/4500 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
                                                                                                            
{'loss': 1.5357, 'grad_norm': 2.7449779510498047, 'learning_rate': 4.978888888888889e-05, 'epoch': 0.01}
{'loss': 0.516, 'grad_norm': 2.9992637634277344, 'learning_rate': 4.956666666666667e-05, 'epoch': 0.03}
{'loss': 0.5066, 'grad_norm': 2.7083616256713867, 'learning_rate': 4.934444444444445e-05, 'epoch': 0.04}
{'loss': 0.5221, 'grad_norm': 3.6985740661621094, 'learning_rate': 4.912222222222223e-05, 'epoch': 0.05}
{'loss': 0.4535, 'grad_norm': 3.3170976638793945, 'learning_rate': 4.89e-05, 'epoch': 0.07}
{'loss': 0.4802, 'grad_norm': 2.2443032264709473, 'learning_rate': 4.867777777777778e-05, 'epoch': 0.08}
{'loss': 0.4235, 'grad_norm': 2.468920946121216, 'learning_rate': 4.845555555555556e-05, 'epoch': 0.09}
{'loss': 0.5093, 'grad_norm': 2.458714485168457, 'learning_rate': 4.823333333333334e-05, 'epoch': 0.11}
{'loss': 0.4561, 'grad_norm': 2.0197715759277344, 'learning_rate': 4.8011111111111114e-05, 'epoch': 0.12}
{'loss': 0.4691, 'grad_norm': 1.714437484741211, 'learning_rate': 4.778888888888889e-05, 'epoch': 0.13}
{'loss': 0.4103, 'grad_norm': 1.6593135595321655, 'learning_rate': 4.756666666666667e-05, 'epoch': 0.15}
{'loss': 0.4669, 'grad_norm': 1.5981816053390503, 'learning_rate': 4.734444444444445e-05, 'epoch': 0.16}
{'loss': 0.4437, 'grad_norm': 1.328294038772583, 'learning_rate': 4.7122222222222225e-05, 'epoch': 0.17}
{'loss': 0.4634, 'grad_norm': 1.9584683179855347, 'learning_rate': 4.69e-05, 'epoch': 0.19}
{'loss': 0.4439, 'grad_norm': 1.4396010637283325, 'learning_rate': 4.6677777777777785e-05, 'epoch': 0.2}
{'loss': 0.4619, 'grad_norm': 1.5101722478866577, 'learning_rate': 4.645555555555556e-05, 'epoch': 0.21}
{'loss': 0.4307, 'grad_norm': 1.167611002922058, 'learning_rate': 4.623333333333334e-05, 'epoch': 0.23}
{'loss': 0.4152, 'grad_norm': 1.4753762483596802, 'learning_rate': 4.601111111111111e-05, 'epoch': 0.24}
{'loss': 0.4264, 'grad_norm': 1.2680305242538452, 'learning_rate': 4.578888888888889e-05, 'epoch': 0.25}
{'loss': 0.4315, 'grad_norm': 1.2943320274353027, 'learning_rate': 4.556666666666667e-05, 'epoch': 0.27}
{'loss': 0.424, 'grad_norm': 0.9907442331314087, 'learning_rate': 4.534444444444445e-05, 'epoch': 0.28}
{'loss': 0.4267, 'grad_norm': 0.8512873649597168, 'learning_rate': 4.5122222222222224e-05, 'epoch': 0.29}
{'loss': 0.4275, 'grad_norm': 1.0037339925765991, 'learning_rate': 4.49e-05, 'epoch': 0.31}
{'loss': 0.441, 'grad_norm': 1.1470192670822144, 'learning_rate': 4.4677777777777777e-05, 'epoch': 0.32}
{'loss': 0.4447, 'grad_norm': 1.049278736114502, 'learning_rate': 4.445555555555555e-05, 'epoch': 0.33}
{'loss': 0.4544, 'grad_norm': 1.1055762767791748, 'learning_rate': 4.4233333333333336e-05, 'epoch': 0.35}
{'loss': 0.4045, 'grad_norm': 1.0242254734039307, 'learning_rate': 4.401111111111111e-05, 'epoch': 0.36}
{'loss': 0.4719, 'grad_norm': 0.7257816195487976, 'learning_rate': 4.378888888888889e-05, 'epoch': 0.37}
{'loss': 0.4313, 'grad_norm': 0.8626216053962708, 'learning_rate': 4.3566666666666664e-05, 'epoch': 0.39}
{'loss': 0.4464, 'grad_norm': 1.0259238481521606, 'learning_rate': 4.334444444444445e-05, 'epoch': 0.4}
{'loss': 0.4208, 'grad_norm': 0.7141054272651672, 'learning_rate': 4.312222222222222e-05, 'epoch': 0.41}
{'loss': 0.3936, 'grad_norm': 1.0412681102752686, 'learning_rate': 4.29e-05, 'epoch': 0.43}
{'loss': 0.4119, 'grad_norm': 0.8211581110954285, 'learning_rate': 4.2677777777777775e-05, 'epoch': 0.44}
{'loss': 0.3885, 'grad_norm': 0.7222384810447693, 'learning_rate': 4.245555555555556e-05, 'epoch': 0.45}
{'loss': 0.4007, 'grad_norm': 0.7338476777076721, 'learning_rate': 4.2233333333333334e-05, 'epoch': 0.47}
{'loss': 0.4036, 'grad_norm': 0.722404420375824, 'learning_rate': 4.201111111111111e-05, 'epoch': 0.48}
{'loss': 0.4159, 'grad_norm': 0.6723411083221436, 'learning_rate': 4.178888888888889e-05, 'epoch': 0.49}
{'loss': 0.4262, 'grad_norm': 0.8989430665969849, 'learning_rate': 4.156666666666667e-05, 'epoch': 0.51}
{'loss': 0.4164, 'grad_norm': 0.7673249840736389, 'learning_rate': 4.1344444444444446e-05, 'epoch': 0.52}
{'loss': 0.4114, 'grad_norm': 0.7743735909461975, 'learning_rate': 4.112222222222222e-05, 'epoch': 0.53}
{'loss': 0.4189, 'grad_norm': 0.7751456499099731, 'learning_rate': 4.09e-05, 'epoch': 0.55}
{'loss': 0.3708, 'grad_norm': 0.7087698578834534, 'learning_rate': 4.067777777777778e-05, 'epoch': 0.56}
{'loss': 0.4058, 'grad_norm': 0.8779277205467224, 'learning_rate': 4.045555555555556e-05, 'epoch': 0.57}
{'loss': 0.4084, 'grad_norm': 0.7818543910980225, 'learning_rate': 4.023333333333333e-05, 'epoch': 0.59}
{'loss': 0.3869, 'grad_norm': 0.641947329044342, 'learning_rate': 4.001111111111111e-05, 'epoch': 0.6}
{'loss': 0.4177, 'grad_norm': 0.7229241132736206, 'learning_rate': 3.978888888888889e-05, 'epoch': 0.61}
{'loss': 0.4205, 'grad_norm': 0.6496186852455139, 'learning_rate': 3.956666666666667e-05, 'epoch': 0.63}
{'loss': 0.4217, 'grad_norm': 0.7729839086532593, 'learning_rate': 3.9344444444444445e-05, 'epoch': 0.64}
{'loss': 0.4017, 'grad_norm': 0.744748055934906, 'learning_rate': 3.912222222222223e-05, 'epoch': 0.65}
{'loss': 0.393, 'grad_norm': 0.7868805527687073, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.67}
{'loss': 0.4308, 'grad_norm': 1.4637608528137207, 'learning_rate': 3.867777777777778e-05, 'epoch': 0.68}
{'loss': 0.4267, 'grad_norm': 0.5848667621612549, 'learning_rate': 3.8455555555555556e-05, 'epoch': 0.69}
{'loss': 0.4289, 'grad_norm': 0.7493292093276978, 'learning_rate': 3.823333333333334e-05, 'epoch': 0.71}
{'loss': 0.4184, 'grad_norm': 0.6332811117172241, 'learning_rate': 3.8011111111111115e-05, 'epoch': 0.72}
{'loss': 0.4162, 'grad_norm': 0.7180962562561035, 'learning_rate': 3.778888888888889e-05, 'epoch': 0.73}
{'loss': 0.4099, 'grad_norm': 0.5942201018333435, 'learning_rate': 3.756666666666667e-05, 'epoch': 0.75}
{'loss': 0.3786, 'grad_norm': 0.7062364816665649, 'learning_rate': 3.734444444444445e-05, 'epoch': 0.76}
{'loss': 0.4161, 'grad_norm': 0.8030933141708374, 'learning_rate': 3.7122222222222226e-05, 'epoch': 0.77}
{'loss': 0.4358, 'grad_norm': 0.6370877623558044, 'learning_rate': 3.69e-05, 'epoch': 0.79}
{'loss': 0.407, 'grad_norm': 0.7205713391304016, 'learning_rate': 3.667777777777778e-05, 'epoch': 0.8}
{'loss': 0.3917, 'grad_norm': 0.6157667636871338, 'learning_rate': 3.645555555555556e-05, 'epoch': 0.81}
{'loss': 0.4036, 'grad_norm': 0.5415421724319458, 'learning_rate': 3.623333333333334e-05, 'epoch': 0.83}
{'loss': 0.3554, 'grad_norm': 0.6684762239456177, 'learning_rate': 3.6011111111111114e-05, 'epoch': 0.84}
{'loss': 0.402, 'grad_norm': 0.7328106760978699, 'learning_rate': 3.578888888888889e-05, 'epoch': 0.85}
{'loss': 0.364, 'grad_norm': 0.5193163156509399, 'learning_rate': 3.556666666666667e-05, 'epoch': 0.87}
{'loss': 0.3783, 'grad_norm': 0.5866727828979492, 'learning_rate': 3.534444444444445e-05, 'epoch': 0.88}
{'loss': 0.4148, 'grad_norm': 0.6257298588752747, 'learning_rate': 3.5122222222222225e-05, 'epoch': 0.89}
{'loss': 0.414, 'grad_norm': 0.7133990526199341, 'learning_rate': 3.49e-05, 'epoch': 0.91}
{'loss': 0.3614, 'grad_norm': 0.6706516742706299, 'learning_rate': 3.4677777777777784e-05, 'epoch': 0.92}
{'loss': 0.3909, 'grad_norm': 0.6003133058547974, 'learning_rate': 3.445555555555556e-05, 'epoch': 0.93}
{'loss': 0.4143, 'grad_norm': 0.6743752360343933, 'learning_rate': 3.4233333333333336e-05, 'epoch': 0.95}
{'loss': 0.377, 'grad_norm': 0.6314189434051514, 'learning_rate': 3.401111111111111e-05, 'epoch': 0.96}
{'loss': 0.381, 'grad_norm': 0.6228288412094116, 'learning_rate': 3.378888888888889e-05, 'epoch': 0.97}
{'loss': 0.3903, 'grad_norm': 0.6584311127662659, 'learning_rate': 3.356666666666667e-05, 'epoch': 0.99}
{'loss': 0.4023, 'grad_norm': 0.6638070344924927, 'learning_rate': 3.334444444444445e-05, 'epoch': 1.0}
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.             | 0/250 [00:00<?, ?it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 2/250 [00:00<01:21,  3.03it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 3/250 [00:01<01:53,  2.17it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 4/250 [00:01<02:09,  1.89it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 5/250 [00:02<02:10,  1.87it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 6/250 [00:02<02:07,  1.92it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 7/250 [00:03<02:15,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 8/250 [00:04<02:14,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     | 9/250 [00:04<02:14,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 10/250 [00:05<02:19,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 11/250 [00:06<02:24,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 12/250 [00:06<02:25,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 13/250 [00:07<02:17,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 14/250 [00:07<02:20,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 15/250 [00:08<02:18,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 16/250 [00:08<02:14,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 17/250 [00:09<02:11,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 18/250 [00:10<02:15,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 19/250 [00:10<02:19,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 20/250 [00:11<02:14,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 21/250 [00:11<02:10,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 22/250 [00:12<02:09,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 23/250 [00:13<02:15,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 24/250 [00:13<02:19,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 25/250 [00:14<02:21,  1.59it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 26/250 [00:15<02:23,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 27/250 [00:15<02:24,  1.54it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 28/250 [00:16<02:21,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 29/250 [00:16<02:12,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 30/250 [00:17<02:09,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 31/250 [00:17<02:06,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 32/250 [00:18<02:02,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 33/250 [00:18<01:59,  1.81it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 34/250 [00:19<02:01,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 35/250 [00:20<02:05,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 36/250 [00:20<02:08,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 37/250 [00:21<02:11,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 38/250 [00:22<02:14,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 39/250 [00:22<02:14,  1.57it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 40/250 [00:23<02:14,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 41/250 [00:24<02:07,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 42/250 [00:24<02:04,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 43/250 [00:25<02:06,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 44/250 [00:25<02:02,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 45/250 [00:26<02:01,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 46/250 [00:26<01:57,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 47/250 [00:27<01:54,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 48/250 [00:27<01:52,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 49/250 [00:28<01:58,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 50/250 [00:29<02:01,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 51/250 [00:29<01:56,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 52/250 [00:30<01:58,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 53/250 [00:31<01:56,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 54/250 [00:31<01:53,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 55/250 [00:32<01:56,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 56/250 [00:32<01:51,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 57/250 [00:33<01:55,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 58/250 [00:33<01:50,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 59/250 [00:34<01:53,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 60/250 [00:35<01:48,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 61/250 [00:35<01:50,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 62/250 [00:36<01:53,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 63/250 [00:36<01:53,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 64/250 [00:37<01:54,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 65/250 [00:38<01:49,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 66/250 [00:38<01:47,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 67/250 [00:39<01:48,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 68/250 [00:39<01:48,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 69/250 [00:40<01:48,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 70/250 [00:41<01:50,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 71/250 [00:41<01:45,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 72/250 [00:42<01:48,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 73/250 [00:42<01:48,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 74/250 [00:43<01:44,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 75/250 [00:44<01:42,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 76/250 [00:44<01:45,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 77/250 [00:45<01:43,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 78/250 [00:45<01:41,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 79/250 [00:46<01:44,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 80/250 [00:47<01:46,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 81/250 [00:47<01:42,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 82/250 [00:48<01:41,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 83/250 [00:49<01:43,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 84/250 [00:49<01:43,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 85/250 [00:50<01:43,  1.59it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 86/250 [00:50<01:38,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 87/250 [00:51<01:39,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 88/250 [00:52<01:39,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 89/250 [00:52<01:34,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 90/250 [00:53<01:33,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 91/250 [00:53<01:31,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 92/250 [00:54<01:32,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 93/250 [00:55<01:35,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 94/250 [00:55<01:37,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 95/250 [00:56<01:37,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 96/250 [00:56<01:34,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 97/250 [00:57<01:36,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 98/250 [00:58<01:33,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.    | 99/250 [00:58<01:35,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 100/250 [00:59<01:36,  1.55it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 101/250 [01:00<01:32,  1.61it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 102/250 [01:00<01:34,  1.57it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 103/250 [01:01<01:35,  1.54it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 104/250 [01:02<01:38,  1.49it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 105/250 [01:02<01:36,  1.50it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 106/250 [01:03<01:35,  1.52it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 107/250 [01:04<01:33,  1.53it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 108/250 [01:04<01:31,  1.55it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 109/250 [01:05<01:27,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 110/250 [01:05<01:29,  1.57it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 111/250 [01:06<01:25,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 112/250 [01:07<01:23,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 113/250 [01:07<01:21,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 114/250 [01:08<01:20,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 115/250 [01:08<01:18,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 116/250 [01:09<01:16,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 117/250 [01:10<01:19,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 118/250 [01:10<01:16,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 119/250 [01:11<01:13,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 120/250 [01:11<01:10,  1.83it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 121/250 [01:12<01:14,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 122/250 [01:12<01:12,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 123/250 [01:13<01:11,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 124/250 [01:13<01:14,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 125/250 [01:14<01:16,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 126/250 [01:15<01:18,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 127/250 [01:15<01:18,  1.57it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 128/250 [01:16<01:18,  1.55it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 129/250 [01:17<01:18,  1.54it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 130/250 [01:17<01:16,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 131/250 [01:18<01:11,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 132/250 [01:19<01:10,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 133/250 [01:19<01:08,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 134/250 [01:20<01:06,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 135/250 [01:20<01:05,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 136/250 [01:21<01:07,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 137/250 [01:21<01:09,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 138/250 [01:22<01:08,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 139/250 [01:23<01:09,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 140/250 [01:23<01:10,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 141/250 [01:24<01:07,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 142/250 [01:25<01:08,  1.59it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 143/250 [01:25<01:05,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 144/250 [01:26<01:05,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 145/250 [01:27<01:11,  1.47it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 146/250 [01:28<01:18,  1.33it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 147/250 [01:28<01:14,  1.39it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 148/250 [01:29<01:08,  1.49it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 149/250 [01:29<01:04,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 150/250 [01:30<01:04,  1.55it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 151/250 [01:31<01:00,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 152/250 [01:31<00:57,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 153/250 [01:32<00:54,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 154/250 [01:32<00:56,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 155/250 [01:33<00:57,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 156/250 [01:33<00:54,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 157/250 [01:34<00:56,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 158/250 [01:35<00:54,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 159/250 [01:35<00:52,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 160/250 [01:36<00:50,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 161/250 [01:36<00:50,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 162/250 [01:37<00:52,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 163/250 [01:37<00:50,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 164/250 [01:38<00:48,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 165/250 [01:39<00:50,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 166/250 [01:39<00:48,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 167/250 [01:40<00:47,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 168/250 [01:40<00:45,  1.79it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 169/250 [01:41<00:44,  1.80it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 170/250 [01:41<00:46,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 171/250 [01:42<00:47,  1.67it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 172/250 [01:43<00:45,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 173/250 [01:43<00:44,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 174/250 [01:44<00:44,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 175/250 [01:44<00:45,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 176/250 [01:45<00:46,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 177/250 [01:46<00:45,  1.61it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 178/250 [01:46<00:43,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 179/250 [01:47<00:49,  1.44it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 180/250 [01:48<00:52,  1.33it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 181/250 [01:49<00:52,  1.32it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 182/250 [01:50<00:54,  1.25it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 183/250 [01:51<00:55,  1.21it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 184/250 [01:52<00:54,  1.22it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 185/250 [01:52<00:50,  1.29it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 186/250 [01:53<00:44,  1.43it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 187/250 [01:53<00:40,  1.55it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 188/250 [01:54<00:38,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 189/250 [01:54<00:36,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 190/250 [01:55<00:35,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 191/250 [01:56<00:35,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 192/250 [01:56<00:33,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 193/250 [01:57<00:33,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 194/250 [01:57<00:31,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 195/250 [01:58<00:32,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 196/250 [01:59<00:35,  1.53it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 197/250 [01:59<00:33,  1.58it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 198/250 [02:00<00:31,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 199/250 [02:00<00:30,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 200/250 [02:01<00:30,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 201/250 [02:02<00:30,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 202/250 [02:02<00:29,  1.62it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 203/250 [02:03<00:28,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 204/250 [02:03<00:26,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 205/250 [02:04<00:26,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 206/250 [02:04<00:24,  1.77it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 207/250 [02:05<00:24,  1.74it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 208/250 [02:06<00:23,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 209/250 [02:06<00:23,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 210/250 [02:07<00:22,  1.78it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 211/250 [02:07<00:21,  1.82it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 212/250 [02:08<00:22,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 213/250 [02:09<00:22,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 214/250 [02:09<00:22,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 215/250 [02:10<00:20,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 216/250 [02:10<00:19,  1.72it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 217/250 [02:11<00:19,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 218/250 [02:11<00:19,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 219/250 [02:12<00:17,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 220/250 [02:13<00:17,  1.73it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 221/250 [02:13<00:16,  1.75it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 222/250 [02:14<00:16,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 223/250 [02:14<00:15,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 224/250 [02:15<00:15,  1.63it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 225/250 [02:16<00:15,  1.66it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 226/250 [02:16<00:14,  1.68it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 227/250 [02:17<00:13,  1.64it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 228/250 [02:17<00:12,  1.70it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 229/250 [02:18<00:11,  1.76it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 230/250 [02:19<00:13,  1.48it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 231/250 [02:19<00:12,  1.56it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 232/250 [02:20<00:11,  1.60it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 233/250 [02:21<00:10,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 234/250 [02:21<00:09,  1.71it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 235/250 [02:22<00:09,  1.57it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 236/250 [02:23<00:10,  1.39it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 237/250 [02:24<00:10,  1.29it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 238/250 [02:24<00:09,  1.26it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.   | 239/250 [02:25<00:08,  1.29it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.▏  | 240/250 [02:26<00:07,  1.42it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.▌  | 241/250 [02:26<00:05,  1.52it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.▊  | 242/250 [02:27<00:05,  1.52it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█  | 243/250 [02:27<00:04,  1.61it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█▎ | 244/250 [02:28<00:03,  1.69it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█▌ | 245/250 [02:29<00:03,  1.61it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.█▉ | 246/250 [02:29<00:02,  1.65it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.██▏| 247/250 [02:30<00:02,  1.49it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.██▍| 248/250 [02:31<00:01,  1.41it/s]
                                                                                                            The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.██▋| 249/250 [02:31<00:00,  1.51it/s]
                                                                                                            Traceback (most recent call last):
  File "/root/autodl-tmp/MLLM/ImageCaption/train_vit-gpt2.py", line 148, in <module>[02:32<00:00,  1.86it/s]
    trainer.train()
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 2656, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 3095, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 3044, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 4173, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py", line 4463, in evaluation_loop
    metrics = self.compute_metrics(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/ImageCaption/train_vit-gpt2.py", line 50, in compute_metrics
    preds = torch.clamp(preds, max=tokenizer.vocab_size - 1)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: clamp() received an invalid combination of arguments - got (numpy.ndarray, max=int), but expected one of:
 * (Tensor input, Tensor min = None, Tensor max = None, *, Tensor out = None)
 * (Tensor input, Number min = None, Number max = None, *, Tensor out = None)
