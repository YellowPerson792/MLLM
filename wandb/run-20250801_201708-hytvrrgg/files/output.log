/root/autodl-tmp/MLLM/ImageCaption/hf_style_trainer.py:178: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.fp16)
Training:   0%|                                                            | 0/9000 [00:00<?, ?it/s]/root/autodl-tmp/MLLM/ImageCaption/hf_style_trainer.py:237: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.bfloat16 if args.bf16 else torch.float16):
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
Training:   6%| | 512/9000 [08:17<2:15:40,  1.04it/s, ep=0.17/3, step=512, loss=2.5703, lr=2.00e-04]/root/miniconda3/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.
[Batch    64] [Opt    8] [Ep  0.021] | Loss:  9.4844 | GradNorm:  76.508 | LR: 2.50e-05
[Batch   128] [Opt   16] [Ep  0.043] | Loss:  5.9766 | GradNorm:  74.373 | LR: 5.00e-05
[Batch   192] [Opt   24] [Ep  0.064] | Loss:  2.5234 | GradNorm:  60.359 | LR: 7.50e-05
[Batch   256] [Opt   32] [Ep  0.085] | Loss:  2.2422 | GradNorm:  49.395 | LR: 1.00e-04
[Batch   320] [Opt   40] [Ep  0.107] | Loss:  2.1094 | GradNorm:  48.331 | LR: 1.25e-04
[Batch   384] [Opt   48] [Ep  0.128] | Loss:  2.7109 | GradNorm:  47.689 | LR: 1.50e-04
[Batch   448] [Opt   56] [Ep  0.149] | Loss:  1.2559 | GradNorm:  43.441 | LR: 1.75e-04
[Batch   512] [Opt   64] [Ep  0.171] | Loss:  2.5703 | GradNorm:  42.153 | LR: 2.00e-04
[Batch   512] [SAVE] | 保存检查点到 checkpoint-512
  warnings.warn(
                                                                                                    
[Batch   576] [Opt   72] [Ep  0.192] | Loss:  2.5547 | GradNorm:  49.256 | LR: 1.98e-04
[Batch   640] [Opt   80] [Ep  0.213] | Loss:  2.0391 | GradNorm:  42.405 | LR: 1.97e-04
[Batch   704] [Opt   88] [Ep  0.235] | Loss:  2.6328 | GradNorm:  50.007 | LR: 1.95e-04
[Batch   768] [Opt   96] [Ep  0.256] | Loss:  2.4844 | GradNorm:  41.374 | LR: 1.94e-04
[Batch   832] [Opt  104] [Ep  0.277] | Loss:  1.9883 | GradNorm:  38.026 | LR: 1.92e-04
[Batch   896] [Opt  112] [Ep  0.299] | Loss:  2.0234 | GradNorm:  19.325 | LR: 1.91e-04
[Batch   960] [Opt  120] [Ep  0.320] | Loss:  1.9219 | GradNorm:  29.672 | LR: 1.89e-04
[Batch  1024] [Opt  128] [Ep  0.341] | Loss:  1.1974 | GradNorm:  29.397 | LR: 1.88e-04
[Batch  1024] [SAVE] | 保存检查点到 checkpoint-1024
[Batch  1088] [Opt  136] [Ep  0.363] | Loss:  1.7773 | GradNorm:  44.717 | LR: 1.86e-04
[Batch  1152] [Opt  144] [Ep  0.384] | Loss:  1.5117 | GradNorm:  38.497 | LR: 1.85e-04
[Batch  1216] [Opt  152] [Ep  0.405] | Loss:  2.2344 | GradNorm:  39.948 | LR: 1.83e-04
[Batch  1280] [Opt  160] [Ep  0.427] | Loss:  2.0234 | GradNorm:  65.778 | LR: 1.82e-04
[Batch  1344] [Opt  168] [Ep  0.448] | Loss:  2.3398 | GradNorm:  52.862 | LR: 1.80e-04
[Batch  1408] [Opt  176] [Ep  0.469] | Loss:  1.6523 | GradNorm:  33.673 | LR: 1.79e-04
[Batch  1472] [Opt  184] [Ep  0.491] | Loss:  2.0859 | GradNorm:  20.079 | LR: 1.77e-04
[Batch  1536] [Opt  192] [Ep  0.512] | Loss:  0.6747 | GradNorm:  38.000 | LR: 1.76e-04
[Batch  1536] [SAVE] | 保存检查点到 checkpoint-1536
[Batch  1600] [Opt  200] [Ep  0.533] | Loss:  2.8086 | GradNorm:  47.908 | LR: 1.74e-04
[Batch  1664] [Opt  208] [Ep  0.555] | Loss:  1.5508 | GradNorm:  32.020 | LR: 1.73e-04
[Batch  1728] [Opt  216] [Ep  0.576] | Loss:  1.0157 | GradNorm:  28.112 | LR: 1.71e-04
[Batch  1792] [Opt  224] [Ep  0.597] | Loss:  0.6199 | GradNorm:  26.059 | LR: 1.70e-04
[Batch  1856] [Opt  232] [Ep  0.619] | Loss:  0.8096 | GradNorm:  35.592 | LR: 1.68e-04
[Batch  1920] [Opt  240] [Ep  0.640] | Loss:  1.3086 | GradNorm:  38.609 | LR: 1.67e-04
[Batch  1984] [Opt  248] [Ep  0.661] | Loss:  0.1846 | GradNorm:  32.219 | LR: 1.65e-04
[Batch  2048] [Opt  256] [Ep  0.683] | Loss:  1.8672 | GradNorm:  30.598 | LR: 1.64e-04
[Batch  2048] [SAVE] | 保存检查点到 checkpoint-2048
[Batch  2112] [Opt  264] [Ep  0.704] | Loss:  0.7578 | GradNorm:  18.794 | LR: 1.62e-04
[Batch  2176] [Opt  272] [Ep  0.725] | Loss:  1.3389 | GradNorm:  41.466 | LR: 1.61e-04
[Batch  2240] [Opt  280] [Ep  0.747] | Loss:  1.7188 | GradNorm:  35.088 | LR: 1.59e-04
[Batch  2304] [Opt  288] [Ep  0.768] | Loss:  0.3541 | GradNorm:  26.963 | LR: 1.58e-04
[Batch  2368] [Opt  296] [Ep  0.789] | Loss:  0.5933 | GradNorm:  22.757 | LR: 1.56e-04
[Batch  2432] [Opt  304] [Ep  0.811] | Loss:  0.2954 | GradNorm:  19.547 | LR: 1.55e-04
[Batch  2496] [Opt  312] [Ep  0.832] | Loss:  1.1396 | GradNorm:  29.641 | LR: 1.53e-04
[Batch  2560] [Opt  320] [Ep  0.853] | Loss:  2.7695 | GradNorm:  14.416 | LR: 1.52e-04
[Batch  2560] [SAVE] | 保存检查点到 checkpoint-2560
[Batch  2624] [Opt  328] [Ep  0.875] | Loss:  3.2969 | GradNorm:  44.651 | LR: 1.50e-04
[Batch  2688] [Opt  336] [Ep  0.896] | Loss:  1.9648 | GradNorm:  23.352 | LR: 1.49e-04
[Batch  2752] [Opt  344] [Ep  0.917] | Loss:  3.1992 | GradNorm: 235.662 | LR: 1.47e-04
[Batch  2816] [Opt  352] [Ep  0.939] | Loss:  1.7227 | GradNorm:  32.497 | LR: 1.46e-04
[Batch  2880] [Opt  360] [Ep  0.960] | Loss:  2.3516 | GradNorm:  37.965 | LR: 1.44e-04
[Batch  2944] [Opt  368] [Ep  0.981] | Loss:  1.8477 | GradNorm:  16.936 | LR: 1.43e-04
=== [EPOCH 1/3 完成] | 平均Loss: 2.2055 | 总Batch步数: 3000 | 总Opt步数: 375 ===
Training:  44%|▍| 3976/9000 [1:04:32<1:22:06,  1.02it/s, ep=1.33/3, step=3976, loss=2.5391, lr=1.18eTraceback (most recent call last):
  File "/root/autodl-tmp/MLLM/ImageCaption/train_jpeglm-gpt2_cls.py", line 349, in <module>         
[EPOCH 1] [EVAL] | Loss: 2.4546 | Metrics: {'accuracy': 0.0, 'rouge2_fmeasure': np.float64(0.0), 'bleu1': np.float64(0.0), 'bleu4': np.float64(0.0)}
[Batch  3008] [Opt  376] [Ep  1.003] | Loss:  1.8828 | GradNorm:  26.794 | LR: 1.41e-04
[Batch  3072] [Opt  384] [Ep  1.024] | Loss:  2.4844 | GradNorm:  43.538 | LR: 1.40e-04
[Batch  3072] [SAVE] | 保存检查点到 checkpoint-3072
[Batch  3136] [Opt  392] [Ep  1.045] | Loss:  2.0859 | GradNorm:  46.239 | LR: 1.38e-04
[Batch  3200] [Opt  400] [Ep  1.067] | Loss:  2.3672 | GradNorm:  43.582 | LR: 1.37e-04
[Batch  3264] [Opt  408] [Ep  1.088] | Loss:  2.3789 | GradNorm:  21.682 | LR: 1.35e-04
[Batch  3328] [Opt  416] [Ep  1.109] | Loss:  2.2344 | GradNorm:  24.631 | LR: 1.34e-04
[Batch  3392] [Opt  424] [Ep  1.131] | Loss:  2.4609 | GradNorm:  31.646 | LR: 1.32e-04
[Batch  3456] [Opt  432] [Ep  1.152] | Loss:  2.6406 | GradNorm:  28.591 | LR: 1.31e-04
[Batch  3520] [Opt  440] [Ep  1.173] | Loss:  2.2188 | GradNorm:  27.386 | LR: 1.29e-04
[Batch  3584] [Opt  448] [Ep  1.195] | Loss:  2.7031 | GradNorm:  36.062 | LR: 1.28e-04
[Batch  3584] [SAVE] | 保存检查点到 checkpoint-3584
[Batch  3648] [Opt  456] [Ep  1.216] | Loss:  2.5078 | GradNorm:  27.910 | LR: 1.26e-04
[Batch  3712] [Opt  464] [Ep  1.237] | Loss:  2.3359 | GradNorm:  23.507 | LR: 1.25e-04
[Batch  3776] [Opt  472] [Ep  1.259] | Loss:  2.5703 | GradNorm:  24.275 | LR: 1.23e-04
[Batch  3840] [Opt  480] [Ep  1.280] | Loss:  2.3906 | GradNorm:  21.566 | LR: 1.22e-04
[Batch  3904] [Opt  488] [Ep  1.301] | Loss:  2.6250 | GradNorm:  32.704 | LR: 1.20e-04
[Batch  3968] [Opt  496] [Ep  1.323] | Loss:  2.3438 | GradNorm:  28.633 | LR: 1.19e-04
    trainer.train()
  File "/root/autodl-tmp/MLLM/ImageCaption/hf_style_trainer.py", line 238, in train
    outputs = self.model(**model_inputs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/peft/peft_model.py", line 2031, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py", line 531, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/MLLM/jpeglm/models/jpeglm_encoder.py", line 99, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 453, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_layers.py", line 47, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/_compile.py", line 51, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 488, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 263, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 308, in forward
    hidden_states, self_attn_weights = self.self_attn(
                                       ^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 243, in forward
    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/peft/tuners/lora/layer.py", line 712, in forward
    result = self.base_layer(x, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
